---
title: "Assignment 04 ‚Äî Lab 02"
subtitle: "Insight from Data and Visualizations"
author:
  - name: "Othmane Elouardi"
    affiliations:
      - id: bu
        name: "Boston University"
        city: "Boston"
        state: "MA"
date: "2025-10-08"
number-sections: true
format:
  html:
    theme:
      light: lux
      dark: slate
    toc: true
    toc-depth: 3
    toc-location: right
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-line-numbers: true
    highlight-style: a11y
    page-layout: article
    css: styles.css
    grid:
      body-width: 900px
      margin-width: 280px
execute:
  echo: true
  warning: false
  error: false
  freeze: auto
jupyter: env
---

# Introduction
The objective of this lab is to apply visual reporting and storytelling techniques to analyze job market data provided by Lightcast. Through this exercise, we will use PySpark and data visualization tools to explore, clean, and interpret a large dataset of job postings. The goal is to transform raw data into meaningful insights by identifying trends, patterns, and relationships between variables such as education level, remote work type, experience, and salary. This lab emphasizes not only technical proficiency in data processing but also the ability to communicate analytical findings effectively through visual narratives.

# Load the Dataset

In this step, we‚Äôll use **PySpark** to load the *Lightcast Job Postings* dataset into a Spark DataFrame.  
This allows us to handle large data efficiently before moving to Pandas for visualization.









```{python}
# Load the Lightcast dataset with PySpark

from pyspark.sql import SparkSession
import os

# Initialize Spark session (optimized for EC2 memory limits)
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .config("spark.driver.memory", "1g")
    .getOrCreate()
)

# Define dataset path
csv_path = "data/lightcast_job_postings.csv"

# Check path validity
if not os.path.exists(csv_path):
    raise FileNotFoundError(
        f"‚ùå Could not find {csv_path}. Please ensure the file exists in the data/ folder."
    )

# Load dataset
df = (
    spark.read
    .option("header", "true")        # First row as headers
    .option("inferSchema", "true")   # Auto-detect data types
    .option("multiLine", "true")     # Handle multi-line text fields
    .option("escape", "\"")          # Handle embedded quotes
    .csv(csv_path)
)

# Confirm load success
print("‚úÖ Dataset successfully loaded!")
df.printSchema()
df.show(5, truncate=False)

```

# Data Cleaning and Typecasting

In this step, we will clean and typecast key columns in our dataset to prepare it for analysis.  
The process involves identifying relevant columns, converting data types, and handling missing values appropriately.

## Typecast Key Columns:

```{python}
# Import necessary libraries
from pyspark.sql import functions as F, types as T
import os

#  Select and typecast key columns
# Convert salary fields to numeric values for later analysis
clean_df = (
    df.withColumn("SALARY_FROM", F.col("SALARY_FROM").cast(T.DoubleType()))
      .withColumn("SALARY_TO", F.col("SALARY_TO").cast(T.DoubleType()))
      .withColumn("MAX_YEARS_EXPERIENCE", F.col("MAX_YEARS_EXPERIENCE").cast(T.DoubleType()))
)
# Handle missing values
# Fill missing numeric values with 0 (or another placeholder if desired)
clean_df = clean_df.fillna({"SALARY_FROM": 0, "SALARY_TO": 0, "MAX_YEARS_EXPERIENCE": 0})

# Optional: Calculate average salary column for convenience
clean_df = clean_df.withColumn(
    "AVERAGE_SALARY",
    (F.col("SALARY_FROM") + F.col("SALARY_TO")) / 2
)

# Show a quick sample
clean_df.select("SALARY_FROM", "SALARY_TO", "AVERAGE_SALARY", "MAX_YEARS_EXPERIENCE").show(5, truncate=False)

```

## Save the Cleaned Dataset
Now that the dataset has been cleaned and typecasted, we will save it as a new CSV file for later use.


```{python}

# --- Save cleaned dataset safely ---------------------------------------------
import os, glob, shutil

# keep only the columns you need downstream (edit as needed)
cols_to_keep = [c for c in [
    "SALARY_FROM","SALARY_TO","AVERAGE_SALARY","MAX_YEARS_EXPERIENCE",
    # "EDUCATION_NAME","REMOTE_TYPE_NAME","LOT_V6_SPECIALIZED_OCCUPATION_NAME",
] if c in clean_df.columns]

tmp_dir   = "_tmp_clean_csv"
final_csv = "data/lightcast_cleaned.csv"

# Write a single CSV part from Spark
(clean_df
 .select(cols_to_keep) if cols_to_keep else clean_df
).coalesce(1).write.mode("overwrite").option("header","true").csv(tmp_dir)

# Move part-*.csv to final path
part_files = glob.glob(os.path.join(tmp_dir, "part-*.csv"))
if not part_files:
    raise RuntimeError("No part file found after Spark write.")
if os.path.exists(final_csv):
    os.remove(final_csv)
shutil.move(part_files[0], final_csv)
shutil.rmtree(tmp_dir, ignore_errors=True)

print("‚úÖ Cleaned dataset saved.")
print(f"üìÅ File location: {final_csv}")


```



