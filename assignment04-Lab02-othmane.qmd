---
title: "Assignment 04 ‚Äî Lab 02"
subtitle: "Insight from Data and Visualizations"
author:
  - name: "Othmane Elouardi"
    affiliations:
      - id: bu
        name: "Boston University"
        city: "Boston"
        state: "MA"
date: "2025-10-08"
number-sections: true
format:
  html:
    theme:
      light: lux
      dark: slate
    toc: true
    toc-depth: 3
    toc-location: right
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-line-numbers: true
    highlight-style: a11y
    page-layout: article
    css: styles.css
    grid:
      body-width: 900px
      margin-width: 280px
execute:
  echo: true
  warning: false
  error: false
  freeze: auto
jupyter: env
---

# Introduction
The objective of this lab is to apply visual reporting and storytelling techniques to analyze job market data provided by Lightcast. Through this exercise, we will use PySpark and data visualization tools to explore, clean, and interpret a large dataset of job postings. The goal is to transform raw data into meaningful insights by identifying trends, patterns, and relationships between variables such as education level, remote work type, experience, and salary. This lab emphasizes not only technical proficiency in data processing but also the ability to communicate analytical findings effectively through visual narratives.

# Load the Dataset

In this step, we‚Äôll use **PySpark** to load the *Lightcast Job Postings* dataset into a Spark DataFrame.  
This allows us to handle large data efficiently before moving to Pandas for visualization.









```{python}
# Load the Lightcast dataset with PySpark

from pyspark.sql import SparkSession
import os

# Initialize Spark session (optimized for EC2 memory limits)
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .config("spark.driver.memory", "1g")
    .getOrCreate()
)

# Define dataset path
csv_path = "data/lightcast_job_postings.csv"

# Check path validity
if not os.path.exists(csv_path):
    raise FileNotFoundError(
        f"‚ùå Could not find {csv_path}. Please ensure the file exists in the data/ folder."
    )

# Load dataset
df = (
    spark.read
    .option("header", "true")        # First row as headers
    .option("inferSchema", "true")   # Auto-detect data types
    .option("multiLine", "true")     # Handle multi-line text fields
    .option("escape", "\"")          # Handle embedded quotes
    .csv(csv_path)
)

# Confirm load success
print("‚úÖ Dataset successfully loaded!")
df.printSchema()
df.show(5, truncate=False)

```

# Data Cleaning and Typecasting

In this step, we will clean and typecast key columns in our dataset to prepare it for analysis.  
The process involves identifying relevant columns, converting data types, and handling missing values appropriately.

## Typecast Key Columns:

```{python}
# Import necessary libraries
from pyspark.sql import functions as F, types as T
import os

#  Select and typecast key columns
# Convert salary fields to numeric values for later analysis
clean_df = (
    df.withColumn("SALARY_FROM", F.col("SALARY_FROM").cast(T.DoubleType()))
      .withColumn("SALARY_TO", F.col("SALARY_TO").cast(T.DoubleType()))
      .withColumn("MAX_YEARS_EXPERIENCE", F.col("MAX_YEARS_EXPERIENCE").cast(T.DoubleType()))
)
# Handle missing values
# Fill missing numeric values with 0 (or another placeholder if desired)
clean_df = clean_df.fillna({"SALARY_FROM": 0, "SALARY_TO": 0, "MAX_YEARS_EXPERIENCE": 0})

# Optional: Calculate average salary column for convenience
clean_df = clean_df.withColumn(
    "AVERAGE_SALARY",
    (F.col("SALARY_FROM") + F.col("SALARY_TO")) / 2
)

# Show a quick sample
clean_df.select("SALARY_FROM", "SALARY_TO", "AVERAGE_SALARY", "MAX_YEARS_EXPERIENCE").show(5, truncate=False)

```

## Save the Cleaned Dataset
Now that the dataset has been cleaned and typecasted, we will save it as a new CSV file for later use.


```{python}

# Save cleaned dataset safely
import os, glob, shutil

# keep only the columns needed downstream 
cols_to_keep = [c for c in [
    "SALARY_FROM","SALARY_TO","AVERAGE_SALARY","MAX_YEARS_EXPERIENCE",
    # "EDUCATION_NAME","REMOTE_TYPE_NAME","LOT_V6_SPECIALIZED_OCCUPATION_NAME",
] if c in clean_df.columns]

tmp_dir   = "_tmp_clean_csv"
final_csv = "data/lightcast_cleaned.csv"

# CSV part from Spark
(clean_df
 .select(cols_to_keep) if cols_to_keep else clean_df
).coalesce(1).write.mode("overwrite").option("header","true").csv(tmp_dir)

# Move part-*.csv to final path
part_files = glob.glob(os.path.join(tmp_dir, "part-*.csv"))
if not part_files:
    raise RuntimeError("No part file found after Spark write.")
if os.path.exists(final_csv):
    os.remove(final_csv)
shutil.move(part_files[0], final_csv)
shutil.rmtree(tmp_dir, ignore_errors=True)

print("‚úÖ Cleaned dataset saved.")
print(f"üìÅ File location: {final_csv}")


```

# Salary by Education Level


```{python}

# --- 3) Salary by Education Level -------------------------------------------
from pyspark.sql import functions as F
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os, re
from datetime import datetime
from IPython.display import Markdown, display

# ========== Helpers: save charts + notes to _output ==========================
os.makedirs("_output", exist_ok=True)
ANALYSIS_MD = "_output/analysis.md"

def _slug(s: str) -> str:
    s = re.sub(r"[^\w\s-]", "", s.lower()).strip()
    return re.sub(r"[\s_-]+", "-", s)

# Save figure as HTML (interactive if mpld3 available; else HTML embedding PNG)
try:
    import mpld3
    _HAS_MPLD3 = True
except Exception:
    _HAS_MPLD3 = False

def save_chart(fig, title: str, tag: str):
    base = _slug(tag)
    html_path = os.path.join("_output", f"{base}.html")
    png_path  = os.path.join("_output", f"{base}.png")

    if _HAS_MPLD3:
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(mpld3.fig_to_html(fig))
    else:
        fig.savefig(png_path, bbox_inches="tight", dpi=125)
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(
                f"<!doctype html><meta charset='utf-8'>"
                f"<title>{title}</title>"
                f"<h2 style='font-family:sans-serif'>{title}</h2>"
                f"<img src='{os.path.basename(png_path)}' style='max-width:100%;height:auto'/>"
            )
    print(f"üíæ Saved: {html_path}")

def note(title: str, lines: list[str]):
    with open(ANALYSIS_MD, "a", encoding="utf-8") as f:
        f.write(f"## {title}\n" + "\n".join(lines) + "\n\n")
        f.write(f"_Saved {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}_\n\n")
    print(f"üìù Appended analysis for: {title}")

def explain_inline(title, lines):
    display(Markdown(f"**{title}**  \n" + "  \n".join(lines)))

def jitter(x, j=0.15):
    x = np.asarray(pd.Series(x).astype(float).fillna(0.0))
    return x + np.random.uniform(-j, j, size=len(x))

# ========== 1) Pick an education column that exists ==========================
edu_col = next(
    (c for c in [
        "EDUCATION_LEVELS_NAME", "MIN_EDULEVELS_NAME", "MAX_EDULEVELS_NAME", "EDUCATION_LEVELS"
    ] if c in clean_df.columns),
    None
)
if edu_col is None:
    raise ValueError("No education-related column found in the dataset.")

# ========== 2) Build a slim Spark DF and sample ==============================
sdf = clean_df.select(
    F.col(edu_col).alias("EDUCATION"),
    F.col("MAX_YEARS_EXPERIENCE").cast("double").alias("MAX_YEARS_EXPERIENCE"),
    F.col("AVERAGE_SALARY").cast("double").alias("AVERAGE_SALARY"),
)

# Light sample (adjust if you need more/less)
# If your dataset is already small you can remove .sample(...)
sdf_sample = sdf.sample(0.12, seed=42)

# ========== 3) Convert to Pandas and categorize education ====================
pdf = sdf_sample.toPandas()

def categorize_education(edu):
    if pd.isna(edu):
        return "Bachelor‚Äôs or lower"
    s = str(edu).lower()
    if any(k in s for k in ["master", "phd", "ph.d", "doctoral", "doctor", "professional"]):
        return "Master‚Äôs or PhD"
    # Treat everything else as <= bachelor's per lab instructions
    if any(k in s for k in ["bachelor", "associate", "ged", "no education"]):
        return "Bachelor‚Äôs or lower"
    return "Bachelor‚Äôs or lower"

pdf["Education_Group"] = pdf["EDUCATION"].apply(categorize_education)

# Optional cleanup: drop rows with missing salary for cleaner plots
pdf = pdf.dropna(subset=["AVERAGE_SALARY"])

groups = [g for g in pdf["Education_Group"].dropna().unique()]

# Explanations (two sentences each)
scatter_notes = {
    "Bachelor‚Äôs or lower": [
        "Salaries increase slightly with experience, but most jobs cluster at low experience with wide pay variation.",
        "Several high-pay outliers at minimal experience suggest niche, high-demand roles."
    ],
    "Master‚Äôs or PhD": [
        "Advanced-degree roles show higher average salaries, especially beyond ~5 years of experience.",
        "Variation remains wide, indicating experience alone doesn‚Äôt fully explain pay differences."
    ],
}
hist_notes = {
    "Bachelor‚Äôs or lower": [
        "The distribution is strongly right-skewed with most postings in lower bands and a long high-pay tail.",
        "A spike near zero indicates missing or placeholder salaries that should be filtered for clean stats."
    ],
    "Master‚Äôs or PhD": [
        "The center of the distribution is higher than the bachelor‚Äôs group, reflecting an earnings premium.",
        "A longer upper tail suggests specialized or lead roles command top-end salaries."
    ],
}

# ========== 4) Scatter plots + inline text + file outputs ====================
for g in groups:
    sub = pdf[pdf["Education_Group"] == g]
    if sub.empty:
        continue

    fig = plt.figure(figsize=(8,5))
    plt.scatter(jitter(sub["MAX_YEARS_EXPERIENCE"]), sub["AVERAGE_SALARY"], s=15, alpha=0.6)
    plt.title(f"Salary vs. Experience ‚Äî {g}")
    plt.xlabel("Max Years Experience (jittered)")
    plt.ylabel("Average Salary")
    plt.tight_layout()
    plt.show()

    title = f"Salary vs. Experience ‚Äî {g}"
    lines = scatter_notes.get(g, [
        "Salaries show a weak positive relationship with experience.",
        "Wide dispersion suggests role and industry strongly influence pay."
    ])
    explain_inline(title, lines)
    save_chart(fig, title, tag=f"q3_scatter_{g}")
    note(title, lines)

# ========== 5) Histograms + inline text + file outputs =======================
for g in groups:
    sub = pdf.loc[pdf["Education_Group"] == g, "AVERAGE_SALARY"].dropna()
    if sub.empty:
        continue

    fig = plt.figure(figsize=(8,5))
    counts, edges, _ = plt.hist(sub, bins=30, density=True, alpha=0.7)
    centers = (edges[:-1] + edges[1:]) / 2
    smooth = pd.Series(counts).rolling(window=3, center=True, min_periods=1).mean()
    plt.plot(centers, smooth, linewidth=2)
    plt.title(f"Salary Distribution ‚Äî {g}")
    plt.xlabel("Average Salary")
    plt.ylabel("Density")
    plt.tight_layout()
    plt.show()

    title = f"Salary Distribution ‚Äî {g}"
    lines = hist_notes.get(g, [
        "Right-skewed distribution with most values in lower bands.",
        "Upper tail indicates a subset of high-pay roles."
    ])
    explain_inline(title, lines)
    save_chart(fig, title, tag=f"q3_hist_{g}")
    note(title, lines)


```

# Salary by Remote Work Type


```{python}
# --- 4) Salary by Remote Work Type ------------------------------------------
from pyspark.sql import functions as F
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os, re
from datetime import datetime
from IPython.display import Markdown, display

# ===== Helpers (defined only if missing) =====================================
if 'save_chart' not in globals():
    os.makedirs("_output", exist_ok=True)
    ANALYSIS_MD = "_output/analysis.md"

    def _slug(s: str) -> str:
        s = re.sub(r"[^\w\s-]", "", s.lower()).strip()
        return re.sub(r"[\s_-]+", "-", s)

    try:
        import mpld3
        _HAS_MPLD3 = True
    except Exception:
        _HAS_MPLD3 = False

    def save_chart(fig, title: str, tag: str):
        base = _slug(tag)
        html_path = os.path.join("_output", f"{base}.html")
        png_path  = os.path.join("_output", f"{base}.png")

        if _HAS_MPLD3:
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(mpld3.fig_to_html(fig))
        else:
            fig.savefig(png_path, bbox_inches="tight", dpi=125)
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(
                    f"<!doctype html><meta charset='utf-8'>"
                    f"<title>{title}</title>"
                    f"<h2 style='font-family:sans-serif'>{title}</h2>"
                    f"<img src='{os.path.basename(png_path)}' style='max-width:100%;height:auto'/>"
                )
        print(f"üíæ Saved: {html_path}")

    def note(title: str, lines: list[str]):
        with open(ANALYSIS_MD, "a", encoding="utf-8") as f:
            f.write(f"## {title}\n" + "\n".join(lines) + "\n\n")
            f.write(f"_Saved {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}_\n\n")
        print(f"üìù Appended analysis for: {title}")

def explain_inline(title, lines):
    display(Markdown(f"**{title}**  \n" + "  \n".join(lines)))

def jitter(x, j=0.15):
    x = np.asarray(pd.Series(x).astype(float).fillna(0.0))
    return x + np.random.uniform(-j, j, size=len(x))

# ===== 1) Validate and prepare columns ======================================
if "REMOTE_TYPE_NAME" not in clean_df.columns:
    raise ValueError("Column 'REMOTE_TYPE_NAME' not found in dataset.")

sdf = clean_df.select(
    F.col("REMOTE_TYPE_NAME").alias("REMOTE"),
    F.col("MAX_YEARS_EXPERIENCE").cast("double").alias("MAX_YEARS_EXPERIENCE"),
    F.col("AVERAGE_SALARY").cast("double").alias("AVERAGE_SALARY"),
)

# sample lightly to keep memory comfortable (tweak fraction as needed)
sdf_sample = sdf.sample(0.12, seed=42)

pdf = sdf_sample.toPandas()

def categorize_remote(val):
    if pd.isna(val) or str(val).strip() == "" or str(val).lower() in {"none", "null"}:
        return "Onsite"
    s = str(val).lower()
    if "hybrid" in s:
        return "Hybrid"
    if "remote" in s:
        return "Remote"
    return "Onsite"

pdf["Remote_Type_Group"] = pdf["REMOTE"].apply(categorize_remote)
pdf = pdf.dropna(subset=["AVERAGE_SALARY"])  # cleaner plots

groups = [g for g in pdf["Remote_Type_Group"].dropna().unique()]

# Explanations (two sentences each)
scatter_notes = {
    "Remote": [
        "Remote roles generally offer higher salaries across experience levels, reflecting demand for distributed digital work.",
        "Salary spread is wider for remote jobs, indicating varied specialization and seniority."
    ],
    "Hybrid": [
        "Hybrid positions show moderate salaries that rise gradually with experience.",
        "Fewer extreme outliers suggest more standardized compensation than fully remote roles."
    ],
    "Onsite": [
        "Onsite roles cluster in lower-to-mid salary ranges with limited dispersion.",
        "Experience appears to have a smaller effect on pay compared to remote or hybrid work."
    ],
}
hist_notes = {
    "Remote": [
        "The salary distribution is right-skewed with a higher center, indicating many roles above the overall median.",
        "A noticeable upper tail suggests specialized remote postings command premium pay."
    ],
    "Hybrid": [
        "Hybrid salaries concentrate around the middle with a balanced spread.",
        "The thinner upper tail implies fewer top-pay opportunities than remote roles."
    ],
    "Onsite": [
        "Onsite salaries show the lowest center with strong right skew toward the lower bands.",
        "The short upper tail confirms fewer high-pay onsite postings compared with other modes."
    ],
}

# ===== 2) Scatter plots + inline text + saved files ==========================
for g in groups:
    sub = pdf[pdf["Remote_Type_Group"] == g]
    if sub.empty:
        continue

    fig = plt.figure(figsize=(8,5))
    plt.scatter(jitter(sub["MAX_YEARS_EXPERIENCE"]), sub["AVERAGE_SALARY"], s=15, alpha=0.6)
    plt.title(f"Salary vs. Experience ‚Äî {g}")
    plt.xlabel("Max Years Experience (jittered)")
    plt.ylabel("Average Salary")
    plt.tight_layout()
    plt.show()

    title = f"Salary vs. Experience ‚Äî {g}"
    lines = scatter_notes.get(g, [
        "Salaries show a modest positive relationship with experience.",
        "Differences across work modes suggest varying job markets and requirements."
    ])
    explain_inline(title, lines)
    save_chart(fig, title, tag=f"q4_scatter_{g}")
    note(title, lines)

# ===== 3) Histograms + inline text + saved files =============================
for g in groups:
    sub = pdf.loc[pdf["Remote_Type_Group"] == g, "AVERAGE_SALARY"].dropna()
    if sub.empty:
        continue

    fig = plt.figure(figsize=(8,5))
    counts, edges, _ = plt.hist(sub, bins=30, density=True, alpha=0.7)
    centers = (edges[:-1] + edges[1:]) / 2
    smooth = pd.Series(counts).rolling(window=3, center=True, min_periods=1).mean()
    plt.plot(centers, smooth, linewidth=2)
    plt.title(f"Salary Distribution ‚Äî {g}")
    plt.xlabel("Average Salary")
    plt.ylabel("Density")
    plt.tight_layout()
    plt.show()

    title = f"Salary Distribution ‚Äî {g}"
    lines = hist_notes.get(g, [
        "Right-skewed distribution with most salaries in the lower ranges.",
        "A smaller upper tail indicates fewer high-pay roles for this work mode."
    ])
    explain_inline(title, lines)
    save_chart(fig, title, tag=f"q4_hist_{g}")
    note(title, lines)

```