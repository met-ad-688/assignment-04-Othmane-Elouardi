---
title: "Assignment 04 ‚Äî Lab 02"
subtitle: "Insight from Data and Visualizations"
author:
  - name: "Othmane Elouardi"
    affiliations:
      - id: bu
        name: "Boston University"
        city: "Boston"
        state: "MA"
date: "2025-10-08"
number-sections: true
format:
  html:
    theme:
      light: lux
      dark: slate
    toc: true
    toc-depth: 3
    toc-location: right
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-line-numbers: true
    highlight-style: a11y
    page-layout: article
    css: styles.css
    grid:
      body-width: 900px
      margin-width: 280px
execute:
  echo: true
  warning: false
  error: false
  freeze: auto
jupyter: env
---

# Introduction
The objective of this lab is to apply visual reporting and storytelling techniques to analyze job market data provided by Lightcast. Through this exercise, we will use PySpark and data visualization tools to explore, clean, and interpret a large dataset of job postings. The goal is to transform raw data into meaningful insights by identifying trends, patterns, and relationships between variables such as education level, remote work type, experience, and salary. This lab emphasizes not only technical proficiency in data processing but also the ability to communicate analytical findings effectively through visual narratives.

# Load the Dataset

In this step, we‚Äôll use **PySpark** to load the *Lightcast Job Postings* dataset into a Spark DataFrame.  
This allows us to handle large data efficiently before moving to Pandas for visualization.









```{python}
# Load the Lightcast dataset with PySpark

from pyspark.sql import SparkSession
import os

# Initialize Spark session (optimized for EC2 memory limits)
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .config("spark.driver.memory", "1g")
    .getOrCreate()
)

# Define dataset path
csv_path = "data/lightcast_job_postings.csv"

# Check path validity
if not os.path.exists(csv_path):
    raise FileNotFoundError(
        f"‚ùå Could not find {csv_path}. Please ensure the file exists in the data/ folder."
    )

# Load dataset
df = (
    spark.read
    .option("header", "true")        # First row as headers
    .option("inferSchema", "true")   # Auto-detect data types
    .option("multiLine", "true")     # Handle multi-line text fields
    .option("escape", "\"")          # Handle embedded quotes
    .csv(csv_path)
)

# Confirm load success
print("‚úÖ Dataset successfully loaded!")
df.printSchema()
df.show(5, truncate=False)

```

# Data Cleaning and Typecasting

In this step, we will clean and typecast key columns in our dataset to prepare it for analysis.  
The process involves identifying relevant columns, converting data types, and handling missing values appropriately.

## Typecast Key Columns:

```{python}
# Import necessary libraries
from pyspark.sql import functions as F, types as T
import os

#  Select and typecast key columns
# Convert salary fields to numeric values for later analysis
clean_df = (
    df.withColumn("SALARY_FROM", F.col("SALARY_FROM").cast(T.DoubleType()))
      .withColumn("SALARY_TO", F.col("SALARY_TO").cast(T.DoubleType()))
      .withColumn("MAX_YEARS_EXPERIENCE", F.col("MAX_YEARS_EXPERIENCE").cast(T.DoubleType()))
)
# Handle missing values
# Fill missing numeric values with 0 (or another placeholder if desired)
clean_df = clean_df.fillna({"SALARY_FROM": 0, "SALARY_TO": 0, "MAX_YEARS_EXPERIENCE": 0})

# Optional: Calculate average salary column for convenience
clean_df = clean_df.withColumn(
    "AVERAGE_SALARY",
    (F.col("SALARY_FROM") + F.col("SALARY_TO")) / 2
)

# Show a quick sample
clean_df.select("SALARY_FROM", "SALARY_TO", "AVERAGE_SALARY", "MAX_YEARS_EXPERIENCE").show(5, truncate=False)

```

## Save the Cleaned Dataset
Now that the dataset has been cleaned and typecasted, we will save it as a new CSV file for later use.


```{python}

# Save cleaned dataset safely
import os, glob, shutil

# keep only the columns needed downstream 
cols_to_keep = [c for c in [
    "SALARY_FROM","SALARY_TO","AVERAGE_SALARY","MAX_YEARS_EXPERIENCE",
    # "EDUCATION_NAME","REMOTE_TYPE_NAME","LOT_V6_SPECIALIZED_OCCUPATION_NAME",
] if c in clean_df.columns]

tmp_dir   = "_tmp_clean_csv"
final_csv = "data/lightcast_cleaned.csv"

# CSV part from Spark
(clean_df
 .select(cols_to_keep) if cols_to_keep else clean_df
).coalesce(1).write.mode("overwrite").option("header","true").csv(tmp_dir)

# Move part-*.csv to final path
part_files = glob.glob(os.path.join(tmp_dir, "part-*.csv"))
if not part_files:
    raise RuntimeError("No part file found after Spark write.")
if os.path.exists(final_csv):
    os.remove(final_csv)
shutil.move(part_files[0], final_csv)
shutil.rmtree(tmp_dir, ignore_errors=True)

print("‚úÖ Cleaned dataset saved.")
print(f"üìÅ File location: {final_csv}")


```

# Salary by Education Level


```{python}
# --- Salary by Education Level -----------------------------------------------
from pyspark.sql import functions as F
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from IPython.display import Markdown, display

# --- 1. Identify the education column dynamically ---
edu_col = next(
    (c for c in ["EDUCATION_LEVELS_NAME", "MIN_EDULEVELS_NAME", "MAX_EDULEVELS_NAME", "EDUCATION_LEVELS"]
     if c in clean_df.columns),
    None
)

if edu_col is None:
    raise ValueError("No education-related column found in the dataset.")

# --- 2. Prepare a smaller sample for visualization ---
sdf = clean_df.select(
    F.col(edu_col).alias("EDUCATION"),
    "MAX_YEARS_EXPERIENCE",
    "AVERAGE_SALARY"
)

# --- 3. Convert to Pandas for visualization ---
pdf = sdf.sample(0.1, seed=42).toPandas()

# --- 4. Group education levels into two categories ---
def categorize_education(edu):
    if pd.isna(edu):
        return "Bachelor‚Äôs or lower"
    edu = edu.lower()
    if any(x in edu for x in ["bachelor", "associate", "ged", "no education"]):
        return "Bachelor‚Äôs or lower"
    elif any(x in edu for x in ["master", "phd", "doctor", "professional"]):
        return "Master‚Äôs or PhD"
    else:
        return "Bachelor‚Äôs or lower"

pdf["Education_Group"] = pdf["EDUCATION"].apply(categorize_education)

# --- 5. Helper functions for clean visuals and inline text ---
def explain(title, lines):
    """Render markdown explanations below each chart."""
    display(Markdown(f"**{title}**  \n" + "  \n".join(lines)))

def jitter(x, j=0.15):
    """Add small noise to reduce point overlap."""
    x = np.asarray(pd.Series(x).fillna(0))
    return x + np.random.uniform(-j, j, size=len(x))

# --- 6. Explanations for each group ---
scatter_notes = {
    "Bachelor‚Äôs or lower": [
        "Salaries increase slightly with experience, but most jobs cluster at low experience with wide pay variation.",
        "Several high-pay outliers at minimal experience suggest niche, high-demand roles."
    ],
    "Master‚Äôs or PhD": [
        "Advanced-degree roles show higher average salaries, especially beyond ~5 years of experience.",
        "Variation remains wide, indicating experience alone doesn‚Äôt fully explain pay differences."
    ],
}

hist_notes = {
    "Bachelor‚Äôs or lower": [
        "The distribution is strongly right-skewed with most postings in lower bands and a long high-pay tail.",
        "A spike near zero indicates missing or placeholder salaries that should be filtered for clean stats."
    ],
    "Master‚Äôs or PhD": [
        "The center of the distribution is higher than the bachelor‚Äôs group, reflecting an earnings premium.",
        "A longer upper tail suggests specialized or lead roles command top-end salaries."
    ],
}

# --- 7. Scatter plots with explanations ---
for group in pdf["Education_Group"].unique():
    subset = pdf[pdf["Education_Group"] == group]

    plt.figure(figsize=(8,5))
    plt.scatter(jitter(subset["MAX_YEARS_EXPERIENCE"]), subset["AVERAGE_SALARY"], s=15, alpha=0.6)
    plt.title(f"Salary vs. Experience ‚Äî {group}")
    plt.xlabel("Max Years Experience (jittered)")
    plt.ylabel("Average Salary")
    plt.tight_layout()
    plt.show()

    explain(f"Salary vs. Experience ‚Äî {group}", scatter_notes.get(group, [
        "Salaries show a weak positive relationship with experience.",
        "Wide dispersion suggests role and industry strongly influence pay."
    ]))

# --- 8. Histograms with explanations ---
for group in pdf["Education_Group"].unique():
    subset = pdf.loc[pdf["Education_Group"] == group, "AVERAGE_SALARY"].dropna()

    plt.figure(figsize=(8,5))
    counts, edges, _ = plt.hist(subset, bins=30, density=True, alpha=0.7)
    centers = (edges[:-1] + edges[1:]) / 2
    smooth = pd.Series(counts).rolling(window=3, center=True, min_periods=1).mean()
    plt.plot(centers, smooth, linewidth=2)
    plt.title(f"Salary Distribution ‚Äî {group}")
    plt.xlabel("Average Salary")
    plt.ylabel("Density")
    plt.tight_layout()
    plt.show()

    explain(f"Salary Distribution ‚Äî {group}", hist_notes.get(group, [
        "Right-skewed distribution with most values in lower bands.",
        "Upper tail indicates a subset of high-pay roles."
    ]))

```

# Salary by Remote Work Type


```{python}
# Salary by Remote Work Type 
from pyspark.sql import functions as F
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from IPython.display import Markdown, display

# --- 1. Check if REMOTE_TYPE_NAME exists ---
if "REMOTE_TYPE_NAME" not in clean_df.columns:
    raise ValueError("Column 'REMOTE_TYPE_NAME' not found in dataset.")

# --- 2. Create a simplified DataFrame for visualization ---
sdf = clean_df.select(
    "REMOTE_TYPE_NAME",
    "MAX_YEARS_EXPERIENCE",
    "AVERAGE_SALARY"
)

# --- 3. Convert to Pandas sample for plotting ---
pdf = sdf.sample(0.1, seed=42).toPandas()

# --- 4. Clean and group remote type labels ---
def categorize_remote(val):
    if pd.isna(val) or str(val).strip() == "":
        return "Onsite"
    val = val.lower()
    if "remote" in val:
        return "Remote"
    elif "hybrid" in val:
        return "Hybrid"
    else:
        return "Onsite"

pdf["Remote_Type_Group"] = pdf["REMOTE_TYPE_NAME"].apply(categorize_remote)

# --- 5. Helper functions for inline explanations ---
def explain(title, lines):
    display(Markdown(f"**{title}**  \n" + "  \n".join(lines)))

def jitter(x, j=0.15):
    x = np.asarray(pd.Series(x).fillna(0))
    return x + np.random.uniform(-j, j, size=len(x))

# --- 6. Define explanations ---
scatter_notes = {
    "Remote": [
        "Remote roles generally offer higher salaries across experience levels, reflecting competition for digital and distributed skills.",
        "Salary spread is wider for remote jobs, suggesting varying levels of specialization and seniority."
    ],
    "Hybrid": [
        "Hybrid positions show moderate salaries, often balancing flexibility with in-office expectations.",
        "Salaries increase gradually with experience, but few extreme outliers exist compared to remote roles."
    ],
    "Onsite": [
        "Onsite roles tend to cluster around lower-to-mid salary ranges with limited variation.",
        "Experience appears to have a smaller impact on salary for onsite work compared to remote or hybrid."
    ]
}

hist_notes = {
    "Remote": [
        "Remote salary distributions are slightly right-skewed, with many postings above the median salary level.",
        "This supports the idea that remote opportunities attract higher-paying, skill-intensive jobs."
    ],
    "Hybrid": [
        "Hybrid roles show a balanced salary distribution with fewer high-end positions.",
        "The curve suggests a steady middle-income concentration across job types."
    ],
    "Onsite": [
        "Onsite roles show the lowest median salaries and the strongest right-skew, with most salaries concentrated near the lower end.",
        "The distribution confirms that traditional in-person jobs tend to offer less pay than hybrid or remote options."
    ]
}

# --- 7. Scatter plots for each group ---
for group in pdf["Remote_Type_Group"].unique():
    subset = pdf[pdf["Remote_Type_Group"] == group]

    plt.figure(figsize=(8,5))
    plt.scatter(jitter(subset["MAX_YEARS_EXPERIENCE"]), subset["AVERAGE_SALARY"], s=15, alpha=0.6)
    plt.title(f"Salary vs. Experience ‚Äî {group}")
    plt.xlabel("Max Years Experience (jittered)")
    plt.ylabel("Average Salary")
    plt.tight_layout()
    plt.show()

    explain(f"Salary vs. Experience ‚Äî {group}", scatter_notes.get(group, [
        "Salaries show some relationship with experience.",
        "Variation across remote types suggests differences in job flexibility and demand."
    ]))

# --- 8. Histogram plots for each group ---
for group in pdf["Remote_Type_Group"].unique():
    subset = pdf.loc[pdf["Remote_Type_Group"] == group, "AVERAGE_SALARY"].dropna()

    plt.figure(figsize=(8,5))
    counts, edges, _ = plt.hist(subset, bins=30, density=True, alpha=0.7)
    centers = (edges[:-1] + edges[1:]) / 2
    smooth = pd.Series(counts).rolling(window=3, center=True, min_periods=1).mean()
    plt.plot(centers, smooth, linewidth=2)
    plt.title(f"Salary Distribution ‚Äî {group}")
    plt.xlabel("Average Salary")
    plt.ylabel("Density")
    plt.tight_layout()
    plt.show()

    explain(f"Salary Distribution ‚Äî {group}", hist_notes.get(group, [
        "The salary distribution is right-skewed with most values in lower ranges.",
        "Higher-paying roles appear less frequently but form the upper salary tail."
    ]))
```