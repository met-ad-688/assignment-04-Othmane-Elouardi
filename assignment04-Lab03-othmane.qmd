---
title: "Assignment 04 — Lab 03"
subtitle: "Regression Modeling on Employment Data"
author:
  - name: "Othmane Elouardi"
    affiliations:
      - id: bu
        name: "Boston University"
        city: "Boston"
        state: "MA"
date: "2025-10-08"
number-sections: true
format:
  html:
    theme:
      light: lux
      dark: slate
    toc: true
    toc-depth: 3
    toc-location: right
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-line-numbers: true
    highlight-style: a11y
    page-layout: article
    css: styles.css
    grid:
      body-width: 900px
      margin-width: 280px
execute:
  echo: true
  warning: false
  error: false
  freeze: auto
jupyter: env
---


# Introduction
The objective of this lab is to apply **PySpark** to perform regression modeling on employment data from the **Lightcast Job Postings** dataset. In this exercise, we use Spark to process and prepare a large dataset for salary prediction, engineer relevant features from structured columns, and train a **Linear Regression** model. We will evaluate model performance using **RMSE** and **R²**, visualize predictions through diagnostic plots, and conclude by pushing the completed analysis to GitHub for submission.

# Load the Dataset
 **PySpark** is used to load the *Lightcast Job Postings* dataset into a Spark DataFrame.  
This approach enables us to efficiently handle large datasets within the EC2 environment before proceeding with feature engineering and regression analysis.

```{python}
# Load the Lightcast dataset with PySpark

from pyspark.sql import SparkSession
import os

# Initialize Spark session 
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .config("spark.driver.memory", "1g")
    .getOrCreate()
)

# Define dataset path
csv_path = "data/lightcast_job_postings.csv"

# Check path validity
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"❌ Could not find {csv_path}. Please ensure the file exists in the data/ folder.")

# Load dataset
df = (
    spark.read
    .option("header", "true")       # First row as headers
    .option("inferSchema", "true")  # Auto-detect data types
    .option("multiLine", "true")    # Handle multi-line text fields
    .option("escape", "\"")         # Handle embedded quotes
    .csv(csv_path)
)

# Confirm load success
print("✅ Dataset successfully loaded!")
df.printSchema()
df.show(5, truncate=False)
```


# Feature Engineering

Feature Engineering is a crucial step in preparing data for machine learning:

1. **Drop rows with missing values** in the target and key features.  
2. **Choose 3 predictors** (2 continuous + 1 categorical) and the dependent variable **salary**.  
3. **Encode** categorical variables using `StringIndexer` + `OneHotEncoder`.  
4. **Assemble** features into a single vector using `VectorAssembler`.  
5. **Split** the data into training and testing sets.

```{python}
# Feature Engineering
from pyspark.sql.functions import col, countDistinct
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler

# Display columns and data types for reference
print("Columns and data types:")
for name, dtype in df.dtypes:
    print(f" - {name}: {dtype}")

# Helper to pick the first available column from a list
def first_existing(candidates, schema_cols):
    for c in candidates:
        if c in schema_cols:
            return c
    return None

# Extract lists of column names by type
schema_cols = [c for c, _ in df.dtypes]
num_cols = [c for c, t in df.dtypes if t in ("int", "double", "float")]
str_cols = [c for c, t in df.dtypes if t == "string"]

# Define the target column (salary)
target = first_existing(["salary", "SALARY", "avg_salary", "median_salary"], schema_cols)
if not target:
    raise ValueError("❌ No salary column found. Please verify dataset headers.")

# Choose continuous variables
cont1 = first_existing(
    ["min_years_experience", "MIN_YEARS_EXPERIENCE", "experience_min", "required_experience_min"],
    schema_cols
)
cont2 = first_existing(
    ["max_years_experience", "MAX_YEARS_EXPERIENCE", "experience_max", "required_experience_max"],
    schema_cols
)

# Fallback continuous columns if none found
avoid = {target, "ID", "id", "DUPLICATES", "duplicates"}
fallback_nums = [c for c in num_cols if c not in avoid]
if cont1 is None and fallback_nums:
    cont1 = fallback_nums[0]
if cont2 is None and len(fallback_nums) > 1:
    cont2 = fallback_nums[1]

# Choose categorical variable
cat1 = first_existing(
    [
        "remote_type_name", "REMOTE_TYPE_NAME", "employment_type", "EMPLOYMENT_TYPE",
        "education_level", "EDUCATION_LEVEL", "experience_level", "EXPERIENCE_LEVEL",
        "work_type", "WORK_TYPE", "state", "STATE"
    ],
    schema_cols
)

# Fallback: pick a low-cardinality string column automatically
if cat1 is None:
    for c in str_cols[:25]:
        try:
            k = df.select(countDistinct(col(c)).alias("k")).collect()[0]["k"]
            if 2 <= k <= 30:
                cat1 = c
                print(f"✅ Auto-selected categorical: {cat1} (distinct={k})")
                break
        except Exception:
            pass

# Validate selections
chosen_cont = [c for c in [cont1, cont2] if c]
if len(chosen_cont) < 2 or cat1 is None:
    raise ValueError(
        f"⚠️ Not enough predictors found.\n"
        f"Target: {target}\n"
        f"Continuous: {chosen_cont}\n"
        f"Categorical: {cat1}\n"
        "Please check column names above and manually assign predictors."
    )

print("✅ Selected columns:")
print("  Target (y):", target)
print("  Continuous (x1, x2):", chosen_cont)
print("  Categorical (x3):", cat1)

# Drop rows with missing values
cols_needed = [target] + chosen_cont + [cat1]
df_clean = df.select(*cols_needed).dropna(subset=cols_needed)
print(f"Rows before cleaning: {df.count():,} | after cleaning: {df_clean.count():,}")

# Encode categorical variable
idx_col = f"{cat1}_idx"
oh_col = f"{cat1}_oh"
indexer = StringIndexer(inputCol=cat1, outputCol=idx_col, handleInvalid="keep")
encoder = OneHotEncoder(inputCols=[idx_col], outputCols=[oh_col])

df_indexed = indexer.fit(df_clean).transform(df_clean)
df_encoded = encoder.fit(df_indexed).transform(df_indexed)

# Assemble final features
assembler = VectorAssembler(inputCols=chosen_cont + [oh_col], outputCol="features")
df_fe = assembler.transform(df_encoded).select(col(target).alias("label"), "features")

print("✅ Schema after feature engineering:")
df_fe.printSchema()
df_fe.show(5, truncate=False)

```

# Train/Test Split
A random split was performed to ensure a balanced and representative sample of the data.  
A fixed random seed was used for reproducibility. In this lab, an **80 % – 20 %** split was applied, which is a standard proportion providing sufficient data for training while retaining a meaningful portion for testing and evaluation.

```{python}
# Train/Test Split
# Perform an 80/20 random split of the dataset with a fixed random seed for reproducibility.

train_df, test_df = df_fe.randomSplit([0.8, 0.2], seed=42)

# Display sizes of train and test sets
print(f"Training set: {train_df.count():,} rows")
print(f"Testing set:  {test_df.count():,} rows")

# Print tuple format for clarity and comparison with lab PDF
print(f"({train_df.count()}, {len(train_df.columns)})")
print(f"({test_df.count()}, {len(test_df.columns)})")

```


# Linear Regression
A linear regression model was fit on the training set using the normal solver to enable coefficient inference.  
Model performance was evaluated on both the training and testing sets, and the coefficient table was reported with standard errors, t-values, p-values, and 95% confidence intervals.




```{python}
# Linear Regression 

from pyspark.ml.regression import LinearRegression
import pandas as pd, numpy as np, os

# 0) Require engineered features
if "df_fe" not in globals():
    raise RuntimeError("df_fe not found. Run the Feature Engineering section first.")

# 1) Ensure train/test exist
if "train_df" not in globals() or "test_df" not in globals():
    train_df, test_df = df_fe.randomSplit([0.8, 0.2], seed=42)

# 2) Fit model (normal solver for inference)
if "lr_model" not in globals():
    lr = LinearRegression(featuresCol="features", labelCol="label",
                          predictionCol="prediction", solver="normal",
                          regParam=0.0, elasticNetParam=0.0)
    lr_model = lr.fit(train_df)

# 3) Metrics
train_sum = lr_model.summary
test_sum  = lr_model.evaluate(test_df)

print("=== Training Metrics ===")
print("R²:", train_sum.r2)
print("RMSE:", train_sum.rootMeanSquaredError)
print("MAE:", train_sum.meanAbsoluteError)

print("\n=== Test Metrics ===")
print("R²:", test_sum.r2)
print("RMSE:", test_sum.rootMeanSquaredError)
print("MAE:", test_sum.meanAbsoluteError)

# 4) Safe getters for inference arrays (may be None / include intercept)
def safe(getter):
    try:
        return getter()
    except Exception:
        return None

se_all = safe(lambda: train_sum.coefficientStandardErrors)
t_all  = safe(lambda: train_sum.tValues)
p_all  = safe(lambda: train_sum.pValues)

# 5) Recover feature names from VectorAssembler metadata
def get_feature_names(df, features_col="features", n=None):
    meta = df.schema[features_col].metadata
    names = []
    if "ml_attr" in meta and "attrs" in meta["ml_attr"]:
        for k in ("binary","numeric"):
            for a in sorted(meta["ml_attr"]["attrs"].get(k, []), key=lambda x: x["idx"]):
                names.append(a["name"])
    return names if names else [f"f{i}" for i in range(n or 0)]

coefs = lr_model.coefficients.toArray()
names = get_feature_names(df_fe, n=len(coefs))

def split_intercept(arr, n_feat):
    if arr is None: return (None, None)
    if len(arr) == n_feat + 1: return (arr[0], arr[1:])
    if len(arr) == n_feat:     return (None, arr)
    return (None, None)

se_int, se_feat = split_intercept(se_all, len(coefs))
t_int,  t_feat  = split_intercept(t_all,  len(coefs))
p_int,  p_feat  = split_intercept(p_all,  len(coefs))

# 6) Build and save coefficients table (works with or without inference)
feat_df = pd.DataFrame({
    "feature": names,
    "coef": coefs,
    "std_err": se_feat if se_feat is not None else [None]*len(coefs),
    "t_value": t_feat if t_feat is not None else [None]*len(coefs),
    "p_value": p_feat if p_feat is not None else [None]*len(coefs),
})

if se_feat is not None:
    se_arr = np.asarray(se_feat, dtype=float)
    feat_df["ci_lower_95"] = coefs - 1.96 * se_arr
    feat_df["ci_upper_95"] = coefs + 1.96 * se_arr
else:
    feat_df["ci_lower_95"] = None
    feat_df["ci_upper_95"] = None

coef_df = pd.concat([
    pd.DataFrame([{
        "feature": "(Intercept)",
        "coef": lr_model.intercept,
        "std_err": se_int,
        "t_value": t_int,
        "p_value": p_int,
        "ci_lower_95": None,
        "ci_upper_95": None
    }]),
    feat_df
], ignore_index=True)

os.makedirs("_output", exist_ok=True)
coef_path = "_output/lr_coefficients_fullrank.csv"
coef_df.to_csv(coef_path, index=False)
print(f"\nSaved coefficients table to {coef_path}")


```

## Generalized Linear Regression Summary



```{python}
# ========================= Generalized Linear Regression Summary (FULL REPLACEMENT) =========================
# Robust GLR (log-salary) with label auto-detection, safe parsing, Boolean categorical fix,
# aggressive back-off, and clear diagnostics.

# ---------------------------------- Imports ----------------------------------
from pyspark.sql import functions as F
from pyspark.sql.functions import col, when, lit, stddev_samp, length
from pyspark.sql.types import BooleanType
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml.regression import GeneralizedLinearRegression
import numpy as np, pandas as pd, os

try:
    from IPython.display import display, HTML
    _CAN_HTML = True
except Exception:
    _CAN_HTML = False

# ---------------------------------- Preconditions ----------------------------------
try:
    spark  # noqa
except NameError:
    raise RuntimeError("SparkSession 'spark' is not defined. Create it earlier in your notebook/script.")

try:
    df  # noqa
except NameError:
    raise RuntimeError("Base DataFrame 'df' is not defined. Load your dataset into 'df' earlier.")

# ---------------------------------- Safe numeric parsers ----------------------------------
def parse_scalar_expr(c):
    # Extract first numeric token; handle 'k' = thousand
    expr = f"try_cast(regexp_replace(regexp_extract(`{c}`,'([-+]?\\d[\\d,]*\\.?\\d*)',1), ',', '') AS double)"
    num = F.expr(expr)
    has_k = F.lower(F.col(c)).like('%k%')
    return F.when(has_k & num.isNotNull(), num * 1000.0).otherwise(num)

def parse_range_mid_expr(c):
    # Mid-point for ranges like "80k - 100k"
    lo = F.expr(
        f"try_cast(regexp_replace(regexp_extract(`{c}`,'([-+]?\\d[\\d,]*\\.?\\d*)\\s*[-–—]\\s*[-+]?\\d',1), ',', '') AS double)"
    )
    hi = F.expr(
        f"try_cast(regexp_replace(regexp_extract(`{c}`,'[-–—]\\s*([-+]?\\d[\\d,]*\\.?\\d*)',1), ',', '') AS double)"
    )
    has_k = F.lower(F.col(c)).like('%k%')
    lo = F.when(has_k & lo.isNotNull(), lo * 1000.0).otherwise(lo)
    hi = F.when(has_k & hi.isNotNull(), hi * 1000.0).otherwise(hi)
    return (lo + hi) / 2.0

def parse_numeric(c):
    mid = parse_range_mid_expr(c)
    sc  = parse_scalar_expr(c)
    return F.when(mid.isNotNull(), mid).when(sc.isNotNull(), sc).otherwise(F.lit(None).cast('double'))

# ---------------------------------- Label auto-detection ----------------------------------
def autodetect_label_expr(df_in):
    """Pick the best salary/pay/wage/compensation column(s) by maximizing kept non-null, positive rows."""
    import re
    cols = df_in.columns

    def looks_like_money(name):
        n = name.lower()
        return any(k in n for k in ["salary", "pay", "wage", "compensat", "income", "earn"])

    # Pair candidates: *_FROM with *_TO (or *_Min/_Max etc.)
    cand_pairs = []
    for c in cols:
        if looks_like_money(c) and c.endswith(("_FROM", "_Min", "_LOW", "_LOWER", "_MIN")):
            stem = re.sub(r"(_FROM|_Min|_LOW|_LOWER|_MIN)$", "", c)
            for mate in [stem + "_TO", stem + "_Max", stem + "_HIGH", stem + "_UPPER", stem + "_MAX"]:
                if mate in cols:
                    cand_pairs.append((c, mate))

    # Evaluate pairs
    best = None
    best_n = -1
    for lo, hi in cand_pairs:
        tmp = (df_in
               .select(parse_numeric(lo).alias("lo"), parse_numeric(hi).alias("hi"))
               .withColumn("m", (F.col("lo") + F.col("hi")) / 2.0)
               .where(F.col("m").isNotNull())
               .where(~F.isnan("m"))
               .where(F.col("m") > 0))
        n = tmp.count()
        if n > best_n:
            best_n = n
            best = (lo, hi)
    if best is not None and best_n > 0:
        lo, hi = best
        return (parse_numeric(lo) + parse_numeric(hi)) / 2.0, f"{lo}+{hi}"

    # Single-column candidates
    singles = [c for c in cols if looks_like_money(c)]
    best_c, best_n = None, -1
    for c in singles:
        tmp = (df_in
               .select(parse_numeric(c).alias("v"))
               .where(F.col("v").isNotNull())
               .where(~F.isnan("v"))
               .where(F.col("v") > 0))
        n = tmp.count()
        if n > best_n:
            best_n, best_c = n, c
    if best_c and best_n > 0:
        return parse_numeric(best_c), best_c

    # Last resort: numeric SALARY column if present
    for c, t in df_in.dtypes:
        if c.upper() == "SALARY" and t in ("double", "float", "int", "bigint"):
            return F.col(c).cast("double"), c
    return None, None

# ---------------------------------- Build label & diagnostics ----------------------------------
label_expr, label_source = autodetect_label_expr(df)
if label_expr is None:
    if "SALARY" in df.columns:
        label_expr, label_source = parse_numeric("SALARY"), "SALARY"
    elif set(["SALARY_FROM", "SALARY_TO"]).issubset(df.columns):
        label_expr, label_source = (parse_numeric("SALARY_FROM") + parse_numeric("SALARY_TO")) / 2.0, "SALARY_FROM+SALARY_TO"
    elif "SALARY_TO" in df.columns:
        label_expr, label_source = parse_numeric("SALARY_TO"), "SALARY_TO"
    else:
        label_expr, label_source = parse_numeric("SALARY_FROM"), "SALARY_FROM"

df = (df.withColumn("label", label_expr)
        .where(F.col("label").isNotNull())
        .where(~F.isnan("label"))
        .where(F.col("label") > 0))
df = df.withColumn("label_log", F.log1p(F.col("label")))
n_label_rows = df.count()
print(f"✅ label & label_log ready | label source: {label_source} | rows after label filter: {n_label_rows:,}")
if n_label_rows == 0:
    raise RuntimeError("All rows dropped by label parsing. Inspect your dataset headers and salary/pay/wage columns.")

# ---------------------------------- Engineer numeric features ----------------------------------
num_specs = [
    ("MIN_YEARS_EXPERIENCE", "MIN_YEARS_EXPERIENCE_num"),
    ("MAX_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE_num"),
    ("MODELED_DURATION",     "MODELED_DURATION_num"),
    ("SALARY_TO",            "SALARY_TO_num"),
]
for raw, out in num_specs:
    if raw in df.columns:
        df = df.withColumn(out, parse_numeric(raw))

# z-score available numerics
for _, out in num_specs:
    if out in df.columns:
        s = df.select(F.mean(out).alias("mu"), F.stddev_samp(out).alias("sd")).first()
        mu = float(s.mu) if s.mu is not None else 0.0
        sd = float(s.sd) if s.sd not in (None, 0.0) else 1.0
        df = df.withColumn(f"z_{out}", (col(out) - lit(mu)) / lit(sd))

numeric_cols = [c for c in [f"z_{n}" for _, n in num_specs] if c in df.columns]
if not numeric_cols:
    # Fallback numeric from text length
    for t in ["TITLE", "TITLE_NAME", "TITLE_RAW", "BODY"]:
        if t in df.columns:
            df = df.withColumn("TITLE_LEN_num", length(col(t)).cast("double"))
            numeric_cols = ["TITLE_LEN_num"]
            break

# ---------------------------------- Candidate categoricals ----------------------------------
cat_raw = [c for c in ["REMOTE_TYPE_NAME", "EMPLOYMENT_TYPE_NAME", "EDUCATION_LEVELS_NAME", "COMPANY_IS_STAFFING"] if c in df.columns]

# Ensure boolean categoricals are strings and fill nulls (StringIndexer requires string or numeric)
for c in list(cat_raw):
    if c in df.columns:
        dt = df.schema[c].dataType
        if isinstance(dt, BooleanType):
            df = df.withColumn(c, F.when(F.col(c).isNull(), F.lit("Unknown"))
                                 .otherwise(F.col(c).cast("string")))
        else:
            # also fill nulls for existing strings
            if "StringType" in str(dt):
                df = df.withColumn(c, F.when(F.col(c).isNull(), F.lit("Unknown")).otherwise(F.col(c)))

# Quick diagnostics
def nn_counts(df_in, cols):
    out = {}
    for c in cols:
        if c in df_in.columns:
            out[c] = df_in.where(F.col(c).isNotNull() & ~F.isnan(c)).count()
    return out

print("ℹ️ Non-null counts (numerics):", nn_counts(df, numeric_cols))
print("ℹ️ Distinct levels (categoricals):", {c: df.select(F.countDistinct(F.col(c))).first()[0] for c in cat_raw})

# ---------------------------------- Helpers: cap/encode/impute/assemble/fit ----------------------------------
def cap_top_k(df_in, c, k):
    top = (df_in.groupBy(c).count().orderBy(F.desc("count")).limit(max(k - 1, 1))
           .select(c).rdd.flatMap(lambda r: r).collect())
    keep = set([v for v in top if v is not None])
    cc = f"{c}_top"
    out = df_in.withColumn(cc, F.when(col(c).isin(list(keep)), col(c)).otherwise(lit("Other")))
    return out, cc

def encode_cats(df_in, cols, k):
    out = df_in
    oh_cols = []
    for c in cols:
        if c not in out.columns:
            continue
        # cap top-k then index + one-hot (degenerate cats skipped)
        out, capped = cap_top_k(out, c, k)
        if out.select(capped).distinct().count() < 2:
            continue
        idx, oh = f"{capped}_idx", f"{capped}_oh"
        out = StringIndexer(inputCol=capped, outputCol=idx, handleInvalid="keep").fit(out).transform(out)
        out = OneHotEncoder(inputCols=[idx], outputCols=[oh], dropLast=True).fit(out).transform(out)
        oh_cols.append(oh)
    return out, oh_cols

def impute_median(df_in, cols):
    out = df_in
    for c in cols:
        if c in out.columns:
            med = out.approxQuantile(c, [0.5], 1e-3)[0] if out.where(F.col(c).isNotNull()).count() else 0.0
            out = out.withColumn(c, when(col(c).isNull() | F.isnan(c), lit(med)).otherwise(col(c)))
    return out

def assemble(df_in, y_col, numeric, cats, k):
    tmp, ohe_cols = encode_cats(df_in, cats, k) if cats else (df_in, [])
    num_keep_local = [c for c in numeric if c in tmp.columns]
    tmp = impute_median(tmp, num_keep_local)
    inputs = num_keep_local + ohe_cols
    if not inputs:
        return None, [], 0
    fe = (VectorAssembler(inputCols=inputs, outputCol="features")
          .transform(tmp)
          .select(F.col(y_col).alias(y_col), "features")
          .where(F.col("features").isNotNull()))
    # derive feature count from metadata
    meta = fe.schema["features"].metadata
    p = 0
    if "ml_attr" in meta and "attrs" in meta["ml_attr"]:
        for k2 in ("binary", "numeric"):
            p += len(meta["ml_attr"]["attrs"].get(k2, []))
    return fe, inputs, p

def fit_glr(df_in, y_col, reg=1e-3, fitIntercept=True):
    glr = GeneralizedLinearRegression(
        featuresCol="features", labelCol=y_col,
        family="gaussian", link="identity",
        regParam=reg, fitIntercept=fitIntercept
    )
    m = glr.fit(df_in)
    return m, m.summary

# ---------------------------------- DoF-controlled fit (robust) ----------------------------------
label_col = "label_log"
df_work = df.where(F.col(label_col).isNotNull()).where(~F.isnan(F.col(label_col)))

n_label = df_work.count()
min_dof = 8 if n_label >= 200 else 4 if n_label >= 50 else 2
reg = 1e-3
initial_topk, min_topk = 8, 2
topk = initial_topk
num_keep = list(numeric_cols)
tried_numeric_only = False
injected_title_len = False

attempts = 0
while True:
    attempts += 1
    df_fe, inputs, p = assemble(df_work, label_col, num_keep, (cat_raw if not tried_numeric_only else []), topk)

    # If assembler had no inputs -> try numeric-only, then inject TITLE_LEN
    if df_fe is None or not inputs:
        if not tried_numeric_only:
            tried_numeric_only = True
            topk = min_topk
            continue
        if not injected_title_len:
            for t in ["TITLE", "TITLE_NAME", "TITLE_RAW", "BODY"]:
                if t in df_work.columns:
                    df_work = df_work.withColumn("TITLE_LEN_num", F.length(F.col(t)).cast("double"))
                    num_keep = list(set(num_keep + ["TITLE_LEN_num"]))
                    injected_title_len = True
                    break
            continue
        raise RuntimeError("No usable features available to assemble.")

    n_eff = df_fe.count()

    # If we lost all rows, try backing off before erroring
    if n_eff == 0:
        print("⚠️ 0 rows after assemble → backing off features…")
        if not tried_numeric_only:
            tried_numeric_only = True
            topk = min_topk
            continue
        if not injected_title_len:
            for t in ["TITLE", "TITLE_NAME", "TITLE_RAW", "BODY"]:
                if t in df_work.columns:
                    df_work = df_work.withColumn("TITLE_LEN_num", F.length(F.col(t)).cast("double"))
                    num_keep = list(set(num_keep + ["TITLE_LEN_num"]))
                    injected_title_len = True
                    break
            continue
        if topk > min_topk:
            topk = max(min_topk, topk // 2)
            continue
        print("Inputs used:", inputs[:12], "..." if len(inputs) > 12 else "")
        raise RuntimeError("No rows left after preparation. Check label parsing and input columns.")

    dof = n_eff - (p + 1)
    print(f"[Attempt {attempts}] rows={n_eff:,} | features≈{p:,} | DoF={dof:,} | topK={topk} | numerics={len(num_keep)} | numeric_only={tried_numeric_only}")

    if (dof > min_dof and p > 0):
        try:
            model, summary = fit_glr(df_fe, label_col, reg=reg, fitIntercept=True)
            print("✅ Model succeeded | DoF:", dof, "| features≈", p, "| topK:", topk, "| numeric_only=", tried_numeric_only)
            print("Using predictors:", inputs)
            break
        except Exception as e:
            print("  Fit failed → shrinking:", str(e)[:160])

    # Shrink path
    if topk > min_topk and not tried_numeric_only:
        topk = max(min_topk, topk // 2)
        continue
    if len(num_keep) > 1:
        sd_map = df_work.agg(*[stddev_samp(c).alias(c) for c in num_keep]).first().asDict()
        num_keep = sorted(num_keep, key=lambda c: float(sd_map.get(c) or 0.0), reverse=True)[:-1]
        continue
    if not tried_numeric_only:
        tried_numeric_only = True
        topk = min_topk
        continue
    if dof > 1 and p > 0:
        try:
            model, summary = fit_glr(df_fe, label_col, reg=max(reg, 1e-2), fitIntercept=True)
            print("⚠️ Proceeding with relaxed fit (tiny data).")
            break
        except Exception as e:
            print("  Final fit failed:", str(e)[:160])
            try:
                model, summary = fit_glr(df_fe, label_col, reg=1e-1, fitIntercept=True)
                print("⚠️ Proceeding with stronger regularization.")
                break
            except Exception as e2:
                print("  Ridge(0.1) failed:", str(e2)[:160])
                raise
    raise RuntimeError(f"Could not reach a stable fit (rows={n_eff}, features≈{p}). Consider fewer predictors or verifying label parsing.")

# ---------------------------------- Named coefficient table ----------------------------------
def feature_names_from_meta(df_in):
    meta = df_in.schema["features"].metadata
    names = []
    if "ml_attr" in meta and "attrs" in meta["ml_attr"]:
        for k in ("binary", "numeric"):
            names += [a["name"] for a in sorted(meta["ml_attr"]["attrs"].get(k, []), key=lambda a: a["idx"])]
    return names

names = feature_names_from_meta(df_fe)
coef  = np.array(model.coefficients.toArray())
intercept = float(model.intercept)

def safe_arr(getter, size):
    try:
        arr = np.array(getter())
        if len(arr) == size + 1:  # intercept included at end
            return np.r_[arr[-1], arr[:-1]]
        if len(arr) == size:
            return np.r_[np.nan, arr]
        return np.r_[arr, [np.nan] * ((size + 1) - len(arr))]
    except Exception:
        return np.r_[np.nan, [np.nan] * size]

se = safe_arr(lambda: summary.coefficientStandardErrors, len(coef))
tv = safe_arr(lambda: summary.tValues,                  len(coef))
try:
    _ = summary.residualDegreeOfFreedom
    pv = safe_arr(lambda: summary.pValues, len(coef))
except Exception:
    pv = np.r_[np.nan, [np.nan] * len(coef)]

table = pd.DataFrame({
    "Feature":  ["(Intercept)"] + names,
    "Estimate": np.r_[intercept, coef],
    "Std Error": se,
    "t-stat":   tv,
    "P-Value":  pv
})

# Pretty print
if _CAN_HTML:
    pretty = table.copy()
    for c in ["Estimate", "Std Error", "t-stat", "P-Value"]:
        pretty[c] = pretty[c].apply(lambda x: f"{x:,.4f}" if pd.notnull(x) and np.isfinite(x) else "")
    display(HTML(pretty.to_html(index=False, classes="table table-striped table-sm", border=0)))
else:
    print(table.head(15).to_string(index=False))

print("\n=== GLR (log-salary) SUMMARY ===")
print("Residual DoF:", getattr(summary, "residualDegreeOfFreedom", None))
print("Null Deviance:", getattr(summary, "nullDeviance", None))
print("Residual Deviance:", getattr(summary, "deviance", None))
print("AIC:", getattr(summary, "aic", None))

# Save CSV
os.makedirs("_output", exist_ok=True)
out_csv = "_output/glr_coefficients_named.csv"
table.to_csv(out_csv, index=False)
print(f"📁 Saved table to {out_csv}")
# =================================================================================================

```

### Interpretation
The Generalized Linear Regression (GLR) model provided a thorough analysis of salary patterns from 30,808 job postings. The results indicate a strong model fit, with a Residual Deviance of 4505 compared to a Null Deviance of 5137. This suggests that the model has meaningful explanatory power. The AIC value of 28,245 further supports the conclusion that the model effectively identifies key salary determinants.
The analysis revealed that experience-related variables and company type were the most significant factors affecting salaries. Higher required experience levels were associated with increased predicted salaries, while positions offered by staffing companies tended to have notably lower salary estimates. Education and employment type did not contribute as consistently, with many categories lacking strong statistical significance. Overall, these findings imply that employers value professional experience and the characteristics of the company more than formal education or the type of job when determining salary levels.


