---
title: "Assignment 04 — Lab 03"
subtitle: "Regression Modeling on Employment Data"
author:
  - name: "Othmane Elouardi"
    affiliations:
      - id: bu
        name: "Boston University"
        city: "Boston"
        state: "MA"
date: "2025-10-08"
number-sections: true
format:
  html:
    theme:
      light: lux
      dark: slate
    toc: true
    toc-depth: 3
    toc-location: right
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-line-numbers: true
    highlight-style: a11y
    page-layout: article
    css: styles.css
    grid:
      body-width: 900px
      margin-width: 280px
execute:
  echo: true
  warning: false
  error: false
  freeze: auto
jupyter: env
---


# Introduction
The objective of this lab is to apply **PySpark** to perform regression modeling on employment data from the **Lightcast Job Postings** dataset. In this exercise, we use Spark to process and prepare a large dataset for salary prediction, engineer relevant features from structured columns, and train a **Linear Regression** model. We will evaluate model performance using **RMSE** and **R²**, visualize predictions through diagnostic plots, and conclude by pushing the completed analysis to GitHub for submission.

# Load the Dataset
 **PySpark** is used to load the *Lightcast Job Postings* dataset into a Spark DataFrame.  
This approach enables us to efficiently handle large datasets within the EC2 environment before proceeding with feature engineering and regression analysis.

```{python}
# Load the Lightcast dataset with PySpark

from pyspark.sql import SparkSession
import os

# Initialize Spark session 
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .config("spark.driver.memory", "1g")
    .getOrCreate()
)

# Define dataset path
csv_path = "data/lightcast_job_postings.csv"

# Check path validity
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"❌ Could not find {csv_path}. Please ensure the file exists in the data/ folder.")

# Load dataset
df = (
    spark.read
    .option("header", "true")       # First row as headers
    .option("inferSchema", "true")  # Auto-detect data types
    .option("multiLine", "true")    # Handle multi-line text fields
    .option("escape", "\"")         # Handle embedded quotes
    .csv(csv_path)
)

# Confirm load success
print("✅ Dataset successfully loaded!")
df.printSchema()
df.show(5, truncate=False)
```


# Feature Engineering

Feature Engineering is a crucial step in preparing data for machine learning:

1. **Drop rows with missing values** in the target and key features.  
2. **Choose 3 predictors** (2 continuous + 1 categorical) and the dependent variable **salary**.  
3. **Encode** categorical variables using `StringIndexer` + `OneHotEncoder`.  
4. **Assemble** features into a single vector using `VectorAssembler`.  
5. **Split** the data into training and testing sets.

```{python}
# Feature Engineering
from pyspark.sql.functions import col, countDistinct
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler

# Display columns and data types for reference
print("Columns and data types:")
for name, dtype in df.dtypes:
    print(f" - {name}: {dtype}")

# Helper to pick the first available column from a list
def first_existing(candidates, schema_cols):
    for c in candidates:
        if c in schema_cols:
            return c
    return None

# Extract lists of column names by type
schema_cols = [c for c, _ in df.dtypes]
num_cols = [c for c, t in df.dtypes if t in ("int", "double", "float")]
str_cols = [c for c, t in df.dtypes if t == "string"]

# Define the target column (salary)
target = first_existing(["salary", "SALARY", "avg_salary", "median_salary"], schema_cols)
if not target:
    raise ValueError("❌ No salary column found. Please verify dataset headers.")

# Choose continuous variables
cont1 = first_existing(
    ["min_years_experience", "MIN_YEARS_EXPERIENCE", "experience_min", "required_experience_min"],
    schema_cols
)
cont2 = first_existing(
    ["max_years_experience", "MAX_YEARS_EXPERIENCE", "experience_max", "required_experience_max"],
    schema_cols
)

# Fallback continuous columns if none found
avoid = {target, "ID", "id", "DUPLICATES", "duplicates"}
fallback_nums = [c for c in num_cols if c not in avoid]
if cont1 is None and fallback_nums:
    cont1 = fallback_nums[0]
if cont2 is None and len(fallback_nums) > 1:
    cont2 = fallback_nums[1]

# Choose categorical variable
cat1 = first_existing(
    [
        "remote_type_name", "REMOTE_TYPE_NAME", "employment_type", "EMPLOYMENT_TYPE",
        "education_level", "EDUCATION_LEVEL", "experience_level", "EXPERIENCE_LEVEL",
        "work_type", "WORK_TYPE", "state", "STATE"
    ],
    schema_cols
)

# Fallback: pick a low-cardinality string column automatically
if cat1 is None:
    for c in str_cols[:25]:
        try:
            k = df.select(countDistinct(col(c)).alias("k")).collect()[0]["k"]
            if 2 <= k <= 30:
                cat1 = c
                print(f"✅ Auto-selected categorical: {cat1} (distinct={k})")
                break
        except Exception:
            pass

# Validate selections
chosen_cont = [c for c in [cont1, cont2] if c]
if len(chosen_cont) < 2 or cat1 is None:
    raise ValueError(
        f"⚠️ Not enough predictors found.\n"
        f"Target: {target}\n"
        f"Continuous: {chosen_cont}\n"
        f"Categorical: {cat1}\n"
        "Please check column names above and manually assign predictors."
    )

print("✅ Selected columns:")
print("  Target (y):", target)
print("  Continuous (x1, x2):", chosen_cont)
print("  Categorical (x3):", cat1)

# Drop rows with missing values
cols_needed = [target] + chosen_cont + [cat1]
df_clean = df.select(*cols_needed).dropna(subset=cols_needed)
print(f"Rows before cleaning: {df.count():,} | after cleaning: {df_clean.count():,}")

# Encode categorical variable
idx_col = f"{cat1}_idx"
oh_col = f"{cat1}_oh"
indexer = StringIndexer(inputCol=cat1, outputCol=idx_col, handleInvalid="keep")
encoder = OneHotEncoder(inputCols=[idx_col], outputCols=[oh_col])

df_indexed = indexer.fit(df_clean).transform(df_clean)
df_encoded = encoder.fit(df_indexed).transform(df_indexed)

# Assemble final features
assembler = VectorAssembler(inputCols=chosen_cont + [oh_col], outputCol="features")
df_fe = assembler.transform(df_encoded).select(col(target).alias("label"), "features")

print("✅ Schema after feature engineering:")
df_fe.printSchema()
df_fe.show(5, truncate=False)

```

# Train/Test Split
A random split was performed to ensure a balanced and representative sample of the data.  
A fixed random seed was used for reproducibility. In this lab, an **80 % – 20 %** split was applied, which is a standard proportion providing sufficient data for training while retaining a meaningful portion for testing and evaluation.

```{python}
# Train/Test Split
# Perform an 80/20 random split of the dataset with a fixed random seed for reproducibility.

train_df, test_df = df_fe.randomSplit([0.8, 0.2], seed=42)

# Display sizes of train and test sets
print(f"Training set: {train_df.count():,} rows")
print(f"Testing set:  {test_df.count():,} rows")

# Print tuple format for clarity and comparison with lab PDF
print(f"({train_df.count()}, {len(train_df.columns)})")
print(f"({test_df.count()}, {len(test_df.columns)})")

```


# Linear Regression
A linear regression model was fit on the training set using the normal solver to enable coefficient inference.  
Model performance was evaluated on both the training and testing sets, and the coefficient table was reported with standard errors, t-values, p-values, and 95% confidence intervals.

```{python}
# Linear Regression 

from pyspark.ml.regression import LinearRegression
import pandas as pd, numpy as np, os

# 0) Require engineered features
if "df_fe" not in globals():
    raise RuntimeError("df_fe not found. Run the Feature Engineering section first.")

# 1) Ensure train/test exist
if "train_df" not in globals() or "test_df" not in globals():
    train_df, test_df = df_fe.randomSplit([0.8, 0.2], seed=42)

# 2) Fit model (normal solver for inference)
if "lr_model" not in globals():
    lr = LinearRegression(featuresCol="features", labelCol="label",
                          predictionCol="prediction", solver="normal",
                          regParam=0.0, elasticNetParam=0.0)
    lr_model = lr.fit(train_df)

# 3) Metrics
train_sum = lr_model.summary
test_sum  = lr_model.evaluate(test_df)

print("=== Training Metrics ===")
print("R²:", train_sum.r2)
print("RMSE:", train_sum.rootMeanSquaredError)
print("MAE:", train_sum.meanAbsoluteError)

print("\n=== Test Metrics ===")
print("R²:", test_sum.r2)
print("RMSE:", test_sum.rootMeanSquaredError)
print("MAE:", test_sum.meanAbsoluteError)

# 4) Safe getters for inference arrays (may be None / include intercept)
def safe(getter):
    try:
        return getter()
    except Exception:
        return None

se_all = safe(lambda: train_sum.coefficientStandardErrors)
t_all  = safe(lambda: train_sum.tValues)
p_all  = safe(lambda: train_sum.pValues)

# 5) Recover feature names from VectorAssembler metadata
def get_feature_names(df, features_col="features", n=None):
    meta = df.schema[features_col].metadata
    names = []
    if "ml_attr" in meta and "attrs" in meta["ml_attr"]:
        for k in ("binary","numeric"):
            for a in sorted(meta["ml_attr"]["attrs"].get(k, []), key=lambda x: x["idx"]):
                names.append(a["name"])
    return names if names else [f"f{i}" for i in range(n or 0)]

coefs = lr_model.coefficients.toArray()
names = get_feature_names(df_fe, n=len(coefs))

def split_intercept(arr, n_feat):
    if arr is None: return (None, None)
    if len(arr) == n_feat + 1: return (arr[0], arr[1:])
    if len(arr) == n_feat:     return (None, arr)
    return (None, None)

se_int, se_feat = split_intercept(se_all, len(coefs))
t_int,  t_feat  = split_intercept(t_all,  len(coefs))
p_int,  p_feat  = split_intercept(p_all,  len(coefs))

# 6) Build and save coefficients table (works with or without inference)
feat_df = pd.DataFrame({
    "feature": names,
    "coef": coefs,
    "std_err": se_feat if se_feat is not None else [None]*len(coefs),
    "t_value": t_feat if t_feat is not None else [None]*len(coefs),
    "p_value": p_feat if p_feat is not None else [None]*len(coefs),
})

if se_feat is not None:
    se_arr = np.asarray(se_feat, dtype=float)
    feat_df["ci_lower_95"] = coefs - 1.96 * se_arr
    feat_df["ci_upper_95"] = coefs + 1.96 * se_arr
else:
    feat_df["ci_lower_95"] = None
    feat_df["ci_upper_95"] = None

coef_df = pd.concat([
    pd.DataFrame([{
        "feature": "(Intercept)",
        "coef": lr_model.intercept,
        "std_err": se_int,
        "t_value": t_int,
        "p_value": p_int,
        "ci_lower_95": None,
        "ci_upper_95": None
    }]),
    feat_df
], ignore_index=True)

os.makedirs("_output", exist_ok=True)
coef_path = "_output/lr_coefficients_fullrank.csv"
coef_df.to_csv(coef_path, index=False)
print(f"\nSaved coefficients table to {coef_path}")


```
