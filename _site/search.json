[
  {
    "objectID": "assignment04-Lab03-othmane.html",
    "href": "assignment04-Lab03-othmane.html",
    "title": "Assignment 04 — Lab 03",
    "section": "",
    "text": "The objective of this lab is to apply PySpark to perform regression modeling on employment data from the Lightcast Job Postings dataset. In this exercise, we use Spark to process and prepare a large dataset for salary prediction, engineer relevant features from structured columns, and train a Linear Regression model. We will evaluate model performance using RMSE and R², visualize predictions through diagnostic plots, and conclude by pushing the completed analysis to GitHub for submission."
  },
  {
    "objectID": "assignment04-Lab03-othmane.html#generalized-linear-regression-summary",
    "href": "assignment04-Lab03-othmane.html#generalized-linear-regression-summary",
    "title": "Assignment 04 — Lab 03",
    "section": "5.1 Generalized Linear Regression Summary",
    "text": "5.1 Generalized Linear Regression Summary\n\n\nCode\n# ========================= Generalized Linear Regression Summary (FULL REPLACEMENT) =========================\n# Robust GLR (log-salary) with label auto-detection, safe parsing, Boolean categorical fix,\n# aggressive back-off, and clear diagnostics.\n\n# ---------------------------------- Imports ----------------------------------\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, when, lit, stddev_samp, length\nfrom pyspark.sql.types import BooleanType\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.regression import GeneralizedLinearRegression\nimport numpy as np, pandas as pd, os\n\ntry:\n    from IPython.display import display, HTML\n    _CAN_HTML = True\nexcept Exception:\n    _CAN_HTML = False\n\n# ---------------------------------- Preconditions ----------------------------------\ntry:\n    spark  # noqa\nexcept NameError:\n    raise RuntimeError(\"SparkSession 'spark' is not defined. Create it earlier in your notebook/script.\")\n\ntry:\n    df  # noqa\nexcept NameError:\n    raise RuntimeError(\"Base DataFrame 'df' is not defined. Load your dataset into 'df' earlier.\")\n\n# ---------------------------------- Safe numeric parsers ----------------------------------\ndef parse_scalar_expr(c):\n    # Extract first numeric token; handle 'k' = thousand\n    expr = f\"try_cast(regexp_replace(regexp_extract(`{c}`,'([-+]?\\\\d[\\\\d,]*\\\\.?\\\\d*)',1), ',', '') AS double)\"\n    num = F.expr(expr)\n    has_k = F.lower(F.col(c)).like('%k%')\n    return F.when(has_k & num.isNotNull(), num * 1000.0).otherwise(num)\n\ndef parse_range_mid_expr(c):\n    # Mid-point for ranges like \"80k - 100k\"\n    lo = F.expr(\n        f\"try_cast(regexp_replace(regexp_extract(`{c}`,'([-+]?\\\\d[\\\\d,]*\\\\.?\\\\d*)\\\\s*[-–—]\\\\s*[-+]?\\\\d',1), ',', '') AS double)\"\n    )\n    hi = F.expr(\n        f\"try_cast(regexp_replace(regexp_extract(`{c}`,'[-–—]\\\\s*([-+]?\\\\d[\\\\d,]*\\\\.?\\\\d*)',1), ',', '') AS double)\"\n    )\n    has_k = F.lower(F.col(c)).like('%k%')\n    lo = F.when(has_k & lo.isNotNull(), lo * 1000.0).otherwise(lo)\n    hi = F.when(has_k & hi.isNotNull(), hi * 1000.0).otherwise(hi)\n    return (lo + hi) / 2.0\n\ndef parse_numeric(c):\n    mid = parse_range_mid_expr(c)\n    sc  = parse_scalar_expr(c)\n    return F.when(mid.isNotNull(), mid).when(sc.isNotNull(), sc).otherwise(F.lit(None).cast('double'))\n\n# ---------------------------------- Label auto-detection ----------------------------------\ndef autodetect_label_expr(df_in):\n    \"\"\"Pick the best salary/pay/wage/compensation column(s) by maximizing kept non-null, positive rows.\"\"\"\n    import re\n    cols = df_in.columns\n\n    def looks_like_money(name):\n        n = name.lower()\n        return any(k in n for k in [\"salary\", \"pay\", \"wage\", \"compensat\", \"income\", \"earn\"])\n\n    # Pair candidates: *_FROM with *_TO (or *_Min/_Max etc.)\n    cand_pairs = []\n    for c in cols:\n        if looks_like_money(c) and c.endswith((\"_FROM\", \"_Min\", \"_LOW\", \"_LOWER\", \"_MIN\")):\n            stem = re.sub(r\"(_FROM|_Min|_LOW|_LOWER|_MIN)$\", \"\", c)\n            for mate in [stem + \"_TO\", stem + \"_Max\", stem + \"_HIGH\", stem + \"_UPPER\", stem + \"_MAX\"]:\n                if mate in cols:\n                    cand_pairs.append((c, mate))\n\n    # Evaluate pairs\n    best = None\n    best_n = -1\n    for lo, hi in cand_pairs:\n        tmp = (df_in\n               .select(parse_numeric(lo).alias(\"lo\"), parse_numeric(hi).alias(\"hi\"))\n               .withColumn(\"m\", (F.col(\"lo\") + F.col(\"hi\")) / 2.0)\n               .where(F.col(\"m\").isNotNull())\n               .where(~F.isnan(\"m\"))\n               .where(F.col(\"m\") &gt; 0))\n        n = tmp.count()\n        if n &gt; best_n:\n            best_n = n\n            best = (lo, hi)\n    if best is not None and best_n &gt; 0:\n        lo, hi = best\n        return (parse_numeric(lo) + parse_numeric(hi)) / 2.0, f\"{lo}+{hi}\"\n\n    # Single-column candidates\n    singles = [c for c in cols if looks_like_money(c)]\n    best_c, best_n = None, -1\n    for c in singles:\n        tmp = (df_in\n               .select(parse_numeric(c).alias(\"v\"))\n               .where(F.col(\"v\").isNotNull())\n               .where(~F.isnan(\"v\"))\n               .where(F.col(\"v\") &gt; 0))\n        n = tmp.count()\n        if n &gt; best_n:\n            best_n, best_c = n, c\n    if best_c and best_n &gt; 0:\n        return parse_numeric(best_c), best_c\n\n    # Last resort: numeric SALARY column if present\n    for c, t in df_in.dtypes:\n        if c.upper() == \"SALARY\" and t in (\"double\", \"float\", \"int\", \"bigint\"):\n            return F.col(c).cast(\"double\"), c\n    return None, None\n\n# ---------------------------------- Build label & diagnostics ----------------------------------\nlabel_expr, label_source = autodetect_label_expr(df)\nif label_expr is None:\n    if \"SALARY\" in df.columns:\n        label_expr, label_source = parse_numeric(\"SALARY\"), \"SALARY\"\n    elif set([\"SALARY_FROM\", \"SALARY_TO\"]).issubset(df.columns):\n        label_expr, label_source = (parse_numeric(\"SALARY_FROM\") + parse_numeric(\"SALARY_TO\")) / 2.0, \"SALARY_FROM+SALARY_TO\"\n    elif \"SALARY_TO\" in df.columns:\n        label_expr, label_source = parse_numeric(\"SALARY_TO\"), \"SALARY_TO\"\n    else:\n        label_expr, label_source = parse_numeric(\"SALARY_FROM\"), \"SALARY_FROM\"\n\ndf = (df.withColumn(\"label\", label_expr)\n        .where(F.col(\"label\").isNotNull())\n        .where(~F.isnan(\"label\"))\n        .where(F.col(\"label\") &gt; 0))\ndf = df.withColumn(\"label_log\", F.log1p(F.col(\"label\")))\nn_label_rows = df.count()\nprint(f\"✅ label & label_log ready | label source: {label_source} | rows after label filter: {n_label_rows:,}\")\nif n_label_rows == 0:\n    raise RuntimeError(\"All rows dropped by label parsing. Inspect your dataset headers and salary/pay/wage columns.\")\n\n# ---------------------------------- Engineer numeric features ----------------------------------\nnum_specs = [\n    (\"MIN_YEARS_EXPERIENCE\", \"MIN_YEARS_EXPERIENCE_num\"),\n    (\"MAX_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE_num\"),\n    (\"MODELED_DURATION\",     \"MODELED_DURATION_num\"),\n    (\"SALARY_TO\",            \"SALARY_TO_num\"),\n]\nfor raw, out in num_specs:\n    if raw in df.columns:\n        df = df.withColumn(out, parse_numeric(raw))\n\n# z-score available numerics\nfor _, out in num_specs:\n    if out in df.columns:\n        s = df.select(F.mean(out).alias(\"mu\"), F.stddev_samp(out).alias(\"sd\")).first()\n        mu = float(s.mu) if s.mu is not None else 0.0\n        sd = float(s.sd) if s.sd not in (None, 0.0) else 1.0\n        df = df.withColumn(f\"z_{out}\", (col(out) - lit(mu)) / lit(sd))\n\nnumeric_cols = [c for c in [f\"z_{n}\" for _, n in num_specs] if c in df.columns]\nif not numeric_cols:\n    # Fallback numeric from text length\n    for t in [\"TITLE\", \"TITLE_NAME\", \"TITLE_RAW\", \"BODY\"]:\n        if t in df.columns:\n            df = df.withColumn(\"TITLE_LEN_num\", length(col(t)).cast(\"double\"))\n            numeric_cols = [\"TITLE_LEN_num\"]\n            break\n\n# ---------------------------------- Candidate categoricals ----------------------------------\ncat_raw = [c for c in [\"REMOTE_TYPE_NAME\", \"EMPLOYMENT_TYPE_NAME\", \"EDUCATION_LEVELS_NAME\", \"COMPANY_IS_STAFFING\"] if c in df.columns]\n\n# Ensure boolean categoricals are strings and fill nulls (StringIndexer requires string or numeric)\nfor c in list(cat_raw):\n    if c in df.columns:\n        dt = df.schema[c].dataType\n        if isinstance(dt, BooleanType):\n            df = df.withColumn(c, F.when(F.col(c).isNull(), F.lit(\"Unknown\"))\n                                 .otherwise(F.col(c).cast(\"string\")))\n        else:\n            # also fill nulls for existing strings\n            if \"StringType\" in str(dt):\n                df = df.withColumn(c, F.when(F.col(c).isNull(), F.lit(\"Unknown\")).otherwise(F.col(c)))\n\n# Quick diagnostics\ndef nn_counts(df_in, cols):\n    out = {}\n    for c in cols:\n        if c in df_in.columns:\n            out[c] = df_in.where(F.col(c).isNotNull() & ~F.isnan(c)).count()\n    return out\n\nprint(\"ℹ️ Non-null counts (numerics):\", nn_counts(df, numeric_cols))\nprint(\"ℹ️ Distinct levels (categoricals):\", {c: df.select(F.countDistinct(F.col(c))).first()[0] for c in cat_raw})\n\n# ---------------------------------- Helpers: cap/encode/impute/assemble/fit ----------------------------------\ndef cap_top_k(df_in, c, k):\n    top = (df_in.groupBy(c).count().orderBy(F.desc(\"count\")).limit(max(k - 1, 1))\n           .select(c).rdd.flatMap(lambda r: r).collect())\n    keep = set([v for v in top if v is not None])\n    cc = f\"{c}_top\"\n    out = df_in.withColumn(cc, F.when(col(c).isin(list(keep)), col(c)).otherwise(lit(\"Other\")))\n    return out, cc\n\ndef encode_cats(df_in, cols, k):\n    out = df_in\n    oh_cols = []\n    for c in cols:\n        if c not in out.columns:\n            continue\n        # cap top-k then index + one-hot (degenerate cats skipped)\n        out, capped = cap_top_k(out, c, k)\n        if out.select(capped).distinct().count() &lt; 2:\n            continue\n        idx, oh = f\"{capped}_idx\", f\"{capped}_oh\"\n        out = StringIndexer(inputCol=capped, outputCol=idx, handleInvalid=\"keep\").fit(out).transform(out)\n        out = OneHotEncoder(inputCols=[idx], outputCols=[oh], dropLast=True).fit(out).transform(out)\n        oh_cols.append(oh)\n    return out, oh_cols\n\ndef impute_median(df_in, cols):\n    out = df_in\n    for c in cols:\n        if c in out.columns:\n            med = out.approxQuantile(c, [0.5], 1e-3)[0] if out.where(F.col(c).isNotNull()).count() else 0.0\n            out = out.withColumn(c, when(col(c).isNull() | F.isnan(c), lit(med)).otherwise(col(c)))\n    return out\n\ndef assemble(df_in, y_col, numeric, cats, k):\n    tmp, ohe_cols = encode_cats(df_in, cats, k) if cats else (df_in, [])\n    num_keep_local = [c for c in numeric if c in tmp.columns]\n    tmp = impute_median(tmp, num_keep_local)\n    inputs = num_keep_local + ohe_cols\n    if not inputs:\n        return None, [], 0\n    fe = (VectorAssembler(inputCols=inputs, outputCol=\"features\")\n          .transform(tmp)\n          .select(F.col(y_col).alias(y_col), \"features\")\n          .where(F.col(\"features\").isNotNull()))\n    # derive feature count from metadata\n    meta = fe.schema[\"features\"].metadata\n    p = 0\n    if \"ml_attr\" in meta and \"attrs\" in meta[\"ml_attr\"]:\n        for k2 in (\"binary\", \"numeric\"):\n            p += len(meta[\"ml_attr\"][\"attrs\"].get(k2, []))\n    return fe, inputs, p\n\ndef fit_glr(df_in, y_col, reg=1e-3, fitIntercept=True):\n    glr = GeneralizedLinearRegression(\n        featuresCol=\"features\", labelCol=y_col,\n        family=\"gaussian\", link=\"identity\",\n        regParam=reg, fitIntercept=fitIntercept\n    )\n    m = glr.fit(df_in)\n    return m, m.summary\n\n# ---------------------------------- DoF-controlled fit (robust) ----------------------------------\nlabel_col = \"label_log\"\ndf_work = df.where(F.col(label_col).isNotNull()).where(~F.isnan(F.col(label_col)))\n\nn_label = df_work.count()\nmin_dof = 8 if n_label &gt;= 200 else 4 if n_label &gt;= 50 else 2\nreg = 1e-3\ninitial_topk, min_topk = 8, 2\ntopk = initial_topk\nnum_keep = list(numeric_cols)\ntried_numeric_only = False\ninjected_title_len = False\n\nattempts = 0\nwhile True:\n    attempts += 1\n    df_fe, inputs, p = assemble(df_work, label_col, num_keep, (cat_raw if not tried_numeric_only else []), topk)\n\n    # If assembler had no inputs -&gt; try numeric-only, then inject TITLE_LEN\n    if df_fe is None or not inputs:\n        if not tried_numeric_only:\n            tried_numeric_only = True\n            topk = min_topk\n            continue\n        if not injected_title_len:\n            for t in [\"TITLE\", \"TITLE_NAME\", \"TITLE_RAW\", \"BODY\"]:\n                if t in df_work.columns:\n                    df_work = df_work.withColumn(\"TITLE_LEN_num\", F.length(F.col(t)).cast(\"double\"))\n                    num_keep = list(set(num_keep + [\"TITLE_LEN_num\"]))\n                    injected_title_len = True\n                    break\n            continue\n        raise RuntimeError(\"No usable features available to assemble.\")\n\n    n_eff = df_fe.count()\n\n    # If we lost all rows, try backing off before erroring\n    if n_eff == 0:\n        print(\"⚠️ 0 rows after assemble → backing off features…\")\n        if not tried_numeric_only:\n            tried_numeric_only = True\n            topk = min_topk\n            continue\n        if not injected_title_len:\n            for t in [\"TITLE\", \"TITLE_NAME\", \"TITLE_RAW\", \"BODY\"]:\n                if t in df_work.columns:\n                    df_work = df_work.withColumn(\"TITLE_LEN_num\", F.length(F.col(t)).cast(\"double\"))\n                    num_keep = list(set(num_keep + [\"TITLE_LEN_num\"]))\n                    injected_title_len = True\n                    break\n            continue\n        if topk &gt; min_topk:\n            topk = max(min_topk, topk // 2)\n            continue\n        print(\"Inputs used:\", inputs[:12], \"...\" if len(inputs) &gt; 12 else \"\")\n        raise RuntimeError(\"No rows left after preparation. Check label parsing and input columns.\")\n\n    dof = n_eff - (p + 1)\n    print(f\"[Attempt {attempts}] rows={n_eff:,} | features≈{p:,} | DoF={dof:,} | topK={topk} | numerics={len(num_keep)} | numeric_only={tried_numeric_only}\")\n\n    if (dof &gt; min_dof and p &gt; 0):\n        try:\n            model, summary = fit_glr(df_fe, label_col, reg=reg, fitIntercept=True)\n            print(\"✅ Model succeeded | DoF:\", dof, \"| features≈\", p, \"| topK:\", topk, \"| numeric_only=\", tried_numeric_only)\n            print(\"Using predictors:\", inputs)\n            break\n        except Exception as e:\n            print(\"  Fit failed → shrinking:\", str(e)[:160])\n\n    # Shrink path\n    if topk &gt; min_topk and not tried_numeric_only:\n        topk = max(min_topk, topk // 2)\n        continue\n    if len(num_keep) &gt; 1:\n        sd_map = df_work.agg(*[stddev_samp(c).alias(c) for c in num_keep]).first().asDict()\n        num_keep = sorted(num_keep, key=lambda c: float(sd_map.get(c) or 0.0), reverse=True)[:-1]\n        continue\n    if not tried_numeric_only:\n        tried_numeric_only = True\n        topk = min_topk\n        continue\n    if dof &gt; 1 and p &gt; 0:\n        try:\n            model, summary = fit_glr(df_fe, label_col, reg=max(reg, 1e-2), fitIntercept=True)\n            print(\"⚠️ Proceeding with relaxed fit (tiny data).\")\n            break\n        except Exception as e:\n            print(\"  Final fit failed:\", str(e)[:160])\n            try:\n                model, summary = fit_glr(df_fe, label_col, reg=1e-1, fitIntercept=True)\n                print(\"⚠️ Proceeding with stronger regularization.\")\n                break\n            except Exception as e2:\n                print(\"  Ridge(0.1) failed:\", str(e2)[:160])\n                raise\n    raise RuntimeError(f\"Could not reach a stable fit (rows={n_eff}, features≈{p}). Consider fewer predictors or verifying label parsing.\")\n\n# ---------------------------------- Named coefficient table ----------------------------------\ndef feature_names_from_meta(df_in):\n    meta = df_in.schema[\"features\"].metadata\n    names = []\n    if \"ml_attr\" in meta and \"attrs\" in meta[\"ml_attr\"]:\n        for k in (\"binary\", \"numeric\"):\n            names += [a[\"name\"] for a in sorted(meta[\"ml_attr\"][\"attrs\"].get(k, []), key=lambda a: a[\"idx\"])]\n    return names\n\nnames = feature_names_from_meta(df_fe)\ncoef  = np.array(model.coefficients.toArray())\nintercept = float(model.intercept)\n\ndef safe_arr(getter, size):\n    try:\n        arr = np.array(getter())\n        if len(arr) == size + 1:  # intercept included at end\n            return np.r_[arr[-1], arr[:-1]]\n        if len(arr) == size:\n            return np.r_[np.nan, arr]\n        return np.r_[arr, [np.nan] * ((size + 1) - len(arr))]\n    except Exception:\n        return np.r_[np.nan, [np.nan] * size]\n\nse = safe_arr(lambda: summary.coefficientStandardErrors, len(coef))\ntv = safe_arr(lambda: summary.tValues,                  len(coef))\ntry:\n    _ = summary.residualDegreeOfFreedom\n    pv = safe_arr(lambda: summary.pValues, len(coef))\nexcept Exception:\n    pv = np.r_[np.nan, [np.nan] * len(coef)]\n\ntable = pd.DataFrame({\n    \"Feature\":  [\"(Intercept)\"] + names,\n    \"Estimate\": np.r_[intercept, coef],\n    \"Std Error\": se,\n    \"t-stat\":   tv,\n    \"P-Value\":  pv\n})\n\n# Pretty print\nif _CAN_HTML:\n    pretty = table.copy()\n    for c in [\"Estimate\", \"Std Error\", \"t-stat\", \"P-Value\"]:\n        pretty[c] = pretty[c].apply(lambda x: f\"{x:,.4f}\" if pd.notnull(x) and np.isfinite(x) else \"\")\n    display(HTML(pretty.to_html(index=False, classes=\"table table-striped table-sm\", border=0)))\nelse:\n    print(table.head(15).to_string(index=False))\n\nprint(\"\\n=== GLR (log-salary) SUMMARY ===\")\nprint(\"Residual DoF:\", getattr(summary, \"residualDegreeOfFreedom\", None))\nprint(\"Null Deviance:\", getattr(summary, \"nullDeviance\", None))\nprint(\"Residual Deviance:\", getattr(summary, \"deviance\", None))\nprint(\"AIC:\", getattr(summary, \"aic\", None))\n\n# Save CSV\nos.makedirs(\"_output\", exist_ok=True)\nout_csv = \"_output/glr_coefficients_named.csv\"\ntable.to_csv(out_csv, index=False)\nprint(f\"📁 Saved table to {out_csv}\")\n# =================================================================================================\n\n\n✅ label & label_log ready | label source: SALARY | rows after label filter: 30,808\n\n\nℹ️ Non-null counts (numerics): {'z_MIN_YEARS_EXPERIENCE_num': 0, 'z_MAX_YEARS_EXPERIENCE_num': 0, 'z_MODELED_DURATION_num': 0, 'z_SALARY_TO_num': 0}\n\n\nℹ️ Distinct levels (categoricals): {'REMOTE_TYPE_NAME': 4, 'EMPLOYMENT_TYPE_NAME': 3, 'EDUCATION_LEVELS_NAME': 27, 'COMPANY_IS_STAFFING': 2}\n\n\n[Attempt 1] rows=30,808 | features≈21 | DoF=30,786 | topK=8 | numerics=4 | numeric_only=False\n\n\n✅ Model succeeded | DoF: 30786 | features≈ 21 | topK: 8 | numeric_only= False\nUsing predictors: ['z_MIN_YEARS_EXPERIENCE_num', 'z_MAX_YEARS_EXPERIENCE_num', 'z_MODELED_DURATION_num', 'z_SALARY_TO_num', 'REMOTE_TYPE_NAME_top_oh', 'EMPLOYMENT_TYPE_NAME_top_oh', 'EDUCATION_LEVELS_NAME_top_oh', 'COMPANY_IS_STAFFING_top_oh']\n\n\n\n\n\nFeature\nEstimate\nStd Error\nt-stat\nP-Value\n\n\n\n\n(Intercept)\n11.4935\n0.1884\n61.0072\n0.0000\n\n\nREMOTE_TYPE_NAME_top_oh_[None]\n0.0000\n\n0.0000\n1.0000\n\n\nREMOTE_TYPE_NAME_top_oh_Remote\n0.0000\n\n0.0000\n1.0000\n\n\nREMOTE_TYPE_NAME_top_oh_Hybrid Remote\n0.0000\n\n0.0000\n1.0000\n\n\nREMOTE_TYPE_NAME_top_oh_Not Remote\n0.0000\n\n0.0000\n1.0000\n\n\nEMPLOYMENT_TYPE_NAME_top_oh_Full-time (&gt; 32 hours)\n0.0069\n0.0679\n0.1011\n0.9195\n\n\nEMPLOYMENT_TYPE_NAME_top_oh_Part-time (â‰¤ 32 hours)\n0.0117\n0.0680\n0.1722\n0.8632\n\n\nEMPLOYMENT_TYPE_NAME_top_oh_Part-time / full-time\n-0.0429\n0.0687\n-0.6245\n0.5323\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"Bachelor's degree\"\\n]\n-0.0987\n0.0696\n-1.4176\n0.1563\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"No Education Listed\"\\n]\n0.0899\n0.1372\n0.6555\n0.5122\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"Bachelor's degree\",\\n \"Master's degree\"\\n]\n-0.1137\n0.1374\n-0.8275\n0.4080\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"Associate degree\",\\n \"Bachelor's degree\"\\n]\n-0.0445\n0.1378\n-0.3229\n0.7468\n\n\nEDUCATION_LEVELS_NAME_top_oh_Other\n0.0064\n0.0518\n0.1234\n0.9018\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"High school or GED\"\\n]\n-0.0116\n0.0519\n-0.2230\n0.8236\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"High school or GED\",\\n \"Bachelor's degree\"\\n]\n0.1122\n0.0519\n2.1609\n0.0307\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"Master's degree\"\\n]\n0.0816\n0.0526\n1.5515\n0.1208\n\n\nCOMPANY_IS_STAFFING_top_oh_false\n-0.0874\n0.0526\n-1.6616\n0.0966\n\n\nCOMPANY_IS_STAFFING_top_oh_true\n-0.6842\n0.0534\n-12.8239\n0.0000\n\n\nz_MIN_YEARS_EXPERIENCE_num\n-0.1511\n0.0534\n-2.8277\n0.0047\n\n\nz_MAX_YEARS_EXPERIENCE_num\n0.2381\n0.0535\n4.4538\n0.0000\n\n\nz_MODELED_DURATION_num\n0.0246\n0.0969\n0.2539\n0.7995\n\n\nz_SALARY_TO_num\n-0.0246\n0.0969\n-0.2539\n0.7995\n\n\n\n\n\n\n=== GLR (log-salary) SUMMARY ===\nResidual DoF: 30786\n\n\nNull Deviance: 5137.427195447666\nResidual Deviance: 4505.057949711946\n\n\nAIC: 28244.667895692117\n📁 Saved table to _output/glr_coefficients_named.csv"
  },
  {
    "objectID": "assignment04-Lab01-othmane.html",
    "href": "assignment04-Lab01-othmane.html",
    "title": "Assignment 04 — Lab 01",
    "section": "",
    "text": "This report analyzes job postings from the Lightcast Job Market dataset, exploring salary trends, employment types, skill demand, and more.\nAll visualizations are interactive, allowing you to hover and explore insights dynamically."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation",
    "href": "assignment04-Lab01-othmane.html#explanation",
    "title": "Assignment 04 — Lab 01",
    "section": "3.1 ✏️ Explanation",
    "text": "3.1 ✏️ Explanation\nThe box plot shows how salary levels vary across different employment types. Full-time positions generally have higher median salaries and a wider pay range, reflecting greater earning potential but also more variability. In contrast, part-time and contract roles exhibit lower median salaries with tighter ranges, suggesting more consistency but fewer high-paying opportunities."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-1",
    "href": "assignment04-Lab01-othmane.html#explanation-1",
    "title": "Assignment 04 — Lab 01",
    "section": "4.1 ✏️ Explanation",
    "text": "4.1 ✏️ Explanation\nThe chart shows that salary levels vary notably across industries. The Information and Accommodation and Food Services sectors exhibit the highest median and upper-range salaries, suggesting strong compensation potential in these fields. Meanwhile, industries like Administrative Support and Retail Trade tend to offer lower median salaries, reflecting more standardized pay structures and fewer high-paying roles."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-2",
    "href": "assignment04-Lab01-othmane.html#explanation-2",
    "title": "Assignment 04 — Lab 01",
    "section": "5.1 ✏️ Explanation",
    "text": "5.1 ✏️ Explanation\nThe trend line reveals noticeable fluctuations in job posting activity, indicating that hiring demand changes frequently over time. Peaks suggest periods of intensified recruitment, possibly driven by seasonal hiring cycles or new project launches, while the dips represent slower hiring phases. Overall, the data highlights a dynamic job market with recurring surges in posting volume."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-3",
    "href": "assignment04-Lab01-othmane.html#explanation-3",
    "title": "Assignment 04 — Lab 01",
    "section": "6.1 ✏️ Explanation",
    "text": "6.1 ✏️ Explanation\nThe chart shows that Data Analyst is by far the most frequently posted job title, indicating a high market demand for data-focused professionals. Other roles like Unclassified, Enterprise Architect, and Data Engineer also appear prominently, reflecting the growing need for both analytical and technical expertise in data-driven organizations."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-4",
    "href": "assignment04-Lab01-othmane.html#explanation-4",
    "title": "Assignment 04 — Lab 01",
    "section": "7.1 ✏️ Explanation",
    "text": "7.1 ✏️ Explanation\nThe chart shows that a large majority of job postings do not specify a remote type, while around 17% explicitly offer remote positions. A smaller portion of listings are hybrid or partially remote, indicating that while remote work is available, most employers still emphasize on-site or unspecified work arrangements."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-5",
    "href": "assignment04-Lab01-othmane.html#explanation-5",
    "title": "Assignment 04 — Lab 01",
    "section": "8.1 ✏️ Explanation",
    "text": "8.1 ✏️ Explanation\nThe chart shows that professional, scientific, and technical services industries have the highest demand for skills, especially in communication, data analysis, and leadership. Across most industries, soft skills like communication and problem-solving appear as consistently essential, highlighting their broad value alongside technical expertise such as SQL and computer science."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-6",
    "href": "assignment04-Lab01-othmane.html#explanation-6",
    "title": "Assignment 04 — Lab 01",
    "section": "9.1 ✏️ Explanation",
    "text": "9.1 ✏️ Explanation\nThis chart shows how median annualized salaries vary across the most common job titles in the dataset. Larger bubbles represent job titles with a higher number of postings, while color indicates the relative salary level. From the visualization, we can see that technical and leadership roles such as “Data Architects,” “Principal Data Engineers,” and “SAP Data Analytics Managers” tend to offer the highest median salaries (often exceeding $160K), while more common positions like “Data Analyst” or “Business Intelligence Analyst” have lower median pay but significantly higher posting volume, reflecting strong demand for analytical talent across industries."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-7",
    "href": "assignment04-Lab01-othmane.html#explanation-7",
    "title": "Assignment 04 — Lab 01",
    "section": "10.1 ✏️ Explanation",
    "text": "10.1 ✏️ Explanation\nThe Sankey diagram shows how various job titles, such as Data Analyst, Data Modeler, and ERP Business Analyst, flow into a single ONET occupation category, Business Intelligence Analysts. This highlights that many data-related positions ultimately map to the same occupational classification, reflecting the overlap and convergence of skills within business intelligence and analytics roles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assignment 04 — Overview",
    "section": "",
    "text": "Overview\nLinks above (Lab 01 / Lab 02) open the labs."
  },
  {
    "objectID": "assignment04-Lab02-othmane.html",
    "href": "assignment04-Lab02-othmane.html",
    "title": "Assignment 04 — Lab 02",
    "section": "",
    "text": "The objective of this lab is to apply visual reporting and storytelling techniques to analyze job market data provided by Lightcast. Through this exercise, we will use PySpark and data visualization tools to explore, clean, and interpret a large dataset of job postings. The goal is to transform raw data into meaningful insights by identifying trends, patterns, and relationships between variables such as education level, remote work type, experience, and salary. This lab emphasizes not only technical proficiency in data processing but also the ability to communicate analytical findings effectively through visual narratives."
  },
  {
    "objectID": "assignment04-Lab02-othmane.html#typecast-key-columns",
    "href": "assignment04-Lab02-othmane.html#typecast-key-columns",
    "title": "Assignment 04 — Lab 02",
    "section": "3.1 Typecast Key Columns:",
    "text": "3.1 Typecast Key Columns:\n\n\nCode\n# Import necessary libraries\nfrom pyspark.sql import functions as F, types as T\nimport os\n\n#  Select and typecast key columns\n# Convert salary fields to numeric values for later analysis\nclean_df = (\n    df.withColumn(\"SALARY_FROM\", F.col(\"SALARY_FROM\").cast(T.DoubleType()))\n      .withColumn(\"SALARY_TO\", F.col(\"SALARY_TO\").cast(T.DoubleType()))\n      .withColumn(\"MAX_YEARS_EXPERIENCE\", F.col(\"MAX_YEARS_EXPERIENCE\").cast(T.DoubleType()))\n)\n# Handle missing values\n# Fill missing numeric values with 0 (or another placeholder if desired)\nclean_df = clean_df.fillna({\"SALARY_FROM\": 0, \"SALARY_TO\": 0, \"MAX_YEARS_EXPERIENCE\": 0})\n\n# Optional: Calculate average salary column for convenience\nclean_df = clean_df.withColumn(\n    \"AVERAGE_SALARY\",\n    (F.col(\"SALARY_FROM\") + F.col(\"SALARY_TO\")) / 2\n)\n\n# Show a quick sample\nclean_df.select(\"SALARY_FROM\", \"SALARY_TO\", \"AVERAGE_SALARY\", \"MAX_YEARS_EXPERIENCE\").show(5, truncate=False)\n\n\n+-----------+---------+--------------+--------------------+\n|SALARY_FROM|SALARY_TO|AVERAGE_SALARY|MAX_YEARS_EXPERIENCE|\n+-----------+---------+--------------+--------------------+\n|0.0        |0.0      |0.0           |2.0                 |\n|0.0        |0.0      |0.0           |3.0                 |\n|0.0        |0.0      |0.0           |0.0                 |\n|0.0        |0.0      |0.0           |0.0                 |\n|35000.0    |150000.0 |92500.0       |0.0                 |\n+-----------+---------+--------------+--------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "assignment04-Lab02-othmane.html#save-the-cleaned-dataset",
    "href": "assignment04-Lab02-othmane.html#save-the-cleaned-dataset",
    "title": "Assignment 04 — Lab 02",
    "section": "3.2 Save the Cleaned Dataset",
    "text": "3.2 Save the Cleaned Dataset\nNow that the dataset has been cleaned and typecasted, it is saved as a new CSV file for later use.\n\n\nCode\n# Save cleaned dataset safely\nimport os, glob, shutil\n\n# keep only the columns needed downstream \ncols_to_keep = [c for c in [\n    \"SALARY_FROM\",\"SALARY_TO\",\"AVERAGE_SALARY\",\"MAX_YEARS_EXPERIENCE\",\n    # \"EDUCATION_NAME\",\"REMOTE_TYPE_NAME\",\"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n] if c in clean_df.columns]\n\ntmp_dir   = \"_tmp_clean_csv\"\nfinal_csv = \"data/lightcast_cleaned.csv\"\n\n# CSV part from Spark\n(clean_df\n .select(cols_to_keep) if cols_to_keep else clean_df\n).coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(tmp_dir)\n\n# Move part-*.csv to final path\npart_files = glob.glob(os.path.join(tmp_dir, \"part-*.csv\"))\nif not part_files:\n    raise RuntimeError(\"No part file found after Spark write.\")\nif os.path.exists(final_csv):\n    os.remove(final_csv)\nshutil.move(part_files[0], final_csv)\nshutil.rmtree(tmp_dir, ignore_errors=True)\n\nprint(\"✅ Cleaned dataset saved.\")\nprint(f\"📁 File location: {final_csv}\")\n\n\n✅ Cleaned dataset saved.\n📁 File location: data/lightcast_cleaned.csv"
  }
]