[
  {
    "objectID": "assignment04-Lab04-othmane.html",
    "href": "assignment04-Lab04-othmane.html",
    "title": "Assignment 04 — Lab 04 Machine Learning on Scale",
    "section": "",
    "text": "PySpark is applied to large-scale employment data from the Lightcast Job Postings dataset to develop and evaluate salary prediction models. The workflow involves engineering key features from structured columns, training a Linear Regression model, and assessing performance using RMSE and R² metrics. Visual diagnostic plots are generated to interpret model accuracy and residual patterns. The final analysis is documented, version-controlled, and submitted via GitHub, demonstrating end-to-end data processing and predictive modeling on a distributed computing platform."
  },
  {
    "objectID": "assignment04-Lab04-othmane.html#generalized-linear-regression-summary",
    "href": "assignment04-Lab04-othmane.html#generalized-linear-regression-summary",
    "title": "Assignment 04 — Lab 04 Machine Learning on Scale",
    "section": "5.1 Generalized Linear Regression Summary",
    "text": "5.1 Generalized Linear Regression Summary\n\n\nCode\n# ======================================================\n# GLR SUMMARY (defensive, with bootstrap fallback)\n# ======================================================\n\nfrom pyspark.ml.regression import GeneralizedLinearRegression\nfrom pyspark.sql import functions as F\nimport math, numpy as np\n\n# ---------- safety: cache the pruned datasets ----------\ntry:\n    train_nz = train_nz.cache(); test_nz = test_nz.cache()\n    _ = train_nz.count(); _ = test_nz.count()\nexcept NameError:\n    # If you haven't run the train/test split + pruning cells yet\n    raise RuntimeError(\"train_nz/test_nz not found. Run the Train/Test Split cell first.\")\n\n# ---------- settings (tweakable) ----------\nDO_BOOTSTRAP = True\nB = 8                    # keep modest for speed\nBOOTSTRAP_FRACTION = 0.85\nZ_95 = 1.96\n\n# ---------- helpers ----------\ndef extract_feature_names(df, features_col=\"features\"):\n    meta = df.schema[features_col].metadata\n    try:\n        attrs = meta[\"ml_attr\"][\"attrs\"]\n        names = []\n        for typ in (\"binary\", \"numeric\"):\n            if typ in attrs:\n                for a in attrs[typ]:\n                    names.append(a.get(\"name\", f\"f_{a.get('idx', len(names))}\"))\n        return names\n    except Exception:\n        size = df.selectExpr(f\"size({features_col}) as n\").first().n\n        return [f\"feature_{i}\" for i in range(size)]\n\ndef normal_pvalue_from_t(t):\n    if t is None or (isinstance(t, float) and math.isnan(t)):\n        return float(\"nan\")\n    z = abs(float(t))\n    return 2.0 * (1.0 - 0.5 * (1 + math.erf(z / math.sqrt(2))))\n\ndef build_table(coefs, ses, names, intercept=None, intercept_se=float(\"nan\")):\n    # align lengths\n    if len(names) != len(coefs):\n        if len(names) &gt; len(coefs):\n            names = names[:len(coefs)]\n        else:\n            names += [f\"feature_{i}\" for i in range(len(names), len(coefs))]\n\n    rows = []\n    for n, b, se in zip(names, coefs, ses):\n        if se is None or (isinstance(se, float) and math.isnan(se)) or se == 0.0:\n            t = float(\"nan\"); p = float(\"nan\"); lo = float(\"nan\"); hi = float(\"nan\")\n        else:\n            t = float(b) / float(se)\n            p = normal_pvalue_from_t(t)\n            lo = float(b) - Z_95*float(se)\n            hi = float(b) + Z_95*float(se)\n        rows.append((n, float(b), float(se) if se == se else float(\"nan\"), t, p, lo, hi))\n\n    coef_df = spark.createDataFrame(rows, [\"feature\", \"coef\", \"se\", \"t\", \"p\", \"ci_lo\", \"ci_hi\"])\n\n    # intercept row (optional)\n    if intercept is not None:\n        if (intercept_se is None) or (isinstance(intercept_se, float) and math.isnan(intercept_se)) or intercept_se == 0.0:\n            t_i = float(\"nan\"); p_i = float(\"nan\"); lo_i = float(\"nan\"); hi_i = float(\"nan\")\n        else:\n            t_i = float(intercept) / float(intercept_se)\n            p_i = normal_pvalue_from_t(t_i)\n            lo_i = float(intercept) - Z_95*float(intercept_se)\n            hi_i = float(intercept) + Z_95*float(intercept_se)\n        irow = spark.createDataFrame(\n            [(\"Intercept\", float(intercept), float(intercept_se) if intercept_se==intercept_se else float(\"nan\"),\n              t_i, p_i, lo_i, hi_i)],\n            [\"feature\", \"coef\", \"se\", \"t\", \"p\", \"ci_lo\", \"ci_hi\"]\n        )\n        coef_df = irow.unionByName(coef_df)\n\n    return coef_df\n\n# ---------- 1) Fit GLR and print model metrics (all getattr-guarded) ----------\nglr = GeneralizedLinearRegression(\n    featuresCol=\"features\", labelCol=\"SALARY\", predictionCol=\"prediction_glr\",\n    family=\"gaussian\", link=\"identity\", fitIntercept=True, regParam=0.0, maxIter=100\n)\nglr_model = glr.fit(train_nz)\nglr_sum   = glr_model.summary\n\nprint(\"✅ GLR Model Summary\")\nprint(f\"AIC: {getattr(glr_sum, 'aic', float('nan')):.2f}\")\nprint(f\"Deviance: {getattr(glr_sum, 'deviance', float('nan')):.2f}\")\nprint(f\"Dispersion: {getattr(glr_sum, 'dispersion', float('nan')):.4f}\")\nprint(f\"Null Deviance: {getattr(glr_sum, 'nullDeviance', float('nan')):.2f}\")\nprint(f\"Residual DF: {getattr(glr_sum, 'residualDegreeOfFreedom', float('nan'))}\")\n\nfeat_names = extract_feature_names(train_nz, \"features\")\ncoefs = list(glr_model.coefficients.toArray())\n\n# ---------- 2) Try analytic SE; else bootstrap; else NA ----------\ncoef_df_glr = None\nintercept_se = float(\"nan\")\n\ntry:\n    ses_all = list(glr_sum.coefficientStandardErrors)  # may include intercept as last element\n    if len(ses_all) == len(coefs) + 1:\n        intercept_se = float(ses_all[-1]); ses = ses_all[:-1]\n    else:\n        ses = ses_all\n    coef_df_glr = build_table(coefs, ses, feat_names, intercept=glr_model.intercept, intercept_se=intercept_se)\n    print(\"ℹ️ Using GLR analytic standard errors.\")\nexcept Exception:\n    print(\"⚠️ GLR analytic SE unavailable.\")\n    if DO_BOOTSTRAP:\n        print(f\"⚠️ Running GLR bootstrap (B={B}).\")\n        coef_mat = []\n        for b in range(B):\n            samp = train_nz.sample(withReplacement=True, fraction=BOOTSTRAP_FRACTION, seed=4100 + b)\n            m = glr.fit(samp)\n            coef_mat.append(m.coefficients.toArray())\n        coef_mat = np.array(coef_mat)\n        ses_bs   = np.std(coef_mat, axis=0, ddof=1)\n        coef_df_glr = build_table(coefs, list(ses_bs), feat_names, intercept=glr_model.intercept, intercept_se=float(\"nan\"))\n        print(\"ℹ️ Bootstrap SE computed.\")\n    else:\n        # As a last resort, output coefficients with NA SE (still a valid table)\n        coef_df_glr = build_table(coefs, [float(\"nan\")]*len(coefs), feat_names, intercept=glr_model.intercept)\n\n# ---------- 3) Order and show ----------\ncoef_df_glr = coef_df_glr.withColumn(\"abs_t\", F.abs(F.col(\"t\"))).orderBy(F.desc_nulls_last(\"abs_t\"))\nprint(\"\\n--- Top 25 GLR coefficients by |t| ---\")\ncoef_df_glr.select(\"feature\", \"coef\", \"se\", \"t\", \"p\", \"ci_lo\", \"ci_hi\").show(25, truncate=False)\n\n# Optional: save\ncoef_df_glr.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"output/glr_coef_table\")\n\n\n✅ GLR Model Summary\nAIC: 71511.26\nDeviance: 2421727380238.17\nDispersion: 808320220.3732\nNull Deviance: 4025242261297.89\nResidual DF: 2996\n⚠️ GLR analytic SE unavailable.\n⚠️ Running GLR bootstrap (B=8).\n\n\nℹ️ Bootstrap SE computed.\n\n--- Top 25 GLR coefficients by |t| ---\n+---------------------------------------------------+-------------------+------------------+-------------------+----------------------+-------------------+-------------------+\n|feature                                            |coef               |se                |t                  |p                     |ci_lo              |ci_hi              |\n+---------------------------------------------------+-------------------+------------------+-------------------+----------------------+-------------------+-------------------+\n|Intercept                                          |68743.77500901464  |NaN               |NaN                |NaN                   |NaN                |NaN                |\n|MIN_EDULEVELS_NAME_OHE_Ph.D. or professional degree|-29043.70315924699 |1342.4497631939091|-21.63485290514502 |0.0                   |-31674.904695107052|-26412.50162338693 |\n|EMPLOYMENT_TYPE_NAME_OHE_Full-time (&gt; 32 hours)    |6258.360295366977  |454.49521925360943|13.769914468285743 |0.0                   |5367.549665629903  |7149.170925104051  |\n|EMPLOYMENT_TYPE_NAME_OHE_Part-time (â‰¤ 32 hours)  |6258.360295366977  |454.49521925360943|13.769914468285743 |0.0                   |5367.549665629903  |7149.170925104051  |\n|MIN_EDULEVELS_NAME_OHE_Master's degree             |6799.902817703081  |884.2407010146808 |7.690103848307459  |1.4654943925052066E-14|5066.791043714307  |8533.014591691855  |\n|STATE_NAME_OHE_Connecticut                         |-17663.86497159954 |3107.0991000577155|-5.6850021202321575|1.308110375575211E-8  |-23753.779207712665|-11573.950735486418|\n|STATE_NAME_OHE_Florida                             |6324.88962145224   |1145.0330526455273|5.523761612679196  |3.31817606724627E-8   |4080.624838267007  |8569.154404637473  |\n|STATE_NAME_OHE_Illinois                            |5753.125358216762  |1261.333717155438 |4.561144509155928  |5.087555114524633E-6  |3280.911272592103  |8225.339443841422  |\n|STATE_NAME_OHE_Kansas                              |-6695.74692615243  |1513.9528938583474|-4.422691718688915 |9.747878385812925E-6  |-9663.09459811479  |-3728.3992541900693|\n|EMPLOYMENT_TYPE_NAME_OHE_Part-time / full-time     |-436.9081242915736 |100.94333146309963|-4.328251484857003 |1.5029779590669534E-5 |-634.7570539592489 |-239.05919462389832|\n|MIN_YEARS_EXPERIENCE                               |-18109.684129863697|4268.6014640451995|-4.242533364241927 |2.210106714262139E-5  |-26476.142999392287|-9743.225260335106 |\n|STATE_NAME_OHE_New York                            |-5803.566031410265 |2265.805349973469 |-2.5613700803902772|0.010426022853324168  |-10244.544517358265|-1362.5875454622656|\n|STATE_NAME_OHE_Alabama                             |-9872.902980979015 |3979.705360045808 |-2.480812544591335 |0.01310832889701441   |-17673.1254866688  |-2072.680475289232 |\n|STATE_NAME_OHE_Montana                             |-11072.05254004016 |4659.376454843126 |-2.376294907128928 |0.017487479972304598  |-20204.43039153269 |-1939.6746885476332|\n|STATE_NAME_OHE_Minnesota                           |-8101.756874638972 |3659.233168525277 |-2.214058656968311 |0.026824744152868396  |-15273.853884948516|-929.6598643294283 |\n|STATE_NAME_OHE_Arizona                             |-5602.609205125748 |2865.2460601445064|-1.955367562687857 |0.050539701884692256  |-11218.49148300898 |13.273072757484442 |\n|MAX_YEARS_EXPERIENCE                               |14766.752424825841 |7842.595912946956 |1.8828908933645474 |0.05971515561415819   |-604.7355645501921 |30138.240414201875 |\n|MIN_EDULEVELS_NAME_OHE_Associate degree            |1556.2469031492185 |872.038869471156  |1.7846072665235637 |0.07432504193937861   |-152.94928101424716|3265.443087312684  |\n|MIN_EDULEVELS_NAME_OHE_High school or GED          |-9930.591776717225 |5701.25180542815  |-1.7418265524182477|0.08153879901529759   |-21105.0453153564  |1243.8617619219494 |\n|STATE_NAME_OHE_Arkansas                            |-6263.415624941469 |3667.084593488783 |-1.7080095823430663|0.08763456447389784   |-13450.901428179484|924.0701782965452  |\n|STATE_NAME_OHE_Pennsylvania                        |-5819.501636977555 |3427.3976782236673|-1.6979359220414871|0.08951985646358684   |-12537.201086295943|898.1978123408326  |\n|STATE_NAME_OHE_Rhode Island                        |-5599.4587120750075|3390.058311193959 |-1.6517293208749881|0.09858974264287057   |-12243.973002015167|1045.0555778651524 |\n|STATE_NAME_OHE_Michigan                            |-4349.005564718021 |3027.5110132562504|-1.4364953738154802|0.15086143179646427   |-10282.92715070027 |1584.91602126423   |\n|STATE_NAME_OHE_Delaware                            |-7108.936300793899 |5057.751381256516 |-1.4055527377519692|0.15985697868948767   |-17022.12900805667 |2804.2564064688722 |\n|STATE_NAME_OHE_North Carolina                      |3764.4294608945975 |2894.8856202916313|1.3003724342364062 |0.19347335300882573   |-1909.5463548769994|9438.405276666195  |\n+---------------------------------------------------+-------------------+------------------+-------------------+----------------------+-------------------+-------------------+\nonly showing top 25 rows\n\n\n\n5.1.1 Interpretation\nThe Generalized Linear Regression model produced an AIC of approximately 71,000, and the dispersion parameter aligns well with the salary scale. According to bootstrap-based standard errors, employment type and education level have the most significant impact on salary. Specifically, full-time positions and advanced degrees, such as Master’s and Ph.D., considerably increase the predicted salary, while part-time roles and lower education levels lead to a decrease. Geographic factors also play an important role, highlighting the variations in labor markets at the state level. Overall, the model effectively identifies and interprets key relationships while ensuring numerical stability through bootstrapped inference."
  },
  {
    "objectID": "assignment04-Lab04-othmane.html#feature-importance-plot",
    "href": "assignment04-Lab04-othmane.html#feature-importance-plot",
    "title": "Assignment 04 — Lab 04 Machine Learning on Scale",
    "section": "6.1 Feature Importance Plot",
    "text": "6.1 Feature Importance Plot\n\n\nCode\n# ======================================================\n# Random Forest Feature Importance — Interactive + PNG\n#  - Robust vector length detection (Vector UDT safe)\n#  - Interactive Plotly bar chart\n#  - Static PNG saved to ./_output/rf_feature_importance.png\n# ======================================================\n\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.functions import vector_to_array\nfrom pyspark.sql import functions as F\nimport os\n\n# ---------- 0) Safety: ensure train/test exist ----------\ntry:\n    _ = train_nz.count(); _ = test_nz.count()\nexcept NameError:\n    raise RuntimeError(\"train_nz/test_nz not found. Run the RF training cell first.\")\n\n# ---------- 1) Get (or fit) RF model ----------\ntry:\n    rf_model  # reuse if already trained\nexcept NameError:\n    rf = RandomForestRegressor(\n        featuresCol=\"features\", labelCol=\"SALARY\", predictionCol=\"prediction_rf\",\n        numTrees=200, maxDepth=6, featureSubsetStrategy=\"sqrt\",\n        subsamplingRate=0.8, minInstancesPerNode=2, maxBins=64, seed=42\n    )\n    rf_model = rf.fit(train_nz)\n\n# ---------- 2) Robust feature-name extraction ----------\ndef extract_feature_names(df, features_col=\"features\"):\n    meta = df.schema[features_col].metadata\n    try:\n        attrs = meta[\"ml_attr\"][\"attrs\"]\n        names = []\n        for typ in (\"binary\", \"numeric\"):\n            if typ in attrs:\n                for a in attrs[typ]:\n                    names.append(a.get(\"name\", f\"f_{a.get('idx', len(names))}\"))\n        return names\n    except Exception:\n        size = df.selectExpr(f\"size({features_col}) as n\").first().n\n        return [f\"feature_{i}\" for i in range(size)]\n\ntry:\n    feature_names = feat_names  # reuse from earlier if available\nexcept NameError:\n    feature_names = extract_feature_names(train_nz, \"features\")\n\n# ---------- 3) Vector length (no AnalysisException) ----------\ntry:\n    # Convert Vector UDT -&gt; array, then size()\n    vec_len = train_nz.select(\n        F.size(vector_to_array(F.col(\"features\"))).alias(\"n\")\n    ).first().n\nexcept Exception:\n    # Fallback to the trained model’s importances size\n    vec_len = int(getattr(rf_model.featureImportances, \"size\", len(feature_names)))\n\n# Align names to actual vector size\nif len(feature_names) != vec_len:\n    if len(feature_names) &gt; vec_len:\n        feature_names = feature_names[:vec_len]\n    else:\n        feature_names += [f\"feature_{i}\" for i in range(len(feature_names), vec_len)]\n\n# ---------- 4) Build importances table ----------\nimp_vec = rf_model.featureImportances  # SparseVector\nimportances = [0.0] * vec_len\nfor idx, val in zip(imp_vec.indices, imp_vec.values):\n    if int(idx) &lt; vec_len:\n        importances[int(idx)] = float(val)\n\nimp_rows = list(zip(feature_names, importances))\nimp_df = spark.createDataFrame(imp_rows, [\"feature\", \"rf_importance\"]) \\\n              .orderBy(F.desc(\"rf_importance\"))\n\ntopN = 10\ntop_df = imp_df.limit(topN)\ntop_pd = top_df.toPandas()\n\n# ---------- 5) Interactive bar (Plotly) ----------\nimport plotly.express as px\n\nfig = px.bar(\n    top_pd.sort_values(\"rf_importance\", ascending=True),\n    x=\"rf_importance\", y=\"feature\",\n    orientation=\"h\",\n    title=f\"Top {topN} Random Forest Feature Importances\",\n    labels={\"rf_importance\": \"Importance\", \"feature\": \"Feature\"},\n)\nfig.update_layout(yaxis=dict(dtick=1), margin=dict(l=10, r=10, t=40, b=10))\nfig.show()\n\n# ---------- 6) Save static PNG to ./_output/ ----------\n# Use matplotlib (and seaborn if available) to avoid Plotly/kaleido dependency.\nimport matplotlib.pyplot as plt\ntry:\n    import seaborn as sns\n    use_sns = True\nexcept Exception:\n    use_sns = False\n\nos.makedirs(\"_output\", exist_ok=True)\nplt.figure(figsize=(8, 5))\nplot_data = top_pd.sort_values(\"rf_importance\", ascending=True)\n\nif use_sns:\n    ax = sns.barplot(data=plot_data, x=\"rf_importance\", y=\"feature\")\nelse:\n    ax = plt.barh(plot_data[\"feature\"], plot_data[\"rf_importance\"])\n\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(f\"Top {topN} Random Forest Feature Importances\")\nplt.tight_layout()\nplt.savefig(\"_output/rf_feature_importance.png\", dpi=200)\nplt.close()\n\nprint(\"✅ Saved static plot to: _output/rf_feature_importance.png\")\n\n\n                            \n                                            \n\n\n✅ Saved static plot to: _output/rf_feature_importance.png"
  },
  {
    "objectID": "assignment04-Lab02-othmane.html",
    "href": "assignment04-Lab02-othmane.html",
    "title": "Assignment 04 — Lab 02",
    "section": "",
    "text": "The objective of this lab is to apply visual reporting and storytelling techniques to analyze job market data provided by Lightcast. Through this exercise, we will use PySpark and data visualization tools to explore, clean, and interpret a large dataset of job postings. The goal is to transform raw data into meaningful insights by identifying trends, patterns, and relationships between variables such as education level, remote work type, experience, and salary. This lab emphasizes not only technical proficiency in data processing but also the ability to communicate analytical findings effectively through visual narratives."
  },
  {
    "objectID": "assignment04-Lab02-othmane.html#typecast-key-columns",
    "href": "assignment04-Lab02-othmane.html#typecast-key-columns",
    "title": "Assignment 04 — Lab 02",
    "section": "3.1 Typecast Key Columns:",
    "text": "3.1 Typecast Key Columns:\n\n\nCode\n# Import necessary libraries\nfrom pyspark.sql import functions as F, types as T\nimport os\n\n#  Select and typecast key columns\n# Convert salary fields to numeric values for later analysis\nclean_df = (\n    df.withColumn(\"SALARY_FROM\", F.col(\"SALARY_FROM\").cast(T.DoubleType()))\n      .withColumn(\"SALARY_TO\", F.col(\"SALARY_TO\").cast(T.DoubleType()))\n      .withColumn(\"MAX_YEARS_EXPERIENCE\", F.col(\"MAX_YEARS_EXPERIENCE\").cast(T.DoubleType()))\n)\n# Handle missing values\n# Fill missing numeric values with 0 (or another placeholder if desired)\nclean_df = clean_df.fillna({\"SALARY_FROM\": 0, \"SALARY_TO\": 0, \"MAX_YEARS_EXPERIENCE\": 0})\n\n# Optional: Calculate average salary column for convenience\nclean_df = clean_df.withColumn(\n    \"AVERAGE_SALARY\",\n    (F.col(\"SALARY_FROM\") + F.col(\"SALARY_TO\")) / 2\n)\n\n# Show a quick sample\nclean_df.select(\"SALARY_FROM\", \"SALARY_TO\", \"AVERAGE_SALARY\", \"MAX_YEARS_EXPERIENCE\").show(5, truncate=False)\n\n\n+-----------+---------+--------------+--------------------+\n|SALARY_FROM|SALARY_TO|AVERAGE_SALARY|MAX_YEARS_EXPERIENCE|\n+-----------+---------+--------------+--------------------+\n|0.0        |0.0      |0.0           |2.0                 |\n|0.0        |0.0      |0.0           |3.0                 |\n|0.0        |0.0      |0.0           |0.0                 |\n|0.0        |0.0      |0.0           |0.0                 |\n|35000.0    |150000.0 |92500.0       |0.0                 |\n+-----------+---------+--------------+--------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "assignment04-Lab02-othmane.html#save-the-cleaned-dataset",
    "href": "assignment04-Lab02-othmane.html#save-the-cleaned-dataset",
    "title": "Assignment 04 — Lab 02",
    "section": "3.2 Save the Cleaned Dataset",
    "text": "3.2 Save the Cleaned Dataset\nNow that the dataset has been cleaned and typecasted, it is saved as a new CSV file for later use.\n\n\nCode\n# Save cleaned dataset safely\nimport os, glob, shutil\n\n# keep only the columns needed downstream \ncols_to_keep = [c for c in [\n    \"SALARY_FROM\",\"SALARY_TO\",\"AVERAGE_SALARY\",\"MAX_YEARS_EXPERIENCE\",\n    # \"EDUCATION_NAME\",\"REMOTE_TYPE_NAME\",\"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n] if c in clean_df.columns]\n\ntmp_dir   = \"_tmp_clean_csv\"\nfinal_csv = \"data/lightcast_cleaned.csv\"\n\n# CSV part from Spark\n(clean_df\n .select(cols_to_keep) if cols_to_keep else clean_df\n).coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(tmp_dir)\n\n# Move part-*.csv to final path\npart_files = glob.glob(os.path.join(tmp_dir, \"part-*.csv\"))\nif not part_files:\n    raise RuntimeError(\"No part file found after Spark write.\")\nif os.path.exists(final_csv):\n    os.remove(final_csv)\nshutil.move(part_files[0], final_csv)\nshutil.rmtree(tmp_dir, ignore_errors=True)\n\nprint(\"✅ Cleaned dataset saved.\")\nprint(f\"📁 File location: {final_csv}\")\n\n\n✅ Cleaned dataset saved.\n📁 File location: data/lightcast_cleaned.csv"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assignment 04 — Overview",
    "section": "",
    "text": "Overview\nLinks above (Lab 01 / Lab 02) open the labs."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html",
    "href": "assignment04-Lab01-othmane.html",
    "title": "Assignment 04 — Lab 01",
    "section": "",
    "text": "This report analyzes job postings from the Lightcast Job Market dataset, exploring salary trends, employment types, skill demand, and more.\nAll visualizations are interactive, allowing you to hover and explore insights dynamically."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation",
    "href": "assignment04-Lab01-othmane.html#explanation",
    "title": "Assignment 04 — Lab 01",
    "section": "3.1 ✏️ Explanation",
    "text": "3.1 ✏️ Explanation\nThe box plot shows how salary levels vary across different employment types. Full-time positions generally have higher median salaries and a wider pay range, reflecting greater earning potential but also more variability. In contrast, part-time and contract roles exhibit lower median salaries with tighter ranges, suggesting more consistency but fewer high-paying opportunities."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-1",
    "href": "assignment04-Lab01-othmane.html#explanation-1",
    "title": "Assignment 04 — Lab 01",
    "section": "4.1 ✏️ Explanation",
    "text": "4.1 ✏️ Explanation\nThe chart shows that salary levels vary notably across industries. The Information and Accommodation and Food Services sectors exhibit the highest median and upper-range salaries, suggesting strong compensation potential in these fields. Meanwhile, industries like Administrative Support and Retail Trade tend to offer lower median salaries, reflecting more standardized pay structures and fewer high-paying roles."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-2",
    "href": "assignment04-Lab01-othmane.html#explanation-2",
    "title": "Assignment 04 — Lab 01",
    "section": "5.1 ✏️ Explanation",
    "text": "5.1 ✏️ Explanation\nThe trend line reveals noticeable fluctuations in job posting activity, indicating that hiring demand changes frequently over time. Peaks suggest periods of intensified recruitment, possibly driven by seasonal hiring cycles or new project launches, while the dips represent slower hiring phases. Overall, the data highlights a dynamic job market with recurring surges in posting volume."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-3",
    "href": "assignment04-Lab01-othmane.html#explanation-3",
    "title": "Assignment 04 — Lab 01",
    "section": "6.1 ✏️ Explanation",
    "text": "6.1 ✏️ Explanation\nThe chart shows that Data Analyst is by far the most frequently posted job title, indicating a high market demand for data-focused professionals. Other roles like Unclassified, Enterprise Architect, and Data Engineer also appear prominently, reflecting the growing need for both analytical and technical expertise in data-driven organizations."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-4",
    "href": "assignment04-Lab01-othmane.html#explanation-4",
    "title": "Assignment 04 — Lab 01",
    "section": "7.1 ✏️ Explanation",
    "text": "7.1 ✏️ Explanation\nThe chart shows that a large majority of job postings do not specify a remote type, while around 17% explicitly offer remote positions. A smaller portion of listings are hybrid or partially remote, indicating that while remote work is available, most employers still emphasize on-site or unspecified work arrangements."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-5",
    "href": "assignment04-Lab01-othmane.html#explanation-5",
    "title": "Assignment 04 — Lab 01",
    "section": "8.1 ✏️ Explanation",
    "text": "8.1 ✏️ Explanation\nThe chart shows that professional, scientific, and technical services industries have the highest demand for skills, especially in communication, data analysis, and leadership. Across most industries, soft skills like communication and problem-solving appear as consistently essential, highlighting their broad value alongside technical expertise such as SQL and computer science."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-6",
    "href": "assignment04-Lab01-othmane.html#explanation-6",
    "title": "Assignment 04 — Lab 01",
    "section": "9.1 ✏️ Explanation",
    "text": "9.1 ✏️ Explanation\nThis chart shows how median annualized salaries vary across the most common job titles in the dataset. Larger bubbles represent job titles with a higher number of postings, while color indicates the relative salary level. From the visualization, we can see that technical and leadership roles such as “Data Architects,” “Principal Data Engineers,” and “SAP Data Analytics Managers” tend to offer the highest median salaries (often exceeding $160K), while more common positions like “Data Analyst” or “Business Intelligence Analyst” have lower median pay but significantly higher posting volume, reflecting strong demand for analytical talent across industries."
  },
  {
    "objectID": "assignment04-Lab01-othmane.html#explanation-7",
    "href": "assignment04-Lab01-othmane.html#explanation-7",
    "title": "Assignment 04 — Lab 01",
    "section": "10.1 ✏️ Explanation",
    "text": "10.1 ✏️ Explanation\nThe Sankey diagram shows how various job titles, such as Data Analyst, Data Modeler, and ERP Business Analyst, flow into a single ONET occupation category, Business Intelligence Analysts. This highlights that many data-related positions ultimately map to the same occupational classification, reflecting the overlap and convergence of skills within business intelligence and analytics roles."
  },
  {
    "objectID": "assignment04-Lab03-othmane.html",
    "href": "assignment04-Lab03-othmane.html",
    "title": "Assignment 04 — Lab 03",
    "section": "",
    "text": "The objective of this lab is to apply PySpark to perform regression modeling on employment data from the Lightcast Job Postings dataset. In this exercise, we use Spark to process and prepare a large dataset for salary prediction, engineer relevant features from structured columns, and train a Linear Regression model. We will evaluate model performance using RMSE and R², visualize predictions through diagnostic plots, and conclude by pushing the completed analysis to GitHub for submission."
  },
  {
    "objectID": "assignment04-Lab03-othmane.html#generalized-linear-regression-summary",
    "href": "assignment04-Lab03-othmane.html#generalized-linear-regression-summary",
    "title": "Assignment 04 — Lab 03",
    "section": "5.1 Generalized Linear Regression Summary",
    "text": "5.1 Generalized Linear Regression Summary\n\n\nCode\n# ========================= Generalized Linear Regression Summary (FULL REPLACEMENT) =========================\n# Robust GLR (log-salary) with label auto-detection, safe parsing, Boolean categorical fix,\n# aggressive back-off, and clear diagnostics.\n\n# ---------------------------------- Imports ----------------------------------\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, when, lit, stddev_samp, length\nfrom pyspark.sql.types import BooleanType\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.regression import GeneralizedLinearRegression\nimport numpy as np, pandas as pd, os\n\ntry:\n    from IPython.display import display, HTML\n    _CAN_HTML = True\nexcept Exception:\n    _CAN_HTML = False\n\n# ---------------------------------- Preconditions ----------------------------------\ntry:\n    spark  # noqa\nexcept NameError:\n    raise RuntimeError(\"SparkSession 'spark' is not defined. Create it earlier in your notebook/script.\")\n\ntry:\n    df  # noqa\nexcept NameError:\n    raise RuntimeError(\"Base DataFrame 'df' is not defined. Load your dataset into 'df' earlier.\")\n\n# ---------------------------------- Safe numeric parsers ----------------------------------\ndef parse_scalar_expr(c):\n    # Extract first numeric token; handle 'k' = thousand\n    expr = f\"try_cast(regexp_replace(regexp_extract(`{c}`,'([-+]?\\\\d[\\\\d,]*\\\\.?\\\\d*)',1), ',', '') AS double)\"\n    num = F.expr(expr)\n    has_k = F.lower(F.col(c)).like('%k%')\n    return F.when(has_k & num.isNotNull(), num * 1000.0).otherwise(num)\n\ndef parse_range_mid_expr(c):\n    # Mid-point for ranges like \"80k - 100k\"\n    lo = F.expr(\n        f\"try_cast(regexp_replace(regexp_extract(`{c}`,'([-+]?\\\\d[\\\\d,]*\\\\.?\\\\d*)\\\\s*[-–—]\\\\s*[-+]?\\\\d',1), ',', '') AS double)\"\n    )\n    hi = F.expr(\n        f\"try_cast(regexp_replace(regexp_extract(`{c}`,'[-–—]\\\\s*([-+]?\\\\d[\\\\d,]*\\\\.?\\\\d*)',1), ',', '') AS double)\"\n    )\n    has_k = F.lower(F.col(c)).like('%k%')\n    lo = F.when(has_k & lo.isNotNull(), lo * 1000.0).otherwise(lo)\n    hi = F.when(has_k & hi.isNotNull(), hi * 1000.0).otherwise(hi)\n    return (lo + hi) / 2.0\n\ndef parse_numeric(c):\n    mid = parse_range_mid_expr(c)\n    sc  = parse_scalar_expr(c)\n    return F.when(mid.isNotNull(), mid).when(sc.isNotNull(), sc).otherwise(F.lit(None).cast('double'))\n\n# ---------------------------------- Label auto-detection ----------------------------------\ndef autodetect_label_expr(df_in):\n    \"\"\"Pick the best salary/pay/wage/compensation column(s) by maximizing kept non-null, positive rows.\"\"\"\n    import re\n    cols = df_in.columns\n\n    def looks_like_money(name):\n        n = name.lower()\n        return any(k in n for k in [\"salary\", \"pay\", \"wage\", \"compensat\", \"income\", \"earn\"])\n\n    # Pair candidates: *_FROM with *_TO (or *_Min/_Max etc.)\n    cand_pairs = []\n    for c in cols:\n        if looks_like_money(c) and c.endswith((\"_FROM\", \"_Min\", \"_LOW\", \"_LOWER\", \"_MIN\")):\n            stem = re.sub(r\"(_FROM|_Min|_LOW|_LOWER|_MIN)$\", \"\", c)\n            for mate in [stem + \"_TO\", stem + \"_Max\", stem + \"_HIGH\", stem + \"_UPPER\", stem + \"_MAX\"]:\n                if mate in cols:\n                    cand_pairs.append((c, mate))\n\n    # Evaluate pairs\n    best = None\n    best_n = -1\n    for lo, hi in cand_pairs:\n        tmp = (df_in\n               .select(parse_numeric(lo).alias(\"lo\"), parse_numeric(hi).alias(\"hi\"))\n               .withColumn(\"m\", (F.col(\"lo\") + F.col(\"hi\")) / 2.0)\n               .where(F.col(\"m\").isNotNull())\n               .where(~F.isnan(\"m\"))\n               .where(F.col(\"m\") &gt; 0))\n        n = tmp.count()\n        if n &gt; best_n:\n            best_n = n\n            best = (lo, hi)\n    if best is not None and best_n &gt; 0:\n        lo, hi = best\n        return (parse_numeric(lo) + parse_numeric(hi)) / 2.0, f\"{lo}+{hi}\"\n\n    # Single-column candidates\n    singles = [c for c in cols if looks_like_money(c)]\n    best_c, best_n = None, -1\n    for c in singles:\n        tmp = (df_in\n               .select(parse_numeric(c).alias(\"v\"))\n               .where(F.col(\"v\").isNotNull())\n               .where(~F.isnan(\"v\"))\n               .where(F.col(\"v\") &gt; 0))\n        n = tmp.count()\n        if n &gt; best_n:\n            best_n, best_c = n, c\n    if best_c and best_n &gt; 0:\n        return parse_numeric(best_c), best_c\n\n    # Last resort: numeric SALARY column if present\n    for c, t in df_in.dtypes:\n        if c.upper() == \"SALARY\" and t in (\"double\", \"float\", \"int\", \"bigint\"):\n            return F.col(c).cast(\"double\"), c\n    return None, None\n\n# ---------------------------------- Build label & diagnostics ----------------------------------\nlabel_expr, label_source = autodetect_label_expr(df)\nif label_expr is None:\n    if \"SALARY\" in df.columns:\n        label_expr, label_source = parse_numeric(\"SALARY\"), \"SALARY\"\n    elif set([\"SALARY_FROM\", \"SALARY_TO\"]).issubset(df.columns):\n        label_expr, label_source = (parse_numeric(\"SALARY_FROM\") + parse_numeric(\"SALARY_TO\")) / 2.0, \"SALARY_FROM+SALARY_TO\"\n    elif \"SALARY_TO\" in df.columns:\n        label_expr, label_source = parse_numeric(\"SALARY_TO\"), \"SALARY_TO\"\n    else:\n        label_expr, label_source = parse_numeric(\"SALARY_FROM\"), \"SALARY_FROM\"\n\ndf = (df.withColumn(\"label\", label_expr)\n        .where(F.col(\"label\").isNotNull())\n        .where(~F.isnan(\"label\"))\n        .where(F.col(\"label\") &gt; 0))\ndf = df.withColumn(\"label_log\", F.log1p(F.col(\"label\")))\nn_label_rows = df.count()\nprint(f\"✅ label & label_log ready | label source: {label_source} | rows after label filter: {n_label_rows:,}\")\nif n_label_rows == 0:\n    raise RuntimeError(\"All rows dropped by label parsing. Inspect your dataset headers and salary/pay/wage columns.\")\n\n# ---------------------------------- Engineer numeric features ----------------------------------\nnum_specs = [\n    (\"MIN_YEARS_EXPERIENCE\", \"MIN_YEARS_EXPERIENCE_num\"),\n    (\"MAX_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE_num\"),\n    (\"MODELED_DURATION\",     \"MODELED_DURATION_num\"),\n    (\"SALARY_TO\",            \"SALARY_TO_num\"),\n]\nfor raw, out in num_specs:\n    if raw in df.columns:\n        df = df.withColumn(out, parse_numeric(raw))\n\n# z-score available numerics\nfor _, out in num_specs:\n    if out in df.columns:\n        s = df.select(F.mean(out).alias(\"mu\"), F.stddev_samp(out).alias(\"sd\")).first()\n        mu = float(s.mu) if s.mu is not None else 0.0\n        sd = float(s.sd) if s.sd not in (None, 0.0) else 1.0\n        df = df.withColumn(f\"z_{out}\", (col(out) - lit(mu)) / lit(sd))\n\nnumeric_cols = [c for c in [f\"z_{n}\" for _, n in num_specs] if c in df.columns]\nif not numeric_cols:\n    # Fallback numeric from text length\n    for t in [\"TITLE\", \"TITLE_NAME\", \"TITLE_RAW\", \"BODY\"]:\n        if t in df.columns:\n            df = df.withColumn(\"TITLE_LEN_num\", length(col(t)).cast(\"double\"))\n            numeric_cols = [\"TITLE_LEN_num\"]\n            break\n\n# ---------------------------------- Candidate categoricals ----------------------------------\ncat_raw = [c for c in [\"REMOTE_TYPE_NAME\", \"EMPLOYMENT_TYPE_NAME\", \"EDUCATION_LEVELS_NAME\", \"COMPANY_IS_STAFFING\"] if c in df.columns]\n\n# Ensure boolean categoricals are strings and fill nulls (StringIndexer requires string or numeric)\nfor c in list(cat_raw):\n    if c in df.columns:\n        dt = df.schema[c].dataType\n        if isinstance(dt, BooleanType):\n            df = df.withColumn(c, F.when(F.col(c).isNull(), F.lit(\"Unknown\"))\n                                 .otherwise(F.col(c).cast(\"string\")))\n        else:\n            # also fill nulls for existing strings\n            if \"StringType\" in str(dt):\n                df = df.withColumn(c, F.when(F.col(c).isNull(), F.lit(\"Unknown\")).otherwise(F.col(c)))\n\n# Quick diagnostics\ndef nn_counts(df_in, cols):\n    out = {}\n    for c in cols:\n        if c in df_in.columns:\n            out[c] = df_in.where(F.col(c).isNotNull() & ~F.isnan(c)).count()\n    return out\n\nprint(\"ℹ️ Non-null counts (numerics):\", nn_counts(df, numeric_cols))\nprint(\"ℹ️ Distinct levels (categoricals):\", {c: df.select(F.countDistinct(F.col(c))).first()[0] for c in cat_raw})\n\n# ---------------------------------- Helpers: cap/encode/impute/assemble/fit ----------------------------------\ndef cap_top_k(df_in, c, k):\n    top = (df_in.groupBy(c).count().orderBy(F.desc(\"count\")).limit(max(k - 1, 1))\n           .select(c).rdd.flatMap(lambda r: r).collect())\n    keep = set([v for v in top if v is not None])\n    cc = f\"{c}_top\"\n    out = df_in.withColumn(cc, F.when(col(c).isin(list(keep)), col(c)).otherwise(lit(\"Other\")))\n    return out, cc\n\ndef encode_cats(df_in, cols, k):\n    out = df_in\n    oh_cols = []\n    for c in cols:\n        if c not in out.columns:\n            continue\n        # cap top-k then index + one-hot (degenerate cats skipped)\n        out, capped = cap_top_k(out, c, k)\n        if out.select(capped).distinct().count() &lt; 2:\n            continue\n        idx, oh = f\"{capped}_idx\", f\"{capped}_oh\"\n        out = StringIndexer(inputCol=capped, outputCol=idx, handleInvalid=\"keep\").fit(out).transform(out)\n        out = OneHotEncoder(inputCols=[idx], outputCols=[oh], dropLast=True).fit(out).transform(out)\n        oh_cols.append(oh)\n    return out, oh_cols\n\ndef impute_median(df_in, cols):\n    out = df_in\n    for c in cols:\n        if c in out.columns:\n            med = out.approxQuantile(c, [0.5], 1e-3)[0] if out.where(F.col(c).isNotNull()).count() else 0.0\n            out = out.withColumn(c, when(col(c).isNull() | F.isnan(c), lit(med)).otherwise(col(c)))\n    return out\n\ndef assemble(df_in, y_col, numeric, cats, k):\n    tmp, ohe_cols = encode_cats(df_in, cats, k) if cats else (df_in, [])\n    num_keep_local = [c for c in numeric if c in tmp.columns]\n    tmp = impute_median(tmp, num_keep_local)\n    inputs = num_keep_local + ohe_cols\n    if not inputs:\n        return None, [], 0\n    fe = (VectorAssembler(inputCols=inputs, outputCol=\"features\")\n          .transform(tmp)\n          .select(F.col(y_col).alias(y_col), \"features\")\n          .where(F.col(\"features\").isNotNull()))\n    # derive feature count from metadata\n    meta = fe.schema[\"features\"].metadata\n    p = 0\n    if \"ml_attr\" in meta and \"attrs\" in meta[\"ml_attr\"]:\n        for k2 in (\"binary\", \"numeric\"):\n            p += len(meta[\"ml_attr\"][\"attrs\"].get(k2, []))\n    return fe, inputs, p\n\ndef fit_glr(df_in, y_col, reg=1e-3, fitIntercept=True):\n    glr = GeneralizedLinearRegression(\n        featuresCol=\"features\", labelCol=y_col,\n        family=\"gaussian\", link=\"identity\",\n        regParam=reg, fitIntercept=fitIntercept\n    )\n    m = glr.fit(df_in)\n    return m, m.summary\n\n# ---------------------------------- DoF-controlled fit (robust) ----------------------------------\nlabel_col = \"label_log\"\ndf_work = df.where(F.col(label_col).isNotNull()).where(~F.isnan(F.col(label_col)))\n\nn_label = df_work.count()\nmin_dof = 8 if n_label &gt;= 200 else 4 if n_label &gt;= 50 else 2\nreg = 1e-3\ninitial_topk, min_topk = 8, 2\ntopk = initial_topk\nnum_keep = list(numeric_cols)\ntried_numeric_only = False\ninjected_title_len = False\n\nattempts = 0\nwhile True:\n    attempts += 1\n    df_fe, inputs, p = assemble(df_work, label_col, num_keep, (cat_raw if not tried_numeric_only else []), topk)\n\n    # If assembler had no inputs -&gt; try numeric-only, then inject TITLE_LEN\n    if df_fe is None or not inputs:\n        if not tried_numeric_only:\n            tried_numeric_only = True\n            topk = min_topk\n            continue\n        if not injected_title_len:\n            for t in [\"TITLE\", \"TITLE_NAME\", \"TITLE_RAW\", \"BODY\"]:\n                if t in df_work.columns:\n                    df_work = df_work.withColumn(\"TITLE_LEN_num\", F.length(F.col(t)).cast(\"double\"))\n                    num_keep = list(set(num_keep + [\"TITLE_LEN_num\"]))\n                    injected_title_len = True\n                    break\n            continue\n        raise RuntimeError(\"No usable features available to assemble.\")\n\n    n_eff = df_fe.count()\n\n    # If we lost all rows, try backing off before erroring\n    if n_eff == 0:\n        print(\"⚠️ 0 rows after assemble → backing off features…\")\n        if not tried_numeric_only:\n            tried_numeric_only = True\n            topk = min_topk\n            continue\n        if not injected_title_len:\n            for t in [\"TITLE\", \"TITLE_NAME\", \"TITLE_RAW\", \"BODY\"]:\n                if t in df_work.columns:\n                    df_work = df_work.withColumn(\"TITLE_LEN_num\", F.length(F.col(t)).cast(\"double\"))\n                    num_keep = list(set(num_keep + [\"TITLE_LEN_num\"]))\n                    injected_title_len = True\n                    break\n            continue\n        if topk &gt; min_topk:\n            topk = max(min_topk, topk // 2)\n            continue\n        print(\"Inputs used:\", inputs[:12], \"...\" if len(inputs) &gt; 12 else \"\")\n        raise RuntimeError(\"No rows left after preparation. Check label parsing and input columns.\")\n\n    dof = n_eff - (p + 1)\n    print(f\"[Attempt {attempts}] rows={n_eff:,} | features≈{p:,} | DoF={dof:,} | topK={topk} | numerics={len(num_keep)} | numeric_only={tried_numeric_only}\")\n\n    if (dof &gt; min_dof and p &gt; 0):\n        try:\n            model, summary = fit_glr(df_fe, label_col, reg=reg, fitIntercept=True)\n            print(\"✅ Model succeeded | DoF:\", dof, \"| features≈\", p, \"| topK:\", topk, \"| numeric_only=\", tried_numeric_only)\n            print(\"Using predictors:\", inputs)\n            break\n        except Exception as e:\n            print(\"  Fit failed → shrinking:\", str(e)[:160])\n\n    # Shrink path\n    if topk &gt; min_topk and not tried_numeric_only:\n        topk = max(min_topk, topk // 2)\n        continue\n    if len(num_keep) &gt; 1:\n        sd_map = df_work.agg(*[stddev_samp(c).alias(c) for c in num_keep]).first().asDict()\n        num_keep = sorted(num_keep, key=lambda c: float(sd_map.get(c) or 0.0), reverse=True)[:-1]\n        continue\n    if not tried_numeric_only:\n        tried_numeric_only = True\n        topk = min_topk\n        continue\n    if dof &gt; 1 and p &gt; 0:\n        try:\n            model, summary = fit_glr(df_fe, label_col, reg=max(reg, 1e-2), fitIntercept=True)\n            print(\"⚠️ Proceeding with relaxed fit (tiny data).\")\n            break\n        except Exception as e:\n            print(\"  Final fit failed:\", str(e)[:160])\n            try:\n                model, summary = fit_glr(df_fe, label_col, reg=1e-1, fitIntercept=True)\n                print(\"⚠️ Proceeding with stronger regularization.\")\n                break\n            except Exception as e2:\n                print(\"  Ridge(0.1) failed:\", str(e2)[:160])\n                raise\n    raise RuntimeError(f\"Could not reach a stable fit (rows={n_eff}, features≈{p}). Consider fewer predictors or verifying label parsing.\")\n\n# ---------------------------------- Named coefficient table ----------------------------------\ndef feature_names_from_meta(df_in):\n    meta = df_in.schema[\"features\"].metadata\n    names = []\n    if \"ml_attr\" in meta and \"attrs\" in meta[\"ml_attr\"]:\n        for k in (\"binary\", \"numeric\"):\n            names += [a[\"name\"] for a in sorted(meta[\"ml_attr\"][\"attrs\"].get(k, []), key=lambda a: a[\"idx\"])]\n    return names\n\nnames = feature_names_from_meta(df_fe)\ncoef  = np.array(model.coefficients.toArray())\nintercept = float(model.intercept)\n\ndef safe_arr(getter, size):\n    try:\n        arr = np.array(getter())\n        if len(arr) == size + 1:  # intercept included at end\n            return np.r_[arr[-1], arr[:-1]]\n        if len(arr) == size:\n            return np.r_[np.nan, arr]\n        return np.r_[arr, [np.nan] * ((size + 1) - len(arr))]\n    except Exception:\n        return np.r_[np.nan, [np.nan] * size]\n\nse = safe_arr(lambda: summary.coefficientStandardErrors, len(coef))\ntv = safe_arr(lambda: summary.tValues,                  len(coef))\ntry:\n    _ = summary.residualDegreeOfFreedom\n    pv = safe_arr(lambda: summary.pValues, len(coef))\nexcept Exception:\n    pv = np.r_[np.nan, [np.nan] * len(coef)]\n\ntable = pd.DataFrame({\n    \"Feature\":  [\"(Intercept)\"] + names,\n    \"Estimate\": np.r_[intercept, coef],\n    \"Std Error\": se,\n    \"t-stat\":   tv,\n    \"P-Value\":  pv\n})\n\n# Pretty print\nif _CAN_HTML:\n    pretty = table.copy()\n    for c in [\"Estimate\", \"Std Error\", \"t-stat\", \"P-Value\"]:\n        pretty[c] = pretty[c].apply(lambda x: f\"{x:,.4f}\" if pd.notnull(x) and np.isfinite(x) else \"\")\n    display(HTML(pretty.to_html(index=False, classes=\"table table-striped table-sm\", border=0)))\nelse:\n    print(table.head(15).to_string(index=False))\n\nprint(\"\\n=== GLR (log-salary) SUMMARY ===\")\nprint(\"Residual DoF:\", getattr(summary, \"residualDegreeOfFreedom\", None))\nprint(\"Null Deviance:\", getattr(summary, \"nullDeviance\", None))\nprint(\"Residual Deviance:\", getattr(summary, \"deviance\", None))\nprint(\"AIC:\", getattr(summary, \"aic\", None))\n\n# Save CSV\nos.makedirs(\"_output\", exist_ok=True)\nout_csv = \"_output/glr_coefficients_named.csv\"\ntable.to_csv(out_csv, index=False)\nprint(f\"📁 Saved table to {out_csv}\")\n# =================================================================================================\n\n\n✅ label & label_log ready | label source: SALARY | rows after label filter: 30,808\n\n\nℹ️ Non-null counts (numerics): {'z_MIN_YEARS_EXPERIENCE_num': 0, 'z_MAX_YEARS_EXPERIENCE_num': 0, 'z_MODELED_DURATION_num': 0, 'z_SALARY_TO_num': 0}\n\n\nℹ️ Distinct levels (categoricals): {'REMOTE_TYPE_NAME': 4, 'EMPLOYMENT_TYPE_NAME': 3, 'EDUCATION_LEVELS_NAME': 27, 'COMPANY_IS_STAFFING': 2}\n\n\n[Attempt 1] rows=30,808 | features≈21 | DoF=30,786 | topK=8 | numerics=4 | numeric_only=False\n\n\n✅ Model succeeded | DoF: 30786 | features≈ 21 | topK: 8 | numeric_only= False\nUsing predictors: ['z_MIN_YEARS_EXPERIENCE_num', 'z_MAX_YEARS_EXPERIENCE_num', 'z_MODELED_DURATION_num', 'z_SALARY_TO_num', 'REMOTE_TYPE_NAME_top_oh', 'EMPLOYMENT_TYPE_NAME_top_oh', 'EDUCATION_LEVELS_NAME_top_oh', 'COMPANY_IS_STAFFING_top_oh']\n\n\n\n\n\nFeature\nEstimate\nStd Error\nt-stat\nP-Value\n\n\n\n\n(Intercept)\n11.4935\n0.1884\n61.0072\n0.0000\n\n\nREMOTE_TYPE_NAME_top_oh_[None]\n0.0000\n\n0.0000\n1.0000\n\n\nREMOTE_TYPE_NAME_top_oh_Remote\n0.0000\n\n0.0000\n1.0000\n\n\nREMOTE_TYPE_NAME_top_oh_Hybrid Remote\n0.0000\n\n0.0000\n1.0000\n\n\nREMOTE_TYPE_NAME_top_oh_Not Remote\n0.0000\n\n0.0000\n1.0000\n\n\nEMPLOYMENT_TYPE_NAME_top_oh_Full-time (&gt; 32 hours)\n0.0069\n0.0679\n0.1011\n0.9195\n\n\nEMPLOYMENT_TYPE_NAME_top_oh_Part-time (â‰¤ 32 hours)\n0.0117\n0.0680\n0.1722\n0.8632\n\n\nEMPLOYMENT_TYPE_NAME_top_oh_Part-time / full-time\n-0.0429\n0.0687\n-0.6245\n0.5323\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"Bachelor's degree\"\\n]\n-0.0987\n0.0696\n-1.4176\n0.1563\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"No Education Listed\"\\n]\n0.0899\n0.1372\n0.6555\n0.5122\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"Bachelor's degree\",\\n \"Master's degree\"\\n]\n-0.1137\n0.1374\n-0.8275\n0.4080\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"Associate degree\",\\n \"Bachelor's degree\"\\n]\n-0.0445\n0.1378\n-0.3229\n0.7468\n\n\nEDUCATION_LEVELS_NAME_top_oh_Other\n0.0064\n0.0518\n0.1234\n0.9018\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"High school or GED\"\\n]\n-0.0116\n0.0519\n-0.2230\n0.8236\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"High school or GED\",\\n \"Bachelor's degree\"\\n]\n0.1122\n0.0519\n2.1609\n0.0307\n\n\nEDUCATION_LEVELS_NAME_top_oh_[\\n \"Master's degree\"\\n]\n0.0816\n0.0526\n1.5515\n0.1208\n\n\nCOMPANY_IS_STAFFING_top_oh_false\n-0.0874\n0.0526\n-1.6616\n0.0966\n\n\nCOMPANY_IS_STAFFING_top_oh_true\n-0.6842\n0.0534\n-12.8239\n0.0000\n\n\nz_MIN_YEARS_EXPERIENCE_num\n-0.1511\n0.0534\n-2.8277\n0.0047\n\n\nz_MAX_YEARS_EXPERIENCE_num\n0.2381\n0.0535\n4.4538\n0.0000\n\n\nz_MODELED_DURATION_num\n0.0246\n0.0969\n0.2539\n0.7995\n\n\nz_SALARY_TO_num\n-0.0246\n0.0969\n-0.2539\n0.7995\n\n\n\n\n\n\n=== GLR (log-salary) SUMMARY ===\nResidual DoF: 30786\n\n\nNull Deviance: 5137.427195447666\nResidual Deviance: 4505.057949711946\n\n\nAIC: 28244.667895692117\n📁 Saved table to _output/glr_coefficients_named.csv\n\n\n\n5.1.1 Interpretation\nThe Generalized Linear Regression (GLR) model provided a thorough analysis of salary patterns from 30,808 job postings. The results indicate a strong model fit, with a Residual Deviance of 4505 compared to a Null Deviance of 5137. This suggests that the model has meaningful explanatory power. The AIC value of 28,245 further supports the conclusion that the model effectively identifies key salary determinants. The analysis revealed that experience-related variables and company type were the most significant factors affecting salaries. Higher required experience levels were associated with increased predicted salaries, while positions offered by staffing companies tended to have notably lower salary estimates. Education and employment type did not contribute as consistently, with many categories lacking strong statistical significance. Overall, these findings imply that employers value professional experience and the characteristics of the company more than formal education or the type of job when determining salary levels."
  }
]