---
title: "Assignment 04 ‚Äî Lab 04 Machine Learning on Scale"
subtitle: "Causal and Predictive Analytics in Spark"
author:
  - name: "Othmane Elouardi"
    affiliations:
      - id: bu
        name: "Boston University"
        city: "Boston"
        state: "MA"
date: "2025-10-08"
number-sections: true
format:
  html:
    theme:
      light: lux
      dark: slate
    toc: true
    toc-depth: 3
    toc-location: right
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-line-numbers: true
    highlight-style: a11y
    page-layout: article
    css: styles.css
    grid:
      body-width: 900px
      margin-width: 280px
execute:
  echo: true
  warning: false
  error: false
  freeze: auto
jupyter: env
---


# Introduction
PySpark is applied to large-scale employment data from the Lightcast Job Postings dataset to develop and evaluate salary prediction models. The workflow involves engineering key features from structured columns, training a Linear Regression model, and assessing performance using RMSE and R¬≤ metrics. Visual diagnostic plots are generated to interpret model accuracy and residual patterns. The final analysis is documented, version-controlled, and submitted via GitHub, demonstrating end-to-end data processing and predictive modeling on a distributed computing platform.

# Load the Dataset
 **PySpark** is used to load the *Lightcast Job Postings* dataset into a Spark DataFrame.  
This approach enables us to efficiently handle large datasets within the EC2 environment before proceeding with feature engineering and regression analysis.

```{python}
# Load the Lightcast dataset with PySpark

from pyspark.sql import SparkSession
import os

# Initialize Spark session 
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .config("spark.driver.memory", "1g")
    .getOrCreate()
)

# Define dataset path
csv_path = "data/lightcast_job_postings.csv"

# Check path validity
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"‚ùå Could not find {csv_path}. Please ensure the file exists in the data/ folder.")

# Load dataset
df = (
    spark.read
    .option("header", "true")       # First row as headers
    .option("inferSchema", "true")  # Auto-detect data types
    .option("multiLine", "true")    # Handle multi-line text fields
    .option("escape", "\"")         # Handle embedded quotes
    .csv(csv_path)
)

# Confirm load success
print("‚úÖ Dataset successfully loaded!")
df.printSchema()
df.show(5, truncate=False)
```



# Feature Engineering

Feature engineering is a crucial step in preparing the dataset for machine learning.  
This section cleans the data, encodes categorical variables, and constructs feature vectors for model training.

```{python}
# ======================================================
# FEATURE ENGINEERING
# ======================================================

from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler

# ------------------------------------------------------
# 1. Drop rows with missing target or key features
# ------------------------------------------------------
df_clean = df.dropna(subset=[
    "SALARY", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE",
    "EMPLOYMENT_TYPE_NAME", "STATE_NAME", "MIN_EDULEVELS_NAME"
])

print(f"‚úÖ Rows after dropping nulls: {df_clean.count():,}")

# ------------------------------------------------------
# 2. Create new feature: MIN_YEARS_EXPERIENCE_SQ
# ------------------------------------------------------
df_clean = df_clean.withColumn(
    "MIN_YEARS_EXPERIENCE_SQ",
    F.pow(F.col("MIN_YEARS_EXPERIENCE"), 2)
)

# ------------------------------------------------------
# 3. Encode categorical variables
# ------------------------------------------------------
cat_cols = ["EMPLOYMENT_TYPE_NAME", "STATE_NAME", "MIN_EDULEVELS_NAME"]

indexers = [
    StringIndexer(inputCol=c, outputCol=f"{c}_IDX", handleInvalid="keep")
    for c in cat_cols
]

encoders = [
    OneHotEncoder(inputCol=f"{c}_IDX", outputCol=f"{c}_OHE")
    for c in cat_cols
]

# Apply indexers sequentially
for idx in indexers:
    df_clean = idx.fit(df_clean).transform(df_clean)

# Apply one-hot encoders sequentially
for enc in encoders:
    df_clean = enc.fit(df_clean).transform(df_clean)

# ------------------------------------------------------
# 4. Assemble final features
# ------------------------------------------------------
numeric_cols = [
    "MIN_YEARS_EXPERIENCE", 
    "MAX_YEARS_EXPERIENCE", 
    "MIN_YEARS_EXPERIENCE_SQ"
]

ohe_cols = [f"{c}_OHE" for c in cat_cols]

assembler = VectorAssembler(
    inputCols=numeric_cols + ohe_cols,
    outputCol="features"
)

df_fe = assembler.transform(df_clean)

# ------------------------------------------------------
# 5. Show sample of processed data
# ------------------------------------------------------
df_fe.select(
    "SALARY", "MIN_YEARS_EXPERIENCE", "STATE_NAME", "features"
).show(5, truncate=False)

```

# Train/Test Split

Splitting the data into training and testing sets ensures that the model is evaluated on unseen data, which helps measure its generalization performance.  
Here an **80/20 split** is used, a common practice that provides a large enough training set while reserving sufficient data for evaluation.

```{python}
# ======================================================
# TRAIN / TEST SPLIT
# ======================================================

# Perform random split with reproducibility
train_df, test_df = df_fe.randomSplit([0.8, 0.2], seed=42)

# Display record counts for each subset
train_count = train_df.count()
test_count = test_df.count()
total_count = df_fe.count()

print(f"‚úÖ Total records: {total_count:,}")
print(f"üß† Training set: {train_count:,} rows ({train_count/total_count:.1%})")
print(f"üßæ Testing set:  {test_count:,} rows ({test_count/total_count:.1%})")

```




# Linear Regression

Zero-variance features to restore full rank, train a Linear Regression model for
prediction, and assemble a coefficient table with **SE, t, p, 95% CI** using LR/GLR or a
bootstrap fallback when analytic standard errors are unavailable.

```{python}
# ======================================================
# LINEAR REGRESSION with robust & fast inference
#   - zero-variance pruning (Summarizer.variance)
#   - LR (L-BFGS + tiny ridge) metrics on test set
#   - LR -> GLR -> Bootstrap fallback for SE/t/p
# ======================================================

from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression
from pyspark.ml.feature import VectorSlicer
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.stat import Summarizer
from pyspark.sql import functions as F
import math, numpy as np

# ---- Speed / stability toggles ----
DO_BOOTSTRAP = False       # turn ON only if you really need SEs; this saves lots of time
B = 8                      # bootstrap iterations if enabled
BOOTSTRAP_FRACTION = 0.85  # subsample per bootstrap to speed up
RIDGE = 1e-4               # tiny L2 to avoid singularities
MAXITER_LR = 200
MAXITER_GLR = 120
TOL = 1e-5

# -----------------------------
# Helpers
# -----------------------------
def extract_feature_names(df, features_col="features"):
    """Read feature names from Vector metadata; fallback to generic names."""
    meta = df.schema[features_col].metadata
    try:
        attrs = meta["ml_attr"]["attrs"]
        names = []
        for typ in ("binary", "numeric"):
            if typ in attrs:
                for a in attrs[typ]:
                    names.append(a.get("name", f"f_{a.get('idx', len(names))}"))
        return names
    except Exception:
        size = df.selectExpr(f"size({features_col}) as n").head().n
        return [f"feature_{i}" for i in range(size)]

def normal_pvalue_from_t(t):
    """Two-sided p-value from z/t using normal approx."""
    if t is None or t != t:
        return float("nan")
    return 2.0 * (1.0 - 0.5 * (1 + math.erf(abs(float(t))/math.sqrt(2))))

def safe_ci(beta, se, z=1.96):
    try:
        if se is None or se != se:
            return float("nan"), float("nan")
        return float(beta - z*se), float(beta + z*se)
    except Exception:
        return float("nan"), float("nan")

def build_table(coefs, ses, names, intercept=None, intercept_se=float("nan")):
    # Align lengths
    if len(names) != len(coefs):
        if len(names) > len(coefs):
            names = names[:len(coefs)]
        else:
            names += [f"feature_{i}" for i in range(len(names), len(coefs))]

    rows = []
    for n, b, se in zip(names, coefs, ses):
        t = float("nan") if (se is None or se != se or se == 0.0) else float(b / se)
        p = normal_pvalue_from_t(t)
        lo, hi = safe_ci(b, se)
        rows.append((n, float(b), float(se) if se == se else float("nan"), t, p, lo, hi))

    coef_df = spark.createDataFrame(rows, ["feature", "coef", "se", "t", "p", "ci_lo", "ci_hi"])

    # Intercept row (if provided)
    if intercept is not None:
        t_i = float("nan") if (intercept_se != intercept_se or intercept_se == 0.0) else float(intercept / intercept_se)
        p_i = normal_pvalue_from_t(t_i)
        lo_i, hi_i = safe_ci(intercept, intercept_se)
        irow = spark.createDataFrame(
            [("Intercept", float(intercept), float(intercept_se), t_i, p_i, lo_i, hi_i)],
            ["feature", "coef", "se", "t", "p", "ci_lo", "ci_hi"]
        )
        coef_df = irow.unionByName(coef_df)

    return coef_df

# ---------------------------------------------
# 1) Zero-variance pruning on TRAIN (robust)
# ---------------------------------------------
feat_names_full = extract_feature_names(train_df, "features")

var_row = train_df.select(
    Summarizer.variance(train_df["features"]).alias("var")
).first()

# Convert variance vector to 1-D numpy safely
variances = var_row["var"].toArray() if hasattr(var_row["var"], "toArray") else np.array(var_row["var"]).ravel()

keep_idx = [int(i) for i, v in enumerate(variances) if (v is not None) and not np.isnan(v) and v > 1e-12]
if not keep_idx:  # safety fallback
    keep_idx = list(range(len(variances)))

removed = len(variances) - len(keep_idx)
print(f"‚ÑπÔ∏è Removed {removed} zero-variance feature(s)." if removed > 0 else "‚ÑπÔ∏è No zero-variance features found.")

slicer = VectorSlicer(inputCol="features", outputCol="features_nz", indices=keep_idx)
train_nz = slicer.transform(train_df).drop("features").withColumnRenamed("features_nz", "features")
test_nz  = slicer.transform(test_df ).drop("features").withColumnRenamed("features_nz", "features")

feat_names = [feat_names_full[i] for i in keep_idx] if keep_idx else feat_names_full

# Cache & materialize once to avoid recomputation across fits
train_nz = train_nz.repartition(4).cache()
test_nz  = test_nz.repartition(4).cache()
_ = train_nz.count(); _ = test_nz.count()

# ---------------------------------------------
# 2) Fit Linear Regression (fast/stable) + TEST metrics
# ---------------------------------------------
lr = LinearRegression(
    featuresCol="features",
    labelCol="SALARY",
    predictionCol="prediction",
    fitIntercept=True,
    regParam=RIDGE,             # tiny L2 prevents singularities
    elasticNetParam=0.0,
    solver="l-bfgs",            # faster & avoids Cholesky path
    maxIter=MAXITER_LR,
    tol=TOL,
    standardization=True
)
lr_model = lr.fit(train_nz)
lr_sum   = lr_model.summary

e_rmse = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="rmse")
e_r2   = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="r2")
e_mae  = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="mae")

pred_test = lr_model.transform(test_nz).cache()
rmse_test = e_rmse.evaluate(pred_test)
r2_test   = e_r2.evaluate(pred_test)
mae_test  = e_mae.evaluate(pred_test)

print(f"‚úÖ Test RMSE: {rmse_test:,.2f}")
print(f"‚úÖ Test MAE : {mae_test:,.2f}")
print(f"‚úÖ Test R¬≤  : {r2_test:,.4f}")

print("\n--- Training Metrics (reference) ---")
print(f"RMSE: {lr_sum.rootMeanSquaredError:,.2f} | MAE: {lr_sum.meanAbsoluteError:,.2f} | R¬≤: {lr_sum.r2:,.4f}")

# ---------------------------------------------
# 3) Inference: LR -> GLR -> Bootstrap fallback
# ---------------------------------------------
coef_df_out = None
got_inference = False

# (a) Try LR analytic inference (may be unavailable)
try:
    ses_lr   = list(lr_sum.coefficientStandardErrors)
    coefs_lr = list(lr_model.coefficients.toArray())
    coef_df_out = build_table(coefs_lr, ses_lr, feat_names, intercept=lr_model.intercept, intercept_se=float("nan"))
    got_inference = True
    print("‚ÑπÔ∏è Using LR analytic standard errors.")
except Exception:
    pass

# (b) GLR fallback (Gaussian/identity) with tiny ridge and L-BFGS
if not got_inference:
    try:
        glr = GeneralizedLinearRegression(
            featuresCol="features", labelCol="SALARY", predictionCol="prediction_glr",
            family="gaussian", link="identity",
            fitIntercept=True,
            regParam=RIDGE,
            maxIter=MAXITER_GLR,
            tol=TOL
        )
        glr_model = glr.fit(train_nz)
        glr_sum   = glr_model.summary

        ses_all = list(glr_sum.coefficientStandardErrors)  # may include intercept as last element
        coefs_g = list(glr_model.coefficients.toArray())

        # Intercept SE often appears as the last element
        intercept_se = float("nan")
        if len(ses_all) == len(coefs_g) + 1:
            intercept_se = float(ses_all[-1])
            ses_g = ses_all[:-1]
        else:
            ses_g = ses_all

        coef_df_out = build_table(coefs_g, ses_g, feat_names, intercept=glr_model.intercept, intercept_se=intercept_se)
        got_inference = True
        print("‚ÑπÔ∏è Using GLR analytic standard errors.")
    except Exception:
        pass

# (c) Bootstrap fallback (lightweight) if analytic SEs are unavailable
if not got_inference:
    if DO_BOOTSTRAP:
        print(f"‚ö†Ô∏è Analytic SE unavailable ‚Äî running bootstrap (B={B}).")
        coef_mat = []
        for b in range(B):
            samp = train_nz.sample(withReplacement=True, fraction=BOOTSTRAP_FRACTION, seed=2025 + b)
            m = lr.fit(samp)
            coef_mat.append(m.coefficients.toArray())
        coef_mat = np.array(coef_mat)  # [B, p]
        coefs = lr_model.coefficients.toArray()
        ses   = np.std(coef_mat, axis=0, ddof=1)
        coef_df_out = build_table(list(coefs), list(ses), feat_names, intercept=lr_model.intercept, intercept_se=float("nan"))
        print("‚ÑπÔ∏è Bootstrap SE computed.")
    else:
        print("‚ÑπÔ∏è Skipping bootstrap; reporting coefficients without SE.")
        coefs = lr_model.coefficients.toArray()
        ses   = [float("nan")] * len(coefs)
        coef_df_out = build_table(list(coefs), ses, feat_names, intercept=lr_model.intercept, intercept_se=float("nan"))

# Order and display
coef_df_out = coef_df_out.withColumn("abs_t", F.abs(F.col("t"))).orderBy(F.desc_nulls_last("abs_t"))

print("\n--- Top 25 coefficients by |t| ---")
coef_df_out.select("feature", "coef", "se", "t", "p", "ci_lo", "ci_hi").show(25, truncate=False)

# save the full table
coef_df_out.coalesce(1).write.mode("overwrite").option("header", True).csv("output/linear_coef_table")

```



## Generalized Linear Regression Summary

```{python}
# ======================================================
# GLR SUMMARY (defensive, with bootstrap fallback)
# ======================================================

from pyspark.ml.regression import GeneralizedLinearRegression
from pyspark.sql import functions as F
import math, numpy as np

# ---------- safety: cache the pruned datasets ----------
try:
    train_nz = train_nz.cache(); test_nz = test_nz.cache()
    _ = train_nz.count(); _ = test_nz.count()
except NameError:
    # If you haven't run the train/test split + pruning cells yet
    raise RuntimeError("train_nz/test_nz not found. Run the Train/Test Split cell first.")

# ---------- settings (tweakable) ----------
DO_BOOTSTRAP = True
B = 8                    # keep modest for speed
BOOTSTRAP_FRACTION = 0.85
Z_95 = 1.96

# ---------- helpers ----------
def extract_feature_names(df, features_col="features"):
    meta = df.schema[features_col].metadata
    try:
        attrs = meta["ml_attr"]["attrs"]
        names = []
        for typ in ("binary", "numeric"):
            if typ in attrs:
                for a in attrs[typ]:
                    names.append(a.get("name", f"f_{a.get('idx', len(names))}"))
        return names
    except Exception:
        size = df.selectExpr(f"size({features_col}) as n").first().n
        return [f"feature_{i}" for i in range(size)]

def normal_pvalue_from_t(t):
    if t is None or (isinstance(t, float) and math.isnan(t)):
        return float("nan")
    z = abs(float(t))
    return 2.0 * (1.0 - 0.5 * (1 + math.erf(z / math.sqrt(2))))

def build_table(coefs, ses, names, intercept=None, intercept_se=float("nan")):
    # align lengths
    if len(names) != len(coefs):
        if len(names) > len(coefs):
            names = names[:len(coefs)]
        else:
            names += [f"feature_{i}" for i in range(len(names), len(coefs))]

    rows = []
    for n, b, se in zip(names, coefs, ses):
        if se is None or (isinstance(se, float) and math.isnan(se)) or se == 0.0:
            t = float("nan"); p = float("nan"); lo = float("nan"); hi = float("nan")
        else:
            t = float(b) / float(se)
            p = normal_pvalue_from_t(t)
            lo = float(b) - Z_95*float(se)
            hi = float(b) + Z_95*float(se)
        rows.append((n, float(b), float(se) if se == se else float("nan"), t, p, lo, hi))

    coef_df = spark.createDataFrame(rows, ["feature", "coef", "se", "t", "p", "ci_lo", "ci_hi"])

    # intercept row (optional)
    if intercept is not None:
        if (intercept_se is None) or (isinstance(intercept_se, float) and math.isnan(intercept_se)) or intercept_se == 0.0:
            t_i = float("nan"); p_i = float("nan"); lo_i = float("nan"); hi_i = float("nan")
        else:
            t_i = float(intercept) / float(intercept_se)
            p_i = normal_pvalue_from_t(t_i)
            lo_i = float(intercept) - Z_95*float(intercept_se)
            hi_i = float(intercept) + Z_95*float(intercept_se)
        irow = spark.createDataFrame(
            [("Intercept", float(intercept), float(intercept_se) if intercept_se==intercept_se else float("nan"),
              t_i, p_i, lo_i, hi_i)],
            ["feature", "coef", "se", "t", "p", "ci_lo", "ci_hi"]
        )
        coef_df = irow.unionByName(coef_df)

    return coef_df

# ---------- 1) Fit GLR and print model metrics (all getattr-guarded) ----------
glr = GeneralizedLinearRegression(
    featuresCol="features", labelCol="SALARY", predictionCol="prediction_glr",
    family="gaussian", link="identity", fitIntercept=True, regParam=0.0, maxIter=100
)
glr_model = glr.fit(train_nz)
glr_sum   = glr_model.summary

print("‚úÖ GLR Model Summary")
print(f"AIC: {getattr(glr_sum, 'aic', float('nan')):.2f}")
print(f"Deviance: {getattr(glr_sum, 'deviance', float('nan')):.2f}")
print(f"Dispersion: {getattr(glr_sum, 'dispersion', float('nan')):.4f}")
print(f"Null Deviance: {getattr(glr_sum, 'nullDeviance', float('nan')):.2f}")
print(f"Residual DF: {getattr(glr_sum, 'residualDegreeOfFreedom', float('nan'))}")

feat_names = extract_feature_names(train_nz, "features")
coefs = list(glr_model.coefficients.toArray())

# ---------- 2) Try analytic SE; else bootstrap; else NA ----------
coef_df_glr = None
intercept_se = float("nan")

try:
    ses_all = list(glr_sum.coefficientStandardErrors)  # may include intercept as last element
    if len(ses_all) == len(coefs) + 1:
        intercept_se = float(ses_all[-1]); ses = ses_all[:-1]
    else:
        ses = ses_all
    coef_df_glr = build_table(coefs, ses, feat_names, intercept=glr_model.intercept, intercept_se=intercept_se)
    print("‚ÑπÔ∏è Using GLR analytic standard errors.")
except Exception:
    print("‚ö†Ô∏è GLR analytic SE unavailable.")
    if DO_BOOTSTRAP:
        print(f"‚ö†Ô∏è Running GLR bootstrap (B={B}).")
        coef_mat = []
        for b in range(B):
            samp = train_nz.sample(withReplacement=True, fraction=BOOTSTRAP_FRACTION, seed=4100 + b)
            m = glr.fit(samp)
            coef_mat.append(m.coefficients.toArray())
        coef_mat = np.array(coef_mat)
        ses_bs   = np.std(coef_mat, axis=0, ddof=1)
        coef_df_glr = build_table(coefs, list(ses_bs), feat_names, intercept=glr_model.intercept, intercept_se=float("nan"))
        print("‚ÑπÔ∏è Bootstrap SE computed.")
    else:
        # As a last resort, output coefficients with NA SE (still a valid table)
        coef_df_glr = build_table(coefs, [float("nan")]*len(coefs), feat_names, intercept=glr_model.intercept)

# ---------- 3) Order and show ----------
coef_df_glr = coef_df_glr.withColumn("abs_t", F.abs(F.col("t"))).orderBy(F.desc_nulls_last("abs_t"))
print("\n--- Top 25 GLR coefficients by |t| ---")
coef_df_glr.select("feature", "coef", "se", "t", "p", "ci_lo", "ci_hi").show(25, truncate=False)

# Optional: save
coef_df_glr.coalesce(1).write.mode("overwrite").option("header", True).csv("output/glr_coef_table")


```


### Interpretation

The Generalized Linear Regression model produced an AIC of approximately 71,000, and the dispersion parameter aligns well with the salary scale. According to bootstrap-based standard errors, employment type and education level have the most significant impact on salary. Specifically, full-time positions and advanced degrees, such as Master‚Äôs and Ph.D., considerably increase the predicted salary, while part-time roles and lower education levels lead to a decrease. Geographic factors also play an important role, highlighting the variations in labor markets at the state level. Overall, the model effectively identifies and interprets key relationships while ensuring numerical stability through bootstrapped inference.





# Random Forest Regressor

```{python}
# ======================================================
# RANDOM FOREST REGRESSOR 
#   - trains on train_nz, evaluates on test_nz
#   - robust feature-name handling for importances
#   - caches data to avoid recomputation
# ======================================================

from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql import functions as F

# ----------- Hyperparameters (edit here) -----------
NUM_TREES = 200        # 100‚Äì500 per instructions
MAX_DEPTH = 6          # 4‚Äì10 per instructions
FEATURE_SUBSET = "sqrt"
SUBSAMPLE = 0.8
MAX_BINS = 64
SEED = 42

# ----------- Safety: ensure train/test exist, cache once -----------
try:
    train_nz  # noqa: F401
    test_nz   # noqa: F401
except NameError:
    raise RuntimeError("train_nz/test_nz not found. Run the train/test split + feature steps first.")

# Cache & materialize to prevent long DAG recomputes
train_nz = train_nz.repartition(4).cache()
test_nz  = test_nz.repartition(4).cache()
_ = train_nz.count(); _ = test_nz.count()

# ----------- Helper: robust feature names -----------
def extract_feature_names(df, features_col="features"):
    meta = df.schema[features_col].metadata
    try:
        attrs = meta["ml_attr"]["attrs"]
        names = []
        for typ in ("binary", "numeric"):
            if typ in attrs:
                for a in attrs[typ]:
                    names.append(a.get("name", f"f_{a.get('idx', len(names))}"))
        return names
    except Exception:
        size = df.selectExpr(f"size({features_col}) as n").first().n
        return [f"feature_{i}" for i in range(size)]

feature_names = extract_feature_names(train_nz, "features")

# ----------- 1) Train RF -----------
rf = RandomForestRegressor(
    featuresCol="features",
    labelCol="SALARY",
    predictionCol="prediction_rf",
    numTrees=NUM_TREES,
    maxDepth=MAX_DEPTH,
    featureSubsetStrategy=FEATURE_SUBSET,
    subsamplingRate=SUBSAMPLE,
    minInstancesPerNode=2,
    maxBins=MAX_BINS,
    seed=SEED
)
rf_model = rf.fit(train_nz)

# ----------- 2) Evaluate on TEST -----------
pred_rf = rf_model.transform(test_nz).cache()
_ = pred_rf.count()  # materialize once

e_rmse = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction_rf", metricName="rmse")
e_mae  = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction_rf", metricName="mae")
e_r2   = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction_rf", metricName="r2")

rmse_rf = e_rmse.evaluate(pred_rf)
mae_rf  = e_mae.evaluate(pred_rf)
r2_rf   = e_r2.evaluate(pred_rf)

print(f"üå≤ RF Test RMSE: {rmse_rf:,.2f}")
print(f"üå≤ RF Test MAE : {mae_rf:,.2f}")
print(f"üå≤ RF Test R¬≤  : {r2_rf:,.4f}")

# ----------- 3) Feature Importances (robust) -----------
imp_vec = rf_model.featureImportances  # SparseVector
# Convert to dense Python list aligned to the actual vector length
try:
    vec_len = pred_rf.selectExpr("size(features) as n").first().n
except Exception:
    vec_len = len(feature_names)

importances = [0.0] * vec_len
for idx, val in zip(imp_vec.indices, imp_vec.values):
    if idx < vec_len:
        importances[int(idx)] = float(val)

# Align names to the vector length
if len(feature_names) != vec_len:
    if len(feature_names) > vec_len:
        feature_names = feature_names[:vec_len]
    else:
        feature_names += [f"feature_{i}" for i in range(len(feature_names), vec_len)]

imp_rows = list(zip(feature_names, importances))
imp_df = spark.createDataFrame(imp_rows, ["feature", "rf_importance"]).orderBy(F.desc("rf_importance"))

print("\n--- Top 20 Random Forest feature importances ---")
imp_df.show(20, truncate=False)

# Optional: save importances
# imp_df.coalesce(1).write.mode("overwrite").option("header", True).csv("output/rf_feature_importances")
```





## Feature Importance Plot

```{python}
# ======================================================
# Random Forest Feature Importance ‚Äî Interactive + PNG
#  - Robust vector length detection (Vector UDT safe)
#  - Interactive Plotly bar chart
#  - Static PNG saved to ./_output/rf_feature_importance.png
# ======================================================

from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.functions import vector_to_array
from pyspark.sql import functions as F
import os

# ---------- 0) Safety: ensure train/test exist ----------
try:
    _ = train_nz.count(); _ = test_nz.count()
except NameError:
    raise RuntimeError("train_nz/test_nz not found. Run the RF training cell first.")

# ---------- 1) Get (or fit) RF model ----------
try:
    rf_model  # reuse if already trained
except NameError:
    rf = RandomForestRegressor(
        featuresCol="features", labelCol="SALARY", predictionCol="prediction_rf",
        numTrees=200, maxDepth=6, featureSubsetStrategy="sqrt",
        subsamplingRate=0.8, minInstancesPerNode=2, maxBins=64, seed=42
    )
    rf_model = rf.fit(train_nz)

# ---------- 2) Robust feature-name extraction ----------
def extract_feature_names(df, features_col="features"):
    meta = df.schema[features_col].metadata
    try:
        attrs = meta["ml_attr"]["attrs"]
        names = []
        for typ in ("binary", "numeric"):
            if typ in attrs:
                for a in attrs[typ]:
                    names.append(a.get("name", f"f_{a.get('idx', len(names))}"))
        return names
    except Exception:
        size = df.selectExpr(f"size({features_col}) as n").first().n
        return [f"feature_{i}" for i in range(size)]

try:
    feature_names = feat_names  # reuse from earlier if available
except NameError:
    feature_names = extract_feature_names(train_nz, "features")

# ---------- 3) Vector length (no AnalysisException) ----------
try:
    # Convert Vector UDT -> array, then size()
    vec_len = train_nz.select(
        F.size(vector_to_array(F.col("features"))).alias("n")
    ).first().n
except Exception:
    # Fallback to the trained model‚Äôs importances size
    vec_len = int(getattr(rf_model.featureImportances, "size", len(feature_names)))

# Align names to actual vector size
if len(feature_names) != vec_len:
    if len(feature_names) > vec_len:
        feature_names = feature_names[:vec_len]
    else:
        feature_names += [f"feature_{i}" for i in range(len(feature_names), vec_len)]

# ---------- 4) Build importances table ----------
imp_vec = rf_model.featureImportances  # SparseVector
importances = [0.0] * vec_len
for idx, val in zip(imp_vec.indices, imp_vec.values):
    if int(idx) < vec_len:
        importances[int(idx)] = float(val)

imp_rows = list(zip(feature_names, importances))
imp_df = spark.createDataFrame(imp_rows, ["feature", "rf_importance"]) \
              .orderBy(F.desc("rf_importance"))

topN = 10
top_df = imp_df.limit(topN)
top_pd = top_df.toPandas()

# ---------- 5) Interactive bar (Plotly) ----------
import plotly.express as px

fig = px.bar(
    top_pd.sort_values("rf_importance", ascending=True),
    x="rf_importance", y="feature",
    orientation="h",
    title=f"Top {topN} Random Forest Feature Importances",
    labels={"rf_importance": "Importance", "feature": "Feature"},
)
fig.update_layout(yaxis=dict(dtick=1), margin=dict(l=10, r=10, t=40, b=10))
fig.show()

# ---------- 6) Save static PNG to ./_output/ ----------
# Use matplotlib (and seaborn if available) to avoid Plotly/kaleido dependency.
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    use_sns = True
except Exception:
    use_sns = False

os.makedirs("_output", exist_ok=True)
plt.figure(figsize=(8, 5))
plot_data = top_pd.sort_values("rf_importance", ascending=True)

if use_sns:
    ax = sns.barplot(data=plot_data, x="rf_importance", y="feature")
else:
    ax = plt.barh(plot_data["feature"], plot_data["rf_importance"])

plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title(f"Top {topN} Random Forest Feature Importances")
plt.tight_layout()
plt.savefig("_output/rf_feature_importance.png", dpi=200)
plt.close()

print("‚úÖ Saved static plot to: _output/rf_feature_importance.png")

```


# Compare 3 Models ‚Äî GLR, Polynomial, RF


```{python}
# ======================================================
# Compare GLR, Polynomial LR (degree-2 on MIN_YEARS only), and RF
#  - Robust to pred column names (uses model.getPredictionCol())
#  - RMSE, MAE, AIC, BIC (RF uses Gaussian RSS pseudo AIC/BIC)
#  - Unified predictions on the same TEST set
#  - Interactive Plotly 2x2 grid + static PNG to _output/
# ======================================================

from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression, RandomForestRegressor
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql import functions as F
import numpy as np, math, os

# ---------- Safety: make sure pruned frames exist and are materialized ----------
try:
    train_nz, test_nz
except NameError:
    raise RuntimeError("train_nz/test_nz not found. Run your split + feature steps first.")

train_nz = train_nz.repartition(4).cache()
test_nz  = test_nz.repartition(4).cache()
_ = train_nz.count(); _ = test_nz.count()

# ---------- Common settings ----------
RIDGE = 1e-4
MAXITER_LR  = 200
MAXITER_GLR = 120
TOL  = 1e-5
SEED = 42

# ---------- Helper: AIC/BIC from residuals (Gaussian) ----------
def aic_bic_from_residuals(y_true, y_pred, k):
    """
    AIC = n*ln(RSS/n) + 2k
    BIC = n*ln(RSS/n) + k*ln(n)
    """
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    n = yt.shape[0]
    rss = float(np.sum((yt - yp) ** 2))
    sigma2 = rss / max(n, 1)
    aic = n * math.log(max(sigma2, 1e-12)) + 2 * k
    bic = n * math.log(max(sigma2, 1e-12)) + k * math.log(max(n, 1))
    return aic, bic, rss, n

# ======================================================
# 1) GLR (reuse if available)
# ======================================================
try:
    glr_model
    glr_sum = glr_model.summary
except NameError:
    glr = GeneralizedLinearRegression(
        featuresCol="features", labelCol="SALARY",
        predictionCol="prediction_glr",
        family="gaussian", link="identity",
        fitIntercept=True, regParam=RIDGE, maxIter=MAXITER_GLR, tol=TOL
    )
    glr_model = glr.fit(train_nz)
    glr_sum = glr_model.summary

pred_col_glr = glr_model.getPredictionCol()  # robust to name
pred_glr_df = glr_model.transform(test_nz).select(
    F.col("SALARY"), F.col(pred_col_glr).alias("pred_glr")
).cache()
_ = pred_glr_df.count()

e_rmse_glr = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_glr", metricName="rmse")
e_mae_glr  = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_glr", metricName="mae")
rmse_glr = e_rmse_glr.evaluate(pred_glr_df)
mae_glr  = e_mae_glr.evaluate(pred_glr_df)

# GLR: official AIC + BIC via residuals (k = p + 1)
aic_glr = float(getattr(glr_sum, "aic", float("nan")))
k_glr   = int(glr_model.coefficients.size) + 1
glr_pd  = pred_glr_df.toPandas()
_, bic_glr, _, _ = aic_bic_from_residuals(glr_pd["SALARY"], glr_pd["pred_glr"], k_glr)

# ======================================================
# 2) Polynomial LR (degree-2 on MIN_YEARS only ‚Äî avoids OHE explosion)
# ======================================================
def attach_poly_sq(df_with_columns):
    """
    Adds MIN_YEARS_EXPERIENCE_SQ; if missing, fills 0.0 to keep the pipeline running.
    Then appends it to the existing 'features' vector -> 'features_poly2'.
    """
    if "MIN_YEARS_EXPERIENCE" in df_with_columns.columns:
        df1 = df_with_columns.withColumn(
            "MIN_YEARS_EXPERIENCE_SQ", F.pow(F.col("MIN_YEARS_EXPERIENCE"), F.lit(2.0))
        )
    else:
        df1 = df_with_columns.withColumn("MIN_YEARS_EXPERIENCE_SQ", F.lit(0.0))
    va = VectorAssembler(inputCols=["features", "MIN_YEARS_EXPERIENCE_SQ"], outputCol="features_poly2")
    return va.transform(df1)

# Prefer original split frames if you have them; else use pruned frames directly
try:
    train_poly = attach_poly_sq(train_df)
    test_poly  = attach_poly_sq(test_df)
except NameError:
    train_poly = attach_poly_sq(train_nz)
    test_poly  = attach_poly_sq(test_nz)

lr_poly = LinearRegression(
    featuresCol="features_poly2", labelCol="SALARY", predictionCol="pred_poly",
    fitIntercept=True, regParam=RIDGE, elasticNetParam=0.0,
    solver="l-bfgs", maxIter=MAXITER_LR, tol=TOL, standardization=True
)
lr_poly_model = lr_poly.fit(train_poly)
pred_poly_df  = lr_poly_model.transform(test_poly).select("SALARY", "pred_poly").cache()
_ = pred_poly_df.count()

e_rmse_poly = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_poly", metricName="rmse")
e_mae_poly  = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_poly", metricName="mae")
rmse_poly = e_rmse_poly.evaluate(pred_poly_df)
mae_poly  = e_mae_poly.evaluate(pred_poly_df)

k_poly   = int(lr_poly_model.coefficients.size) + 1
poly_pd  = pred_poly_df.toPandas()
aic_poly, bic_poly, _, _ = aic_bic_from_residuals(poly_pd["SALARY"], poly_pd["pred_poly"], k_poly)

# ======================================================
# 3) Random Forest (reuse if exists)
# ======================================================
try:
    rf_model
except NameError:
    rf_model = RandomForestRegressor(
        featuresCol="features", labelCol="SALARY", predictionCol="pred_rf",
        numTrees=200, maxDepth=6, featureSubsetStrategy="sqrt",
        subsamplingRate=0.8, minInstancesPerNode=2, maxBins=64, seed=SEED
    ).fit(train_nz)

pred_col_rf = rf_model.getPredictionCol()
pred_rf_df = rf_model.transform(test_nz).select(
    F.col("SALARY"), F.col(pred_col_rf).alias("pred_rf")
).cache()
_ = pred_rf_df.count()

e_rmse_rf = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_rf", metricName="rmse")
e_mae_rf  = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_rf", metricName="mae")
rmse_rf = e_rmse_rf.evaluate(pred_rf_df)
mae_rf  = e_mae_rf.evaluate(pred_rf_df)

# Pseudo AIC/BIC via Gaussian RSS; k = (# nonzero importances + 1)
k_rf  = int(len(rf_model.featureImportances.indices) + 1)
rf_pd = pred_rf_df.toPandas()
aic_rf, bic_rf, _, _ = aic_bic_from_residuals(rf_pd["SALARY"], rf_pd["pred_rf"], k_rf)

# ======================================================
# 4) Unified predictions DataFrame (for plotting)
# ======================================================
preds_join = pred_glr_df.alias("g") \
    .join(pred_poly_df.alias("p"), on="SALARY", how="inner") \
    .join(pred_rf_df.alias("r"),   on="SALARY", how="inner") \
    .select(
        F.col("SALARY").alias("actual"),
        F.col("g.pred_glr").alias("pred_glr"),
        F.col("p.pred_poly").alias("pred_poly"),
        F.col("r.pred_rf").alias("pred_rf"),
    ).cache()
_ = preds_join.count()

print("‚úÖ Metrics")
print(f"GLR:   RMSE={rmse_glr:,.2f} | MAE={mae_glr:,.2f} | AIC={aic_glr:,.2f} | BIC={bic_glr:,.2f}")
print(f"Poly:  RMSE={rmse_poly:,.2f} | MAE={mae_poly:,.2f} | AIC={aic_poly:,.2f} | BIC={bic_poly:,.2f}")
print(f"RF:    RMSE={rmse_rf:,.2f}  | MAE={mae_rf:,.2f}  | AIC~{aic_rf:,.2f} | BIC~{bic_rf:,.2f}  (pseudo)")

# ======================================================
# 5) Interactive 2x2 grid (Plotly) + Static PNG to _output/
# ======================================================
import plotly.graph_objects as go
from plotly.subplots import make_subplots

pdf = preds_join.toPandas()  # test set is modest; ok to collect

rmse_dict = {"GLR": rmse_glr, "Poly": rmse_poly, "RF": rmse_rf}
best_name = min(rmse_dict, key=rmse_dict.get)
best_pred_col = {"GLR": "pred_glr", "Poly": "pred_poly", "RF": "pred_rf"}[best_name]

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=("GLR: Actual vs Pred", "Polynomial LR: Actual vs Pred",
                    "Random Forest: Actual vs Pred", f"Residuals ({best_name})"),
    horizontal_spacing=0.08, vertical_spacing=0.12
)

def diag_minmax(x):
    return float(np.nanmin(x)), float(np.nanmax(x))

a_min, a_max = diag_minmax(pdf["actual"])

fig.add_trace(go.Scattergl(x=pdf["actual"], y=pdf["pred_glr"], mode="markers",
                           name=f"GLR (RMSE={rmse_glr:,.0f})", opacity=0.5), row=1, col=1)
fig.add_trace(go.Scattergl(x=[a_min, a_max], y=[a_min, a_max], mode="lines",
                           line=dict(dash="dash"), showlegend=False), row=1, col=1)

fig.add_trace(go.Scattergl(x=pdf["actual"], y=pdf["pred_poly"], mode="markers",
                           name=f"Poly (RMSE={rmse_poly:,.0f})", opacity=0.5), row=1, col=2)
fig.add_trace(go.Scattergl(x=[a_min, a_max], y=[a_min, a_max], mode="lines",
                           line=dict(dash="dash"), showlegend=False), row=1, col=2)

fig.add_trace(go.Scattergl(x=pdf["actual"], y=pdf["pred_rf"], mode="markers",
                           name=f"RF (RMSE={rmse_rf:,.0f})", opacity=0.5), row=2, col=1)
fig.add_trace(go.Scattergl(x=[a_min, a_max], y=[a_min, a_max], mode="lines",
                           line=dict(dash="dash"), showlegend=False), row=2, col=1)

resid = pdf["actual"] - pdf[best_pred_col]
fig.add_trace(go.Histogram(x=resid, name=f"{best_name} Residuals", nbinsx=40), row=2, col=2)

fig.update_xaxes(title_text="Actual Salary", row=1, col=1)
fig.update_yaxes(title_text="Predicted Salary", row=1, col=1)
fig.update_xaxes(title_text="Actual Salary", row=1, col=2)
fig.update_yaxes(title_text="Predicted Salary", row=1, col=2)
fig.update_xaxes(title_text="Actual Salary", row=2, col=1)
fig.update_yaxes(title_text="Predicted Salary", row=2, col=1)
fig.update_xaxes(title_text="Residual", row=2, col=2)

fig.update_layout(height=720, width=980, title_text="Model Comparison: GLR vs Polynomial vs Random Forest")
fig.show()

# ----- Save static PNG without kaleido (matplotlib fallback) -----
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    use_sns = True
except Exception:
    use_sns = False

os.makedirs("_output", exist_ok=True)
plt.figure(figsize=(11.5, 8.5))

ax1 = plt.subplot(2,2,1)
if use_sns: sns.scatterplot(x=pdf["actual"], y=pdf["pred_glr"], s=15, alpha=0.5, ax=ax1)
else:       ax1.scatter(pdf["actual"], pdf["pred_glr"], s=10, alpha=0.5)
ax1.plot([a_min, a_max], [a_min, a_max], linestyle="--")
ax1.set_title(f"GLR (RMSE={rmse_glr:,.0f})"); ax1.set_xlabel("Actual"); ax1.set_ylabel("Predicted")

ax2 = plt.subplot(2,2,2)
if use_sns: sns.scatterplot(x=pdf["actual"], y=pdf["pred_poly"], s=15, alpha=0.5, ax=ax2)
else:       ax2.scatter(pdf["actual"], pdf["pred_poly"], s=10, alpha=0.5)
ax2.plot([a_min, a_max], [a_min, a_max], linestyle="--")
ax2.set_title(f"Polynomial (RMSE={rmse_poly:,.0f})"); ax2.set_xlabel("Actual"); ax2.set_ylabel("Predicted")

ax3 = plt.subplot(2,2,3)
if use_sns: sns.scatterplot(x=pdf["actual"], y=pdf["pred_rf"], s=15, alpha=0.5, ax=ax3)
else:       ax3.scatter(pdf["actual"], pdf["pred_rf"], s=10, alpha=0.5)
ax3.plot([a_min, a_max], [a_min, a_max], linestyle="--")
ax3.set_title(f"Random Forest (RMSE={rmse_rf:,.0f})"); ax3.set_xlabel("Actual"); ax3.set_ylabel("Predicted")

ax4 = plt.subplot(2,2,4)
if use_sns: sns.histplot(resid, bins=40, kde=False, ax=ax4)
else:       ax4.hist(resid, bins=40)
ax4.set_title(f"{best_name} Residuals"); ax4.set_xlabel("Residual"); ax4.set_ylabel("Count")

plt.tight_layout()
plt.savefig("_output/model_comparison_grid.png", dpi=200)
plt.close()

print("‚úÖ Saved static grid to: _output/model_comparison_grid.png")

```

## Calculating Log-Likelihood and BIC for PySpark Models


```{python}
#| label: glr-loglik-bic
#| echo: false
#| warning: true     # override global suppression so you can see issues
#| error: true       # if something fails, you'll see it in the page

import math, os
import pandas as pd

def _safe_int(x, default=None):
    try:
        return int(x)
    except Exception:
        return default

try:
    # 1) Make sure GLR is available
    glr_model
    glr_sum = glr_model.summary

    # 2) Pull values (robust to Spark version diffs)
    deviance   = float(getattr(glr_sum, "deviance", float("nan")))
    dispersion = float(getattr(glr_sum, "dispersion", float("nan")))

    # n: try direct attrs; else reconstruct as dof + params
    n_obs = None
    for attr in ("numInstances", "n", "numObservations"):
        if hasattr(glr_sum, attr):
            n_obs = _safe_int(getattr(glr_sum, attr))
            if n_obs is not None:
                break
    if n_obs is None:
        dof = _safe_int(getattr(glr_sum, "degreesOfFreedom", 0), 0)
        k_from_sum = _safe_int(getattr(glr_sum, "numParameters",
                                       glr_model.coefficients.size + 1),
                               glr_model.coefficients.size + 1)
        n_obs = dof + k_from_sum

    # k: parameters = #coefs + intercept
    k_params = _safe_int(getattr(glr_sum, "numParameters",
                                 glr_model.coefficients.size + 1),
                         glr_model.coefficients.size + 1)

    # 3) Log-Likelihood per assignment
    log_likelihood = -0.5 * (
        n_obs * math.log(2 * math.pi)
        + n_obs * math.log(dispersion)
        + (deviance / dispersion)
    )

    # 4) BIC per assignment (two equivalent forms)
    bic_loglik   = k_params * math.log(n_obs) - 2.0 * log_likelihood
    bic_expanded = k_params * math.log(n_obs) + (
        n_obs * math.log(2 * math.pi)
        + n_obs * math.log(dispersion)
        + (deviance / dispersion)
    )

    aic_glr = float(getattr(glr_sum, "aic", float("nan")))

    # 5) Show as a small table (always renders on the page)
    out = pd.DataFrame([{
        "n (observations)": n_obs,
        "k (parameters)": k_params,
        "dispersion": dispersion,
        "deviance": deviance,
        "log_likelihood": log_likelihood,
        "AIC (GLR summary)": aic_glr,
        "BIC (k*log(n) - 2*LL)": bic_loglik,
        "BIC (expanded)": bic_expanded,
    }])
    display(out)

    # 6) Persist so you have an artifact in _output/
    os.makedirs("_output", exist_ok=True)
    out.to_csv("_output/glr_loglik_bic.csv", index=False)
    print("üíæ Saved: _output/glr_loglik_bic.csv")

except NameError:
    print("‚ö†Ô∏è GLR model not found. Run your GLR training cell first.")
except Exception as e:
    print(f"‚ö†Ô∏è Could not compute GLR Log-Likelihood/BIC: {e}")


```


## Evaluation Metrics


```{python}
# ======================================================
# Evaluation metrics for GLR, Polynomial LR (degree-2 on MIN_YEARS), and RF
#  - RMSE / MAE / R¬≤
#  - AIC / BIC:
#      * GLR: AIC from summary, BIC from 7.1
#      * Polynomial LR & RF: Gaussian RSS-based AIC/BIC (RF labeled 'pseudo')
#  - Interactive Plotly comparison + saved CSV/PNG (no kaleido needed)
# ======================================================

from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import RandomForestRegressor, LinearRegression
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import functions as F
import numpy as np, math, os
import pandas as pd

# ---------- Helpers ----------
def aic_bic_from_residuals(y_true, y_pred, k):
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    n = yt.shape[0]
    rss = float(np.sum((yt - yp) ** 2))
    sigma2 = rss / max(n, 1)
    aic = n * math.log(max(sigma2, 1e-12)) + 2 * k
    bic = n * math.log(max(sigma2, 1e-12)) + k * math.log(max(n, 1))
    return aic, bic

def get_or_make_poly_frames(base_train, base_test):
    # Attach MIN_YEARS_EXPERIENCE_SQ (safe default if column missing)
    def add_sq(df):
        if "MIN_YEARS_EXPERIENCE" in df.columns:
            df1 = df.withColumn("MIN_YEARS_EXPERIENCE_SQ", F.pow(F.col("MIN_YEARS_EXPERIENCE"), F.lit(2.0)))
        else:
            df1 = df.withColumn("MIN_YEARS_EXPERIENCE_SQ", F.lit(0.0))
        va = VectorAssembler(inputCols=["features", "MIN_YEARS_EXPERIENCE_SQ"], outputCol="features_poly2")
        return va.transform(df1)
    try:
        # Prefer original frames if available (more columns available for plotting later)
        _ = train_df.count(); _ = test_df.count()
        return add_sq(train_df), add_sq(test_df)
    except NameError:
        return add_sq(base_train), add_sq(base_test)

# ---------- Preconditions ----------
try:
    _ = train_nz.count(); _ = test_nz.count()
except NameError:
    raise RuntimeError("train_nz/test_nz not found. Run the split + feature steps first.")

# ---------- GLR predictions ----------
try:
    glr_model
    glr_sum = glr_model.summary
except NameError:
    raise RuntimeError("GLR model not found. Run GLR training first.")

pred_col_glr = glr_model.getPredictionCol()
pred_glr_df = glr_model.transform(test_nz).select(F.col("SALARY"), F.col(pred_col_glr).alias("pred_glr")).cache()
_ = pred_glr_df.count()

ev_rmse = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_glr", metricName="rmse")
ev_mae  = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_glr", metricName="mae")
ev_r2   = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_glr", metricName="r2")
rmse_glr, mae_glr, r2_glr = ev_rmse.evaluate(pred_glr_df), ev_mae.evaluate(pred_glr_df), ev_r2.evaluate(pred_glr_df)

# BIC for GLR from 7.1 (recompute quickly using same formula to avoid cross-cell state)
deviance   = float(getattr(glr_sum, "deviance", float("nan")))
dispersion = float(getattr(glr_sum, "dispersion", float("nan")))
n_obs = None
for attr in ["numInstances", "n", "numObservations"]:
    if hasattr(glr_sum, attr):
        try:
            n_obs = int(getattr(glr_sum, attr)); break
        except Exception:
            pass
if n_obs is None:
    dof = int(getattr(glr_sum, "degreesOfFreedom", 0))
    k_from_sum = int(getattr(glr_sum, "numParameters", glr_model.coefficients.size + 1))
    n_obs = dof + k_from_sum
k_glr  = int(getattr(glr_sum, "numParameters", glr_model.coefficients.size + 1))
loglik = -0.5 * (n_obs * math.log(2*math.pi) + n_obs * math.log(dispersion) + (deviance/dispersion))
bic_glr = k_glr * math.log(n_obs) - 2.0 * loglik
aic_glr = float(getattr(glr_sum, "aic", float("nan")))

# ---------- Polynomial LR (degree-2 on MIN_YEARS only) ----------
train_poly, test_poly = get_or_make_poly_frames(train_nz, test_nz)
lr_poly = LinearRegression(
    featuresCol="features_poly2", labelCol="SALARY", predictionCol="pred_poly",
    fitIntercept=True, regParam=1e-4, elasticNetParam=0.0,
    solver="l-bfgs", maxIter=200, tol=1e-5, standardization=True
)
lr_poly_model = lr_poly.fit(train_poly)
pred_poly_df  = lr_poly_model.transform(test_poly).select("SALARY", "pred_poly").cache(); _ = pred_poly_df.count()

ev_rmse_p = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_poly", metricName="rmse")
ev_mae_p  = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_poly", metricName="mae")
ev_r2_p   = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_poly", metricName="r2")
rmse_poly, mae_poly, r2_poly = ev_rmse_p.evaluate(pred_poly_df), ev_mae_p.evaluate(pred_poly_df), ev_r2_p.evaluate(pred_poly_df)

k_poly = int(lr_poly_model.coefficients.size) + 1
poly_pd = pred_poly_df.toPandas()
aic_poly, bic_poly = aic_bic_from_residuals(poly_pd["SALARY"], poly_pd["pred_poly"], k_poly)

# ---------- Random Forest ----------
try:
    rf_model
except NameError:
    rf_model = RandomForestRegressor(
        featuresCol="features", labelCol="SALARY", predictionCol="pred_rf",
        numTrees=200, maxDepth=6, featureSubsetStrategy="sqrt",
        subsamplingRate=0.8, minInstancesPerNode=2, maxBins=64, seed=42
    ).fit(train_nz)

pred_col_rf = rf_model.getPredictionCol()
pred_rf_df = rf_model.transform(test_nz).select(F.col("SALARY"), F.col(pred_col_rf).alias("pred_rf")).cache(); _ = pred_rf_df.count()

ev_rmse_r = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_rf", metricName="rmse")
ev_mae_r  = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_rf", metricName="mae")
ev_r2_r   = RegressionEvaluator(labelCol="SALARY", predictionCol="pred_rf", metricName="r2")
rmse_rf, mae_rf, r2_rf = ev_rmse_r.evaluate(pred_rf_df), ev_mae_r.evaluate(pred_rf_df), ev_r2_r.evaluate(pred_rf_df)

# Pseudo AIC/BIC for RF
k_rf  = int(len(rf_model.featureImportances.indices) + 1)
rf_pd = pred_rf_df.toPandas()
aic_rf, bic_rf = aic_bic_from_residuals(rf_pd["SALARY"], rf_pd["pred_rf"], k_rf)

# ---------- Metrics table ----------
metrics = pd.DataFrame([
    ["GLR",        rmse_glr,  mae_glr,  r2_glr,  aic_glr,  bic_glr,            ""],
    ["Polynomial", rmse_poly, mae_poly, r2_poly, aic_poly, bic_poly,           ""],
    ["RF",         rmse_rf,   mae_rf,   r2_rf,   aic_rf,   bic_rf,   "pseudo AIC/BIC"],
], columns=["Model","RMSE","MAE","R2","AIC","BIC","Notes"])

# Display neatly in text + save CSV
print("\n‚úÖ Evaluation Metrics (test set)")
print(metrics.to_string(index=False, formatters={
    "RMSE": "{:,.2f}".format, "MAE": "{:,.2f}".format, "R2": "{:.4f}".format,
    "AIC": "{:,.2f}".format, "BIC": "{:,.2f}".format
}))
os.makedirs("_output", exist_ok=True)
metrics.to_csv("_output/metrics_summary.csv", index=False)
print("üìÑ Saved: _output/metrics_summary.csv")

# ---------- Interactive comparison (Plotly) ----------
import plotly.express as px
fig1 = px.bar(metrics, x="Model", y="RMSE", color="Model", title="RMSE by Model", text=metrics["RMSE"].map(lambda v: f"{v:,.0f}"))
fig2 = px.bar(metrics, x="Model", y="R2",   color="Model", title="R¬≤ by Model",   text=metrics["R2"].map(lambda v: f"{v:.3f}"))
fig1.update_traces(textposition="outside"); fig2.update_traces(textposition="outside")
fig1.show(); fig2.show()

# ---------- Static PNG grid (Matplotlib, no kaleido) ----------
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    use_sns = True
except Exception:
    use_sns = False

plt.figure(figsize=(9.5, 4.5))
ax = plt.subplot(1,2,1)
if use_sns: sns.barplot(data=metrics, x="Model", y="RMSE", ax=ax)
else:       ax.bar(metrics["Model"], metrics["RMSE"])
ax.set_title("RMSE by Model"); ax.set_ylabel("RMSE"); ax.set_xlabel("")

ax2 = plt.subplot(1,2,2)
if use_sns: sns.barplot(data=metrics, x="Model", y="R2", ax=ax2)
else:       ax2.bar(metrics["Model"], metrics["R2"])
ax2.set_title("R¬≤ by Model"); ax2.set_ylabel("R¬≤"); ax2.set_xlabel("")

plt.tight_layout()
plt.savefig("_output/metrics_comparison.png", dpi=200)
plt.close()
print("üñºÔ∏è Saved: _output/metrics_comparison.png")

```