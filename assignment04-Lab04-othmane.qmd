---
title: "Assignment 04 ‚Äî Lab 04 Machine Learning on Scale"
subtitle: "Causal and Predictive Analytics in Spark"
author:
  - name: "Othmane Elouardi"
    affiliations:
      - id: bu
        name: "Boston University"
        city: "Boston"
        state: "MA"
date: "2025-10-08"
number-sections: true
format:
  html:
    theme:
      light: lux
      dark: slate
    toc: true
    toc-depth: 3
    toc-location: right
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-line-numbers: true
    highlight-style: a11y
    page-layout: article
    css: styles.css
    grid:
      body-width: 900px
      margin-width: 280px
execute:
  echo: true
  warning: false
  error: false
  freeze: auto
jupyter: env
---


# Introduction
PySpark is applied to large-scale employment data from the Lightcast Job Postings dataset to develop and evaluate salary prediction models. The workflow involves engineering key features from structured columns, training a Linear Regression model, and assessing performance using RMSE and R¬≤ metrics. Visual diagnostic plots are generated to interpret model accuracy and residual patterns. The final analysis is documented, version-controlled, and submitted via GitHub, demonstrating end-to-end data processing and predictive modeling on a distributed computing platform.

# Load the Dataset
 **PySpark** is used to load the *Lightcast Job Postings* dataset into a Spark DataFrame.  
This approach enables us to efficiently handle large datasets within the EC2 environment before proceeding with feature engineering and regression analysis.

```{python}
# Load the Lightcast dataset with PySpark

from pyspark.sql import SparkSession
import os

# Initialize Spark session 
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .config("spark.driver.memory", "1g")
    .getOrCreate()
)

# Define dataset path
csv_path = "data/lightcast_job_postings.csv"

# Check path validity
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"‚ùå Could not find {csv_path}. Please ensure the file exists in the data/ folder.")

# Load dataset
df = (
    spark.read
    .option("header", "true")       # First row as headers
    .option("inferSchema", "true")  # Auto-detect data types
    .option("multiLine", "true")    # Handle multi-line text fields
    .option("escape", "\"")         # Handle embedded quotes
    .csv(csv_path)
)

# Confirm load success
print("‚úÖ Dataset successfully loaded!")
df.printSchema()
df.show(5, truncate=False)
```



# Feature Engineering

Feature engineering is a crucial step in preparing the dataset for machine learning.  
This section cleans the data, encodes categorical variables, and constructs feature vectors for model training.

```{python}
# ======================================================
# FEATURE ENGINEERING
# ======================================================

from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler

# ------------------------------------------------------
# 1. Drop rows with missing target or key features
# ------------------------------------------------------
df_clean = df.dropna(subset=[
    "SALARY", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE",
    "EMPLOYMENT_TYPE_NAME", "STATE_NAME", "MIN_EDULEVELS_NAME"
])

print(f"‚úÖ Rows after dropping nulls: {df_clean.count():,}")

# ------------------------------------------------------
# 2. Create new feature: MIN_YEARS_EXPERIENCE_SQ
# ------------------------------------------------------
df_clean = df_clean.withColumn(
    "MIN_YEARS_EXPERIENCE_SQ",
    F.pow(F.col("MIN_YEARS_EXPERIENCE"), 2)
)

# ------------------------------------------------------
# 3. Encode categorical variables
# ------------------------------------------------------
cat_cols = ["EMPLOYMENT_TYPE_NAME", "STATE_NAME", "MIN_EDULEVELS_NAME"]

indexers = [
    StringIndexer(inputCol=c, outputCol=f"{c}_IDX", handleInvalid="keep")
    for c in cat_cols
]

encoders = [
    OneHotEncoder(inputCol=f"{c}_IDX", outputCol=f"{c}_OHE")
    for c in cat_cols
]

# Apply indexers sequentially
for idx in indexers:
    df_clean = idx.fit(df_clean).transform(df_clean)

# Apply one-hot encoders sequentially
for enc in encoders:
    df_clean = enc.fit(df_clean).transform(df_clean)

# ------------------------------------------------------
# 4. Assemble final features
# ------------------------------------------------------
numeric_cols = [
    "MIN_YEARS_EXPERIENCE", 
    "MAX_YEARS_EXPERIENCE", 
    "MIN_YEARS_EXPERIENCE_SQ"
]

ohe_cols = [f"{c}_OHE" for c in cat_cols]

assembler = VectorAssembler(
    inputCols=numeric_cols + ohe_cols,
    outputCol="features"
)

df_fe = assembler.transform(df_clean)

# ------------------------------------------------------
# 5. Show sample of processed data
# ------------------------------------------------------
df_fe.select(
    "SALARY", "MIN_YEARS_EXPERIENCE", "STATE_NAME", "features"
).show(5, truncate=False)

```

# Train/Test Split

Splitting the data into training and testing sets ensures that the model is evaluated on unseen data, which helps measure its generalization performance.  
Here an **80/20 split** is used, a common practice that provides a large enough training set while reserving sufficient data for evaluation.

```{python}
# ======================================================
# TRAIN / TEST SPLIT
# ======================================================

# Perform random split with reproducibility
train_df, test_df = df_fe.randomSplit([0.8, 0.2], seed=42)

# Display record counts for each subset
train_count = train_df.count()
test_count = test_df.count()
total_count = df_fe.count()

print(f"‚úÖ Total records: {total_count:,}")
print(f"üß† Training set: {train_count:,} rows ({train_count/total_count:.1%})")
print(f"üßæ Testing set:  {test_count:,} rows ({test_count/total_count:.1%})")

```




# Linear Regression

Zero-variance features to restore full rank, train a Linear Regression model for
prediction, and assemble a coefficient table with **SE, t, p, 95% CI** using LR/GLR or a
bootstrap fallback when analytic standard errors are unavailable.

```{python}
# ======================================================
# LINEAR REGRESSION with robust inference
#   - zero-variance pruning (Summarizer.variance)
#   - LR metrics on test set
#   - LR -> GLR -> Bootstrap fallback for SE/t/p
# ======================================================

from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression
from pyspark.ml.feature import VectorSlicer
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.stat import Summarizer
from pyspark.sql import functions as F
import math
import numpy as np

# -----------------------------
# Helpers
# -----------------------------
def extract_feature_names(df, features_col="features"):
    """Read feature names from Vector metadata; fallback to generic names."""
    meta = df.schema[features_col].metadata
    try:
        attrs = meta["ml_attr"]["attrs"]
        names = []
        for typ in ("binary", "numeric"):
            if typ in attrs:
                for a in attrs[typ]:
                    names.append(a.get("name", f"f_{a.get('idx', len(names))}"))
        return names
    except Exception:
        size = df.selectExpr(f"size({features_col}) as n").head().n
        return [f"feature_{i}" for i in range(size)]

def normal_pvalue_from_t(t):
    """Two-sided p-value from z/t using normal approx."""
    if t is None or t != t:
        return float("nan")
    return 2.0 * (1.0 - 0.5 * (1 + math.erf(abs(float(t))/math.sqrt(2))))

def safe_ci(beta, se, z=1.96):
    try:
        if se is None or se != se:
            return float("nan"), float("nan")
        return float(beta - z*se), float(beta + z*se)
    except Exception:
        return float("nan"), float("nan")

def build_table(coefs, ses, names, intercept=None, intercept_se=float("nan")):
    # Align lengths
    if len(names) != len(coefs):
        if len(names) > len(coefs):
            names = names[:len(coefs)]
        else:
            names += [f"feature_{i}" for i in range(len(names), len(coefs))]

    rows = []
    for n, b, se in zip(names, coefs, ses):
        t = float("nan") if (se is None or se != se or se == 0.0) else float(b / se)
        p = normal_pvalue_from_t(t)
        lo, hi = safe_ci(b, se)
        rows.append((n, float(b), float(se) if se == se else float("nan"), t, p, lo, hi))

    coef_df = spark.createDataFrame(rows, ["feature", "coef", "se", "t", "p", "ci_lo", "ci_hi"])

    # Intercept row (if provided)
    if intercept is not None:
        t_i = float("nan") if (intercept_se != intercept_se or intercept_se == 0.0) else float(intercept / intercept_se)
        p_i = normal_pvalue_from_t(t_i)
        lo_i, hi_i = safe_ci(intercept, intercept_se)
        irow = spark.createDataFrame(
            [("Intercept", float(intercept), float(intercept_se), t_i, p_i, lo_i, hi_i)],
            ["feature", "coef", "se", "t", "p", "ci_lo", "ci_hi"]
        )
        coef_df = irow.unionByName(coef_df)

    return coef_df

# ---------------------------------------------
# 1) Zero-variance pruning on TRAIN (robust)
# ---------------------------------------------
feat_names_full = extract_feature_names(train_df, "features")

var_row = train_df.select(
    Summarizer.variance(train_df["features"]).alias("var")
).first()

# Convert variance vector to 1-D numpy safely
variances = var_row["var"].toArray() if hasattr(var_row["var"], "toArray") else np.array(var_row["var"]).ravel()

keep_idx = [int(i) for i, v in enumerate(variances) if (v is not None) and not np.isnan(v) and v > 1e-12]
if not keep_idx:  # safety fallback
    keep_idx = list(range(len(variances)))

removed = len(variances) - len(keep_idx)
print(f"‚ÑπÔ∏è Removed {removed} zero-variance feature(s)." if removed > 0 else "‚ÑπÔ∏è No zero-variance features found.")

slicer = VectorSlicer(inputCol="features", outputCol="features_nz", indices=keep_idx)
train_nz = slicer.transform(train_df).drop("features").withColumnRenamed("features_nz", "features")
test_nz  = slicer.transform(test_df ).drop("features").withColumnRenamed("features_nz", "features")

feat_names = [feat_names_full[i] for i in keep_idx] if keep_idx else feat_names_full

# ---------------------------------------------
# 2) Fit Linear Regression and evaluate on TEST
# ---------------------------------------------
lr = LinearRegression(
    featuresCol="features",
    labelCol="SALARY",
    predictionCol="prediction",
    fitIntercept=True,
    regParam=0.0,
    elasticNetParam=0.0,
    solver="normal",
    maxIter=100
)
lr_model = lr.fit(train_nz)
lr_sum   = lr_model.summary

e_rmse = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="rmse")
e_r2   = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="r2")
e_mae  = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="mae")

pred_test = lr_model.transform(test_nz).cache()
rmse_test = e_rmse.evaluate(pred_test)
r2_test   = e_r2.evaluate(pred_test)
mae_test  = e_mae.evaluate(pred_test)

print(f"‚úÖ Test RMSE: {rmse_test:,.2f}")
print(f"‚úÖ Test MAE : {mae_test:,.2f}")
print(f"‚úÖ Test R¬≤  : {r2_test:,.4f}")

print("\n--- Training Metrics (reference) ---")
print(f"RMSE: {lr_sum.rootMeanSquaredError:,.2f} | MAE: {lr_sum.meanAbsoluteError:,.2f} | R¬≤: {lr_sum.r2:,.4f}")

# ---------------------------------------------
# 3) Inference: LR -> GLR -> Bootstrap fallback
# ---------------------------------------------
coef_df_out = None
got_inference = False

# (a) Try LR analytic inference
try:
    ses_lr   = list(lr_sum.coefficientStandardErrors)
    coefs_lr = list(lr_model.coefficients.toArray())
    coef_df_out = build_table(coefs_lr, ses_lr, feat_names, intercept=lr_model.intercept, intercept_se=float("nan"))
    got_inference = True
    print("‚ÑπÔ∏è Using LR analytic standard errors.")
except Exception:
    pass

# (b) GLR fallback (Gaussian/identity)
if not got_inference:
    try:
        glr = GeneralizedLinearRegression(
            featuresCol="features", labelCol="SALARY", predictionCol="prediction_glr",
            family="gaussian", link="identity", fitIntercept=True, regParam=0.0, maxIter=100
        )
        glr_model = glr.fit(train_nz)
        glr_sum   = glr_model.summary

        ses_all = list(glr_sum.coefficientStandardErrors)  # may include intercept as last element
        coefs_g = list(glr_model.coefficients.toArray())

        # Intercept SE often appears as the last element
        intercept_se = float("nan")
        if len(ses_all) == len(coefs_g) + 1:
            intercept_se = float(ses_all[-1])
            ses_g = ses_all[:-1]
        else:
            ses_g = ses_all

        coef_df_out = build_table(coefs_g, ses_g, feat_names, intercept=glr_model.intercept, intercept_se=intercept_se)
        got_inference = True
        print("‚ÑπÔ∏è Using GLR analytic standard errors.")
    except Exception:
        pass

# (c) Bootstrap fallback (lightweight) if analytic SEs are unavailable
if not got_inference:
    print("‚ö†Ô∏è Analytic SE unavailable ‚Äî running bootstrap (B=30).")
    B = 30
    coef_mat = []
    for b in range(B):
        samp = train_nz.sample(withReplacement=True, fraction=1.0, seed=2025 + b)
        m = lr.fit(samp)
        coef_mat.append(m.coefficients.toArray())
    coef_mat = np.array(coef_mat)  # [B, p]
    coefs = lr_model.coefficients.toArray()
    ses   = np.std(coef_mat, axis=0, ddof=1)
    coef_df_out = build_table(list(coefs), list(ses), feat_names, intercept=lr_model.intercept, intercept_se=float("nan"))
    print("‚ÑπÔ∏è Bootstrap SE computed.")

# Order and display
coef_df_out = coef_df_out.withColumn("abs_t", F.abs(F.col("t"))).orderBy(F.desc_nulls_last("abs_t"))

print("\n--- Top 25 coefficients by |t| ---")
coef_df_out.select("feature", "coef", "se", "t", "p", "ci_lo", "ci_hi").show(25, truncate=False)

# save the full table
coef_df_out.coalesce(1).write.mode("overwrite").option("header", True).csv("output/linear_coef_table")


```