---
title: "Assignment 04 ‚Äî Lab 04 Machine Learning on Scale"
subtitle: "Causal and Predictive Analytics in Spark"
author:
  - name: "Othmane Elouardi"
    affiliations:
      - id: bu
        name: "Boston University"
        city: "Boston"
        state: "MA"
date: "2025-10-08"
number-sections: true
format:
  html:
    theme:
      light: lux
      dark: slate
    toc: true
    toc-depth: 3
    toc-location: right
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-line-numbers: true
    highlight-style: a11y
    page-layout: article
    css: styles.css
    grid:
      body-width: 900px
      margin-width: 280px
execute:
  echo: true
  warning: false
  error: false
  freeze: auto
jupyter: env
---


# Introduction
PySpark is applied to large-scale employment data from the Lightcast Job Postings dataset to develop and evaluate salary prediction models. The workflow involves engineering key features from structured columns, training a Linear Regression model, and assessing performance using RMSE and R¬≤ metrics. Visual diagnostic plots are generated to interpret model accuracy and residual patterns. The final analysis is documented, version-controlled, and submitted via GitHub, demonstrating end-to-end data processing and predictive modeling on a distributed computing platform.

# Load the Dataset
 **PySpark** is used to load the *Lightcast Job Postings* dataset into a Spark DataFrame.  
This approach enables us to efficiently handle large datasets within the EC2 environment before proceeding with feature engineering and regression analysis.

```{python}
# Load the Lightcast dataset with PySpark

from pyspark.sql import SparkSession
import os

# Initialize Spark session 
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .config("spark.driver.memory", "1g")
    .getOrCreate()
)

# Define dataset path
csv_path = "data/lightcast_job_postings.csv"

# Check path validity
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"‚ùå Could not find {csv_path}. Please ensure the file exists in the data/ folder.")

# Load dataset
df = (
    spark.read
    .option("header", "true")       # First row as headers
    .option("inferSchema", "true")  # Auto-detect data types
    .option("multiLine", "true")    # Handle multi-line text fields
    .option("escape", "\"")         # Handle embedded quotes
    .csv(csv_path)
)

# Confirm load success
print("‚úÖ Dataset successfully loaded!")
df.printSchema()
df.show(5, truncate=False)
```



# Feature Engineering

Feature engineering is a crucial step in preparing the dataset for machine learning.  
This section cleans the data, encodes categorical variables, and constructs feature vectors for model training.

```{python}
# ======================================================
# FEATURE ENGINEERING
# ======================================================

from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler

# ------------------------------------------------------
# 1. Drop rows with missing target or key features
# ------------------------------------------------------
df_clean = df.dropna(subset=[
    "SALARY", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE",
    "EMPLOYMENT_TYPE_NAME", "STATE_NAME", "MIN_EDULEVELS_NAME"
])

print(f"‚úÖ Rows after dropping nulls: {df_clean.count():,}")

# ------------------------------------------------------
# 2. Create new feature: MIN_YEARS_EXPERIENCE_SQ
# ------------------------------------------------------
df_clean = df_clean.withColumn(
    "MIN_YEARS_EXPERIENCE_SQ",
    F.pow(F.col("MIN_YEARS_EXPERIENCE"), 2)
)

# ------------------------------------------------------
# 3. Encode categorical variables
# ------------------------------------------------------
cat_cols = ["EMPLOYMENT_TYPE_NAME", "STATE_NAME", "MIN_EDULEVELS_NAME"]

indexers = [
    StringIndexer(inputCol=c, outputCol=f"{c}_IDX", handleInvalid="keep")
    for c in cat_cols
]

encoders = [
    OneHotEncoder(inputCol=f"{c}_IDX", outputCol=f"{c}_OHE")
    for c in cat_cols
]

# Apply indexers sequentially
for idx in indexers:
    df_clean = idx.fit(df_clean).transform(df_clean)

# Apply one-hot encoders sequentially
for enc in encoders:
    df_clean = enc.fit(df_clean).transform(df_clean)

# ------------------------------------------------------
# 4. Assemble final features
# ------------------------------------------------------
numeric_cols = [
    "MIN_YEARS_EXPERIENCE", 
    "MAX_YEARS_EXPERIENCE", 
    "MIN_YEARS_EXPERIENCE_SQ"
]

ohe_cols = [f"{c}_OHE" for c in cat_cols]

assembler = VectorAssembler(
    inputCols=numeric_cols + ohe_cols,
    outputCol="features"
)

df_fe = assembler.transform(df_clean)

# ------------------------------------------------------
# 5. Show sample of processed data
# ------------------------------------------------------
df_fe.select(
    "SALARY", "MIN_YEARS_EXPERIENCE", "STATE_NAME", "features"
).show(5, truncate=False)

```

# Train/Test Split

Splitting the data into training and testing sets ensures that the model is evaluated on unseen data, which helps measure its generalization performance.  
Here an **80/20 split** is used, a common practice that provides a large enough training set while reserving sufficient data for evaluation.

```{python}
# ======================================================
# TRAIN / TEST SPLIT
# ======================================================

# Perform random split with reproducibility
train_df, test_df = df_fe.randomSplit([0.8, 0.2], seed=42)

# Display record counts for each subset
train_count = train_df.count()
test_count = test_df.count()
total_count = df_fe.count()

print(f"‚úÖ Total records: {total_count:,}")
print(f"üß† Training set: {train_count:,} rows ({train_count/total_count:.1%})")
print(f"üßæ Testing set:  {test_count:,} rows ({test_count/total_count:.1%})")

```


